# 项目技术路线概要

本项目旨在基于**Faster R-CNN**框架，在DOTA遥感图像数据集中实现对飞机的**倾斜目标框 (Oriented Bounding Box - OBB)** 检测，并特别研究在模拟高斯光斑遮挡条件下，模型性能受遮挡程度和目标尺度的影响。

## 一、数据处理与准备

1.  **DOTA数据集适配：**
    *   **标注解析：** 解析DOTA提供的OBB标注（8个顶点坐标）。
    *   **图像分块 (Patching)：** 将DOTA的大尺寸原始图像（如4000x4000像素）切割成适合网络输入的较小图像块（例如1024x1024像素），并设置重叠区域（如100-200像素）。
    *   **标注转换：** 将原始图像中的OBB标注准确映射到各个图像块的坐标系下。

2.  **高斯光斑遮挡模拟：**
    *   在数据预处理阶段，于图像块送入网络前，按参数化方式（控制光斑位置、大小/标准差、强度）添加高斯光斑，以模拟不同程度的飞机目标遮挡。
    *   **重要：** 真实标注（Ground Truth OBB）始终使用DOTA提供的原始无遮挡标注。

3.  **数据增强：**
    *   **基础增强：** 水平翻转、色彩抖动等。
    *   **旋转增强：** 对图像块进行随机旋转（如0°, 90°, 180°, 270°，或小角度随机旋转），并确保OBB顶点的坐标也进行相应几何变换。这对于任意朝向的飞机和OBB检测至关重要。

## 二、模型构建与修改 (基于Faster R-CNN)

1.  **骨干网络 (Backbone Network)：**
    *   推荐使用如 **ResNet-50** 或 **ResNet-101** 作为骨干网络。
    *   强烈建议集成 **特征金字塔网络 (FPN - Feature Pyramid Network)**，以有效处理DOTA数据集中目标的多尺度特性，尤其是小目标。

2.  **区域提议网络 (RPN - Region Proposal Network)：**
    *   **锚点框 (Anchor Boxes)：**
        *   **类型：** 仍然使用**水平锚点框**。
        *   **设计：**
            *   分析DOTA飞机目标的最小外接水平矩形 (HBB) 的尺寸和长宽比分布。
            *   为FPN的每个特征层级（如P2-P6）设计合适的基础锚点尺寸 (base sizes/scales)，例如 `[32^2, 64^2, 128^2, 256^2, 512^2]` 对应的边长。
            *   根据飞机通常的细长形状，设计合适的长宽比 (aspect ratios)，例如 `[0.33, 0.5, 1.0, 2.0, 3.0]`。
    *   **RPN回归目标：** 仍然是4个参数 `(dx, dy, dw, dh)`，用于优化水平候选框。

3.  **RoI 头部 (RoI Head) - 核心修改区域：**
    *   **输入：** 接收由RPN生成的水平候选区域 (RoIs)，并通过RoI Align/Pooling提取特征。
    *   **分类器 (Classifier)：** 标准的分类分支，用于区分飞机目标与背景。
    *   **边界框回归器 (Bounding Box Regressor for OBB)：**
        *   **输出层修改：** 将原Faster R-CNN中预测4个参数（对应HBB）的最终全连接层，修改为预测 **8个参数**。
        *   **参数化定义：** 这8个参数代表了目标倾斜框的4个顶点 `(gx_i, gy_i)` 相对于输入水平RoI（例如其左上角点 `(x_roi, y_roi)` 和宽高 `(w_roi, h_roi)`）的归一化偏移量。例如，对第 `i` 个顶点：
            *   `t_xi = (gx_i - x_roi) / w_roi`
            *   `t_yi = (gyi - y_roi) / h_roi`
            *   （或采用DOTA论文中基于RoI四个顶点的偏移定义方式，确保一致性即可）。

## 三、损失函数与训练

1.  **RPN损失：** 保持标准Faster R-CNN的RPN损失（分类损失 + 水平框回归损失）。
2.  **RoI头部损失：**
    *   **分类损失：** 标准交叉熵损失。
    *   **OBB回归损失：**
        *   **目标值计算：** 对于每个与真实OBB匹配的RoI，根据上述参数化方法计算其对应的8个目标回归值 `(target_tx1, target_ty1, ..., target_tx4, target_ty4)`。
        *   **损失函数：** 使用 **Smooth L1 Loss** (或其他合适的回归损失) 计算模型预测的8个参数与这8个目标回归值之间的差异。

## 四、推理与后处理

1.  **模型推理：** 将（可能带有模拟遮挡的）测试图像块输入训练好的模型。
2.  **OBB参数解码：** 将模型回归头输出的8个归一化偏移量转换回图像块坐标系下OBB的绝对顶点坐标。
3.  **结果合并与NMS：**
    *   将从各个图像块中检测到的OBB坐标转换回原始大图的坐标系。
    *   **倾斜非极大值抑制 (Oriented NMS)：** 使用适用于OBB的NMS算法（基于旋转框的IoU计算）来去除冗余检测结果。DOTA论文建议的OBB NMS阈值为0.1。
4.  **中心点计算：** 根据最终预测的OBB顶点坐标，计算出任务要求的飞机中心点。

## 五、实验设计与分析

1.  **基线建立：** 在无遮挡的原始DOTA飞机数据上训练并评估模型性能。
2.  **影响分析：**
    *   系统地改变高斯光斑的参数（模拟不同遮挡程度）。
    *   根据目标大小（尺度）对飞机进行分组。
    *   评估在不同遮挡程度和目标尺度下，模型的检测性能（如mAP for OBB, 中心点预测误差等）。

## 六、技术栈建议

*   **深度学习框架：** PyTorch (推荐) 或 TensorFlow/Keras。
*   **核心库：** OpenCV, NumPy, Matplotlib/Seaborn。
*   **目标检测库参考：** 可借鉴MMDetection, Detectron2等成熟框架的实现思路和模块。 