1
《模式识别与机器学习》
课程设计
指导书
华中科技大学
人工智能与自动化学院
二Ｏ二五年四月
1
目录
一、课程设计目的........................................................................................................................................... 1
二、课程设计选题........................................................................................................................................... 1
三、总体设计要求........................................................................................................................................... 5
四、课程具体步骤与时间安排.......................................................................................................................5
五、模式识别课程设计内容及具体要求.......................................................................................................6
5.1 遮挡条件下遥感图像中的飞机目标检测........................................................................................6
5.2 蜜蜂视频检测与跟踪........................................................................................................................8
5.3 鲁棒的深度检测网络对抗图像生成..............................................................................................11
5.4 伪装目标检测与识别......................................................................................................................12
5.5 微小目标检测.................................................................................................................................. 16
5.6 跨域少样本目标识别......................................................................................................................18
5.7 噪声条件下的心脏音频数据识别..................................................................................................21
5.8 无人机视角下的车辆目标检测......................................................................................................26
5.9 植物根系图像抠图与表型提取......................................................................................................28
5.10 密集人群运动分析........................................................................................................................29
5.11 密集视频行人流量计数................................................................................................................ 31
5.12 全球麦穗分割挑战赛....................................................................................................................33
5.13 面向视觉 ViT 模型的通用特征上采样....................................................................................... 34
5.14 品种无关的植物计数....................................................................................................................35
5.15 模特虚拟街拍合成........................................................................................................................38
5.16 虚拟试衣........................................................................................................................................ 39
5.17 三维视线估计................................................................................................................................ 41
5.18 基于 RGB 图像的三维交互手势估计......................................................................................... 42
5.19 非受限条件下的人体眨眼检测....................................................................................................44
5.20 基于视觉和大语言模型的羽毛球比赛战术分析与预测........................................................... 45
5.21 基于立体手势信息的人体身份识别*..........................................................................................46
5.22 基于文本的手-物体交互动作序列生成...................................................................................... 48
5.23 中文唇语识别................................................................................................................................ 49
5.24 非受限条件下的跌倒检测............................................................................................................50
5.25 无人机关键部位识别（Altlas200 模块部署*）.........................................................................51
5.26 雷达图像强点检测（Altlas200 模块部署*）.............................................................................53
5.27 红外图像车辆检测（Arm/Atlas200 模型移植*）......................................................................54
5.28 无人机航拍图像拼接....................................................................................................................55
5.29 红外图像样本生成........................................................................................................................56
5.30 基于深度学习的遥感图像变化检测............................................................................................58
5.31 基于深度学习的模糊图像复原....................................................................................................59
5.32 基于深度学习的图像匹配（Arm/Atlas200 模型移植*）......................................................... 60
5.33 海底目标检测与识别....................................................................................................................61
5.34 遥感图像中的目标检测................................................................................................................63
5.35 密集场景下的行人跟踪................................................................................................................65
5.36 基于轻量化网络的麦穗目标检测................................................................................................67
2
5.37 物体 6D 姿态估计......................................................................................................................... 69
5.38 人脸关键点检测............................................................................................................................71
5.39 工业品表面缺陷检测....................................................................................................................72
5.40 无人机视角下的目标检测与识别................................................................................................73
5.41 基于深度学习的鸟声识别............................................................................................................75
5.42 视频序列中人脸微表情识别........................................................................................................76
5.43 基于视觉的垃圾检测分类............................................................................................................77
5.44 基于图像的饮食营养评估............................................................................................................79
5.45 基于图像的植物叶片病害识别轻量化方法................................................................................80
5.46 基于结构信息挖掘的鸟类识别....................................................................................................82
5.47 基于迁移学习的花卉识别............................................................................................................84
5.48 花粉识别方法对比分析................................................................................................................86
5.49 有限样本条件三维场景重建........................................................................................................87
5.50 动态场景重建-分割-理解一体化方法.........................................................................................89
5.51 伪装目标检测................................................................................................................................ 91
5.52 增量学习目标检测........................................................................................................................93
5.53 红外弱小目标检测........................................................................................................................95
5.54 小样本目标检测............................................................................................................................97
5.55 旋转目标检测................................................................................................................................ 98
5.56 人-物-交互检测........................................................................................................................... 100
5.57 高分辨率时-空-谱融合计算成像............................................................................................... 103
5.58 低光图像饱和去模糊..................................................................................................................105
5.59 基于语言-视觉模型的图像复原方法........................................................................................ 106
5.60 事件相机高速高动态成像..........................................................................................................108
5.61 远距高清晰成像.......................................................................................................................... 110
5.62 远距透雾成像.............................................................................................................................. 111
5.63 雨雪雾霾恶劣天气图像统一增强方法......................................................................................113
5.64 零样本单目度量深度估计..........................................................................................................115
5.65 基于单细胞转录组数据的细胞状态转换分析..........................................................................117
5.66 面向系统状态时序数据的状态转变临界预警信号识别..........................................................118
5.67 面向网络瓦解的节点重要性评估..............................................................................................120
5.68 网络韧性推断.............................................................................................................................. 122
5.69 药物-靶标相互作用预测.............................................................................................................123
5.70 药物组合相互作用预测..............................................................................................................124
5.71 药物重定位.................................................................................................................................. 125
5.72 疾病-microRNA 关联预测..........................................................................................................126
5.73 真实环境海面目标检测..............................................................................................................127
5.74 智能水下感知.............................................................................................................................. 129
5.75 心电可解释性分析......................................................................................................................130
5.76 左心室肥大二分类检测..............................................................................................................131
5.77 3D Match 数据集点云配准..........................................................................................................132
5.78 Modelnet40 数据集点云分类...................................................................................................... 134
5.79 铣削加工刀具状态监测..............................................................................................................135
5.80 复杂曲面加工质量预测..............................................................................................................136
5.81 自主竞赛选题.............................................................................................................................. 137
3
六、资源与设备使用................................................................................................................................... 138
6.1 Atlas200 硬件...............................................................................................................................138
6.2 算力服务器.................................................................................................................................... 138
七、课程设计的考查................................................................................................................................... 139
7.1 考核方式......................................................................................................................................... 139
7.2 成绩评定......................................................................................................................................... 139
八、课程设计报告撰写要求.......................................................................................................................140
1
一、课程设计目的
《模式识别与机器学习》课程设计作为独立的教学环节，是模式识别与智能系统专
业集中实践性环节系列之一，是学习完《模式识别》、《模式识别与机器学习》课程后
进行的一次全面的综合练习。其目的在于加深对模式识别与机器学习基础理论和基本知
识的理解，掌握使用特征分析、特征提取及模式辨识的基本方法，提高解决实际问题、
开发图像自动识别系统的实践能力。同时课程设计应充分体现“教师指导下的以学生为
中心”的教学模式，以学生为认知主体，充分调动学生的积极性和能动性，重视学生自
学能力的培养。 二、课程设计选题
本课程设计可供参考选择的课题如下：
1. 遮挡条件下遥感图像中的飞机目标检测
2. 蜜蜂视频检测与跟踪
3. 鲁棒的深度检测网络对抗图像生成
4. 伪装目标检测与识别
5. 微小目标检测
6. 跨域少样本目标识别
7. 噪声条件下的心脏音频数据识别
8. 无人机视角下的车辆目标检测
9. 植物根系图像抠图与表型提取
10.密集人群运动分析
11.密集视频行人流量计数
12.全球麦穗分割挑战赛
2
13.面向视觉 ViT 模型的通用特征上采样
14.品种无关的植物计数
15.模特虚拟街拍合成
16.虚拟试衣
17.三维视线估计
18.基于 RGB 图像的三维交互手势估计
19.非受限条件下的人体眨眼检测
20.基于视觉和大语言模型的羽毛球比赛战术分析与预测
21.基于立体手势信息的人体身份识别* 22.基于文本的手-物体交互动作序列生成
23.中文唇语识别
24.非受限条件下的跌倒检测
25.无人机关键部件识别* 26.雷达图像强点检测* 27.复杂场景红外弱小目标检测
28.无人机航拍图像拼接
29.红外图像样本生成
30.基于深度学习的遥感图像变化检测
31.目标识别对抗样本生成
32.图像线特征提取与匹配
33.海底目标检测与识别
34.遥感图像中的目标检测
35. 密集场景下的行人跟踪
3
36.基于轻量化网络的麦穗检测
37.物体 6D 姿态估计
38.人脸关键点检测
39.工业品表面缺陷检测
40.无人机视角下的目标检测与识别
41.基于深度学习的鸟声识别
42.视频序列中人脸微表情识别
43.基于视觉的垃圾检测分类
44.基于图像的饮食营养评估
45.基于图像的植物叶片病害识别轻量化方法
46.基于结构信息挖掘的鸟类识别
47.基于迁移学习的花卉识别
48.花粉识别方法对比分析
49.有限样本条件三维场景重建
50.动态场景重建-分割-理解一体化方法
51.伪装目标检测
52.增量学习目标检测
53.红外弱小目标检测
54.小样本目标检测
55.旋转目标检测
56.人-物-交互检测
57.高分辨率时-空-谱融合计算成像
58.低光图像饱和去模糊
4
59.基于语言-视觉模型的图像复原方法
60.事件相机高速高动态成像
61.远距高清晰成像
62.远距透雾成像
63.雨雪雾霾恶劣天气图像统一增强方法
64.零样本单目度量深度估计
65.基于单细胞转录组数据的细胞状态转换分析
66.面向系统状态时序数据的状态转变临界预警信号识别
67.面向网络瓦解的节点重要性评估
68.网络韧性推断
69.药物-靶标相互作用预测
70.药物组合相互作用预测
71.药物重定位
72.疾病-microRNA 关联预测
73.真实环境海面目标检测
74.智能水下感知
75.心电可解释性分析
76.左心室肥大二分类检测
77.3D Match 数据集点云配准
78.Modelnet40 数据集点云分类
79.铣削加工刀具状态监测
80.复杂曲面加工质量预测
81.自主竞赛选题
5
三、总体设计要求
模式识别课程设计由于涉及到的内容较多，工作量大，根据课程本身的特点，特提
出如下要求：
1．课程设计需要在课程内容讲完后给学生布置，让学生结合课堂讲授内容，并在教师
的具体指导下，逐步开始分析工作。
2．学生开展课程设计按 5 人一组进行分组，并确定一个组长，明确组员分工与协调。
3．各组在分工的前提下制定相应的任务完成计划，并按计划开展课程设计，接受教师
检查。
4．课程设计的题目的选择可根据学生的自身理论学习体会和研究兴趣，结合实际或熟
悉的课题，体现“麻雀虽小、五脏俱全”，充分练习模式识别的各个方面的内容。
5．课程设计必须完成对模式识别与机器学习处理系统的分析与设计任务，编写相应的
分析与设计报告。实施部分的内容可根据小组的实际能力决定取舍。 四、课程具体步骤与时间安排
本课程设计历时 10 周，学分 2 分。
具体步骤安排如下：
1. 课题选取，选取 1 个题目，并进行系统调研、资料整理。
2．收集、整理、学习与设计相关的技术资料。
3．进行相关选题的特征分析/模式分析，研究并设计实现任务的算法。可以参考复现文
献中别人的研究成果，如能提出自己的见解和改进则更好。
4. 算法程序的设计和调试。
5. 按照进度对数据库数据进行实验，认真进行设计相关内容的记录,试验结果分析。试
验结果如不能满足课题要求,需进行试验设计调整或算法调整,再次试验,直至试验
结果满足课题要求,或进行不同算法试验结果比对与分析。
6．完成课程设计报告。这是出成果的阶段，要认真整理前面各阶段的成果，包括完整
的研究过程，实验中的失败与经验教训等，可以一一誊写在设计报告中。报告要求
6
文字通顺、计算准确、画图清晰整洁。注意按统一规定格式和封面，打印、装订成
册。工作量大、时间紧，需要付出一定的辛苦。
时间安排及方式(每 5 人一组)
1．课程设计任务书的布置，讲解（0.5 天）
2．学生根据任务书的要求初步进行需求分析（第 7-8 周）
3．进行方案设计，并撰写设计方案（第 9-10 周）
4．指导老师审阅方案设计报告，根据意见修改设计方案（第 11 周）
5．算法设计与实现（第 12-13 周）
6．软件实现与算法测试（第 14-15 周）
7．撰写课程设计报告（第 16 周）
8．考核答辩（第 17 周）
五、模式识别课程设计内容及具体要求
5.1 遮挡条件下遥感图像中的飞机目标检测
任务描述：用高斯光斑模拟遥感图像中的飞机目标的部分遮挡场景，构建遮挡场景测试
数据集，完成遮挡条件下遥感图像中飞机目标的自动检测，并给出目标中心（以原始无
遮挡数据集中目标斜框中心为参考标注）。分析遮挡程度（可自行设计遮挡尺度指标）、
目标尺度等因素对目标检测结果的影响。
自动检测，标记出其具体区域。注意检测结果采用倾斜目标范围框(回归)的方法，如下
图所示。
数据说明：无遮挡原始数据集采用 DOTA 数据集，选取其中飞机目标进行实验，数据集
下载地址：http://captain.whu.edu.cn/DOTAweb/。
7
原图 遮挡 1
8
检测结果
遮挡条件下飞机目标检测结果示意图
参考文献：
[1] You Only Look Twice—Multi-Scale Object Detection in Satellite Imagery With
Convolutional Neural Networks (Part I),Adam Van Etten,2017. [2] Lin Na, Feng Lirong, Zhang Xiaoqing. Aircraft Detection in Remote Sensing Image based
on Optimized Faster-RCNN. Remote Sensing Technology and Application [J], 2021, 36(2):
275-284 doi:10.11873/j.issn.1004-0323.2021.2.0275
[3] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: A largescale dataset for object detection in aerial
images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3974–3983, 2018
指导老师:胡静,by6040130@163.com，qq：413953381
5.2 蜜蜂视频检测与跟踪
任务描述：实现视频图像中的蜜蜂实时检测与跟踪。视频中有空中的蜜蜂飞行、蜂
巢内的蜜蜂爬行。在多目标跟踪（Multiple Object Tracking，MOT）领域，基于检测的
跟踪（Tracking-by-detection）是主流方案之一，其先对视频序列的每一帧进行目标检测，
将跟踪问题转化为前后两帧之间的目标关联问题，通过 IoU、外观等构建相似度矩阵，
并通过匈牙利算法、贪婪算法等数据关联方法对矩阵求解。随着近年来图神经网络
（Graph Neural Networks，GNN）的发展，许多研究使用 GNN 来解决 MOT 问题，取得
了较好的效果。因此可以考虑利用 GNN 来对二维或三维空间中的蜂群目标进行关联。
由于群目标外观特征的相似性，需要寻找更有效的特征如姿态特征来构建网络图。建议
考虑实时检测跟踪的能力，优化计算复杂度。
9
飞行蜜蜂跟踪结果示意图
蜂巢内蜜蜂跟踪结果示意图
数据说明：
二维蜂巢数据集 https://groups.oist.jp/bptu/honeybee-tracking-dataset
10
蜂群飞行数据集：科技楼十楼找陈逸凡拷贝。联系方式: 微信 chenyf42
参考文献
[1] Weng X, Wang Y, Man Y, et al. Gnn3dmot: Graph neural network for 3d multi-object
tracking with 2d-3d multi-feature learning[C]//Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2020: 6499-6508. [2] Brasó G, Leal-Taixé L. Learning a neural solver for multiple object
tracking[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2020: 6247-6257. [3] Jiang X, Li P, Li Y, et al. Graph neural based end-to-end data association framework for
online multiple-object tracking[J]. arXiv preprint arXiv:1907.05315, 2019. [4] Bozek K, Hebert L, Mikheyev A S, et al. Towards dense object tracking in a 2D honeybee
hive[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4185-4193. [5] Bjerge K, Mann H M R, Høye T T. Real ‐ time insect tracking and monitoring with
computer vision and deep learning[J]. Remote Sensing in Ecology and Conservation, 2022, 8(3): 315-327. [6] Bjerge K, Nielsen J B, Sepstrup M V, et al. An automated light trap to monitor moths
(Lepidoptera) using computer vision-based tracking and deep learning[J]. Sensors, 2021, 21(2): 343. [7] Sun C, Gaydecki P. A visual tracking system for honey bee (hymenoptera: Apidae) 3D
flight trajectory reconstruction and analysis[J]. Journal of Insect Science, 2021, 21(2): 17. [8] L. Rice, S. Tate, D. Farynyk, J. Sun, G. Chism, D. Charbonneau, T. Fasciano, A. Dornhaus, M. C. Shin, “ABCTracker: an easy-to-use, cloud-based application for tracking multiple
objects,” Computer Vision and Pattern Recognition, 2020. [9] K. Bozek, L. Hebert, Y. Portugal, A. S Mikheyev, G. J Stephens, “Markerless tracking of
an entire honey bee colony,” Nature communications, vol. 12,1 1733. 19 Mar. 2021. [10] K. Bozek, L. Hebert, A. S Mikheyev, G. J Stephens, “Towards Dense Object Tracking in
a 2D Honeybee Hive,” Computer Vision and Pattern Recognition, 2018.
11
[11]Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with
region proposal networks[J]. Advances in neural information processing systems, 2015, 28. [12]Bewley A, Ge Z, Ott L, et al. Simple online and realtime tracking[C]//2016 IEEE
international conference on image processing (ICIP). IEEE, 2016: 3464-3468. [13]Yang C, Collins J. A model for honey bee tracking on 2D video[C]//2015 International
conference on image and vision computing New Zealand (IVCNZ). IEEE, 2015: 1-6. [14]L. Leal-Taix´e, A. Milan, I. Reid, S. Roth, and K. Schindler, “MOTChallenge 2015:
Towards a Benchmark for Multi-Target Tracking,” arXiv preprint, 2015. 指导老师:胡静,by6040130@163.com，qq：413953381
5.3 鲁棒的深度检测网络对抗图像生成
任务描述：针对典型深度网络目标检测算法，学习对抗数码图像生成技术，考虑真实世
界中的成像条件的对抗不可控性，针对不同观测角度、不同光照条件、不同观测距离等
成像条件，研究鲁棒的对抗数码生成方法。
加干扰 patch 实验结果图
12
数据集：DOTA 数据集，数据集下载地址：http://captain.whu.edu.cn/DOTAweb/。
参考文献
[1] Goodfellow I J, Shlens J, Szegedy C. Explaining and harnessing adversarial examples[J]. arXiv preprint arXiv:1412.6572, 2014. [2] Liu X, Yang H, Liu Z, et al. Dpatch: An adversarial patch attack on object detectors[J]. arXiv preprint arXiv:1806.02299, 2018. [3] Brown T B, Mané D, Roy A, et al. Adversarial patch[J]. arXiv preprint arXiv:1712.09665, 2017. [4] Huang L, Gao C, Zhou Y, et al. Universal physical camouflage attacks on object
detectors[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2020: 720-729. [5] Suryanto N, Kim Y, Kang H, et al. Dta: Physical camouflage attacks using differentiable
transformation network[C]//Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2022: 15305-15314. 5.4 伪装目标检测与识别
任务描述：实现静态图像中伪装目标的检测与识别。
数据集：
1、CAMO:
CAMO 包含 1250 张图片（训练集 1000 张，测试集 250 张）。CAMO 数据集可以粗
略地分为自然伪装和人工伪装两种情况。自然伪装动物包括两栖动物、鸟类、昆虫、哺
乳动物、爬行动物和各种环境中的水下动物。自然伪装背景包括水下、沙漠、森林、山
和雪等复杂的场景。而人工伪装则包括人体绘画艺术和军事伪装迷彩等。
项目页面:https://sites.google.com/view/ltnghia/research/camo
数据集:https://drive.google.com/open?id=1hOqZdwkuPhBvGcVAwmh0f1NGqlH_4B6
13
IM
A
G
E
G
T
CAMO 数据集及其 GT 标注示例
2、CHAMELEON：
CHAMELEON 仅包含 76 张图像，手工标注了对象级的真值图（GTs）。这些图像是
以“伪装的动物”为关键字，通过谷歌搜索引擎收集的。因为数据集图片数量较少，一般
作为测试集。
项目页面：http://kgwisc.aei.polsl.pl/index.php/en/dataset/63-animal-camouflage-analysis
数据集：
http://kgwisc.aei.polsl.pl/datasets/CamouflageBase/animals.7z
http://kgwisc.aei.polsl.pl/datasets/CamouflageBase/masks.7z
IM
A
G
E
14
G
T
CHAMELEON 数据集及其 GT 标注示例
3、COD10K：
包含 10,000 张图片(5,066 张伪装图片，3,000 张背景图片，1,934 张非伪装图片)，分
为 10 个超级类，以及 78 个子类(69 伪装，9 非伪装)，其中伪装图像 3040 张作为训练集，
2026 张作为测试集。这些图像来源于多个摄影学网站,主要是 Flickr 网站。为了避免选
择偏差，还从 Flickr 上收集了 3000 张显著图像。为了进一步丰富负样本，又从互联网
上选取了 1934 幅非伪装图像，包括森林、雪、草原、天空、海水等类别的背景场景。
最新更新的 v3 版本除了对象基本的分割标注，还提供了类别标签、实例级别的标注以
及 coco 格式的检测框和分割标注，可以进一步作为检测和识别的数据集。
项目页面：http://dpfan.net/Camouflage/
数据集：https://drive.google.com/file/d/1vRYAie0JcNStcSwagmCq55eirGyMYGm5/view
IM
A
G
E IM
A
G
E
IN
F
O
图 片 名 称 为
COD10K-CAM-1-Aquatic-2-ClownFish-16
从左到右分别代表 COD10K 数据集，伪
装图片，超类编号 1，超类类别 Aquatic，
子类编号 12，子类类别 ClownFish，图片
编号 16
图片名称为
COD10K-CAM-2-Terrestrial-34-Human-2015
从左到右分别代表 COD10K 数据集，伪装
图片，超类编号 2，超类类别 Terrestrial，子
类编号 34，子类类别 Human，图片编号 2015
15
G
T
_
O
bject G
T
_Instance C
O
C
O
格
式
标
注
可
视
化
（超
类）
COD10K 数据集及标注示例
参考文献：
[1] 张立新.基于自然背景的数码迷彩设计及伪装效果评价[J]. 西安工业大学学报，
2019，39（3）：358. [2] 武国晶，吕绪良，邢海宁，等.三维凸面分析法在迷彩伪装检测中的应用[J]. 解放
军理工大学学报（自然科学版），2015，16（6）：582-586. [3] Fan, Deng-Ping, et al. "Concealed object detection." IEEE transactions on pattern
analysis and machine intelligence 44.10 (2021): 6024-6042. [4] Pei, Jialun, et al. "Osformer: One-stage camouflaged instance segmentation with
transformers." European Conference on Computer Vision. Cham: Springer Nature
Switzerland, 2022. [5] Fan, Deng-Ping, et al. "Camouflaged object detection." Proceedings of the IEEE/CVF
16
conference on computer vision and pattern recognition. 2020. [6] 邓小桐,曹铁勇,方正,郑云飞.改进 RetinaNet 的伪装人员检测方法研究[J].计算机
工程与应用,2021,57(05):190-196. [7] 吴涛,王伦文,朱敬成.基于迁移学习和注意力机制的伪装图像分割[J].系统工程与
电子技术,2022,44(02):376-384. [8] DONG B， ZHUGE M， WANG Y， et al. Towards accurate camouflaged object
detection with mixture convolution and Interactive Fusion
［DB/OL］.https//arxiv.org/abs/2101.05687，2021. [9] Le, Trung-Nghia, et al. "Camouflaged instance segmentation in-the-wild: Dataset, method, and benchmark suite." IEEE Transactions on Image Processing 31 (2021):
287-300. [10]Le T N, Nguyen T V, Nie Z, et al. Anabranch network for camouflaged object
segmentation[J]. Computer vision and image understanding, 2019, 184: 45-56. 指导老师:胡静,by6040130@163.com，qq：413953381
5.5 微小目标检测
任务描述：微小目标通常指占用像素面积小于或等于 16*16 像素的目标，其通常尺寸极
小、视觉信息极为匮乏，并且易受背景和噪声的干扰。本课题要求对微小目标检测进行
研究，提升微小目标的检测性能。
数据说明：
𝑫𝑶_{𝒎𝒎𝒏}：在 13386 张图像中提供了 410305 个实例，包含 10 类目标，飞机、
小车、大车、船、网球场、棒球场、游泳池、篮球场、储油罐和直升飞机。其中，训练
集有 10653 张图像，328086 个实例，测试集有 2733 张图像，82219 个实例。
数据集𝑫𝑶_{𝒎𝒎𝒏}中的各类目标的标注框示例
AI-TOD：在 28036 张航拍图像中提供了 700621 个实例，包含飞机、桥、油罐、船、游泳池、
17
车辆、人和风车。经统计，AITOD 中的目标平均尺寸为 12.8 像素。 数据集 AI-TOD 中的各类目标的标注框示例
数据库下载联系科技楼 10 楼 1008 石梓灿，微信：shizi-can。
一些可供参考的数据集：
SODA 数据集：https://shaunyuan22.github.io/SODA/
参考文献：
[1]. G. Cheng, X. Yuan, X. Yao, K. Yan, Q. Zeng, X. Xie, et al. Towards Large-Scale Small
Object Detection: Survey and Benchmarks. IEEE Transactions on Pattern Analysis and
Machine Intelligence. 2023, 45(11):13467-13488. [2]. J. Wang, W. Yang, H. Guo, R. Zhang, G.-S. Xia. Tiny object detection in aerial images. in: Proceedings of the International Conference on Pattern Recognition (ICPR). Virtual, originally scheduled in Milan, Italy, January 10–15, 2021, IEEE, 2021: 3791–3798. [3]. C. Xu, J. Wang, W. Yang, H. Yu, L. Yu, G.-S. Xia. RFLA: Gaussian receptive field
based label assignment for tiny object detection. in: Proceedings of the European
conference on computer vision (ECCV). Tel Aviv, Israel, October 23–27, 2022, Springer, 2022: 526–543. [4]. C. Xu, J. Wang, W. Yang, H. Yu, L. Yu, G.-S. Xia. Detecting tiny objects in aerial
images: A normalized Wasserstein distance and a new benchmark. ISPRS Journal of
Photogrammetry and Remote Sensing. 2022, 190:79–93. [5]. Zican Shi, Jing Hu, Jie Ren, Hengkang Ye, Xuyang Yuan, Yan Ouyang, Jia He, Bo Ji, Junyu Guo. HS-FPN: High Frequency and Spatial Perception FPN for Tiny Object
Detection. in: Proceedings of the 39th Annual AAAI Conference on Artificial
Intelligence (AAAI). Philadelphia, USA, February 25–March 4, 2025, AAAI Press, 2025:6896–6904. [6]. T. Zhang, X. Zhang, X. Zhu, G. Wang, X. Han, X. Tang, et al. Multistage enhancement
18
network for tiny object detection in remote sensing images. IEEE Transactions on
Geoscience and Remote Sensing. 2024, 62:1–12. 指导老师:胡静,by6040130@163.com，qq：413953381
5.6 跨域少样本目标识别
任务描述： 1）首先通过对以下域适应数据集抽样，从而构建少样本目标域；
2）使用非自然数据（如 officehome 的 Art，Cl 域）进行预训练，对真实场景成像的小样
本图像(如 OfficeHome 的 Rw，Pr 域)进行识别分类；3）使用真实场景进行预训练，对非
自然场景的小样本图像进行识别分类。
数据说明：Office31，office-home，CUFSF
进阶拓展：
1） 以上三个数据集都是源域和目标域类别空间相同的闭集问题，在实际的不同成像
方式之间的域适应识别任务场景中，可能会存在目标域和源域类别无法一一对应
的情况。应该考虑类别空间有重合但不完全相同的域适应识别问题，例如将域适
应少样本方法和一般跨域少样本方法结合起来，利用部分类别获得领域之间的公
共分类特征，进而扩展到非公共的目标域类别上。
2） 一般来说，标签无误的样本数量越多，对模型的训练越有利，但由于域适应问题
中源域和目标域的样本存在着分布差异，一些域差异较大的源域样本的引入可能
会对模型有负面影响。考虑进一步研究多源域的域适应识别方法中不同源域对模
型在目标域上效果的影响。
19
域适应目标识别数据集 Office31
Office31 数据集包括了三个域，分别是亚马逊商城（在线电商）图片，单反相
机拍摄图片，网络摄像头拍摄图片，包括了 31 类数据。
下载地址：https://faculty.cc.gatech.edu/~judy/domainadapt/
数据集 office-home
Office-home 是域适应的一个基准数据集，包含了 4 个不同域，每个域由 65 个类别
组成，分别是艺术——素描、绘画、装饰等形式的艺术形象、剪贴画——剪贴画图像的
集合、产品——没有背景的对象图像和真实世界——使用常规相机拍摄的对象图像。它
包含 15500 个图像，每个类平均约 70 个图像，一个类最多 99 个图像。
下载地址：https://www.hemanthdv.org/officeHomeDataset.html
20
数据集 CUFSF
CUFSF 数据集是一个中大人脸素描数据集，也可以用于人工图像和真实图像的域适
应少样本目标识别研究，这个数据集包括了 1194 个人的照片，以及一张由艺术家观看
照片时绘制的带有形状夸张的草图。
下载地址：http://mmlab.ie.cuhk.edu.hk/archive/cufsf/
参考文献：
[1] O. Vinyals, C. Blundell, T. Lillicrap, et al. Matching networks for one shot learning. Advances in neural information processing systems, 2016, 29
[2] X. He, Y. Peng. Fine-grained visual-textual representation learning. IEEE Transactions
on Circuits and Systems for Video Technology, 2019, 30(2): 520-531
[3] Y. Guo, N. C. Codella, L. Karlinsky, et al. A broader study of cross-domain few-shot
learning. in: Proceedings of the European conference on computer vision, Glasgow, UK, 2020. Springer, 2020: 124-141
[4] A. Krizhevsky, I. Sutskever, G. E. Hinton. Imagenet classification with deep
convolutional neural networks. Advances in neural information processing systems, 2012, 25
[5] A. Nakamura, T. Harada. Revisiting fine-tuning for few-shot learning. arXiv preprint
arXiv:1910.00216, 2019
[6] G. Koch, R. Zemel, R. Salakhutdinov, et al. Siamese neural networks for one-shot
image recognition. in: Proceedings of the International conference on machine learning, Lille, France. ACM, 2015
[7] J. Snell, K. Swersky, R. Zemel. Prototypical networks for few-shot learning. Advances
in neural information processing systems, 2017, 30
21
[8] P. Wang, L. Liu, C. Shen, et al. Multi-attention network for one shot learning. in:
Proceedings of the IEEE conference on computer vision and pattern recognition, Honolulu, HI, USA,2017. IEEE, 2017: 2721-2729
[9] B. Hariharan, R. Girshick. Low-shot visual recognition by shrinking and hallucinating
features. in: Proceedings of the IEEE international conference on computer vision, Venice, Italy,2017. IEEE, 2017: 3018-3027
[10] Y. X. Wang, R. Girshick, M. Hebert, et al. Low-shot learning from imaginary data. in:
Proceedings of the IEEE conference on computer vision and pattern recognition, Salt
Lake City, UT, USA, 2018. IEEE, 2018: 7278-7286
[11] E. Schwartz, L. Karlinsky, J. Shtok, et al. Delta-encoder: an effective sample synthesis
method for few-shot object recognition. Advances in neural information processing
systems, 2018, 31
[12] S. Ravi, H. Larochelle. Optimization as a model for few-shot learning. in: Proceedings
of the International conference on learning representations, San Juan, Puerto Rico,2016. Ithaca, 2016
[13] C. Finn, P. Abbeel, S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. in: Proceedings of the International conference on machine learning, Sydney, Australia,2017. PMLR, 2017: 1126-1135
[14] Q. Sun, Y. Liu, T.-S. Chua, et al. Meta-transfer learning for few-shot learning. in:
Proceedings of the IEEE conference on computer vision and pattern recognition, Long
Beach, CA, USA,2019. IEEE, 2019: 403-412
[15] Y. Liu, Q. Sun, A.-A. Liu, et al. LCC: learning to customize and combine neural
networks for few-shot learning. arXiv preprint arXiv:1904.08479, 2019
指导老师:胡静,by6040130@163.com，qq：413953381
5.7 噪声条件下的心脏音频数据识别
任务描述：对从各种临床或非临床（例如家庭访问）环境中收集的心音记录进行分类。
目的是对单个心前区的单个记录进行识别，将其划分为正常、主动脉瓣狭窄、二尖瓣狭
窄、二尖瓣反流、二尖瓣脱垂五类。在实验的过程中，需要先对公开的心音数据集进行
22
整理，从中选取符合上述五类的心脏音频构造初始数据集。同时，考虑到心音信号作为
人体微弱的生物信号的特性，在人体这样一个充斥着众多噪音的环境中，直接采集到的
心音信号往往包含有各种噪音。因此在实验中需要对原始的心音数据进行去噪处理，去
除心脏部分之外产生的杂音。这里除了常用的心音去噪方法外，也可以观察心音的可视
化图像，结合心音的特点，设计更有针对性的去噪方法。此外，若数据集中记录的心音
数据各个类别的样本分布不均衡或时长差异较大，则需要对数量较少的类别或时间较长
的心音进行分割处理，一方面生成新样本使样本分布更加均衡；另一方面避免音频过长。
对于处理后的心音段，添加环境噪声，得到含有环境噪声的音频数据。其中环境噪声的
类别需要考虑实际情况，如衣服摩擦声、呼吸声、咳嗽声等。然后提取含噪心音在时域、
频域、统计域中的特征，设计心音的特征向量。再使用分类器对心音进行分类，将所有
的心脏音频分为上述五个类别的心音，对比各种分类器得到较好结果。其中分类器的选
择既可以考虑直接利用特征向量进行分类的，也可以考虑将特征向量进行图像化表达再
对图像分类的。分类器的对比指标除了最常见的分类准确率之外，也可以纳入 F1 分数、
混淆矩阵、计算效率等指标。最后根据心脏病病理特性给出算法改进方法。
数据描述：
下面给出三个公开数据集的下载链接及其数据描述，可以在此基础上构建包含正常、主
动脉瓣狭窄、二尖瓣狭窄、二尖瓣反流、二尖瓣脱垂五个类别的基础数据集。当然也可
以寻找其他公开可用的、符合课题要求的数据。
2016 年 PhysioNet / CinC 心音分类挑战赛开源的数据集。这些心音以 44kHz 的采样率记
录下来，然后向下采样到 2000Hz。
下载地址：https://www.physionet.org/content/challenge-2016/1.0.0/
表 1 PhysioNet / CinC 心音分类挑战赛数据集总览
数据集名称 异常心音 正常心音 总计
数据集 A 292 117 409
数据集 B 104 386 490
数据集 C 27 7 31
数据集 D 28 27 55
数据集 E 183 1958 2141
数据集 F 34 80 114
总计 665 2575 3240
23
每个数据集都来自不同的医疗机构或家庭，无论是在临床还是非临床环境中，都从健康
受试者和病理患者那里收集到。挑战赛的数据集由六个数据库（A 到 F）组成，总共包
含 3240 个心音记录，持续时间从 5 秒到刚超过 120 秒。每个记录中的平均心脏周期长
度为 1.5 秒至 2 秒。整个数据集有大约 80000 个心脏周期。
Yaseen[2018]提供的数据集。
下载地址：
https://link.zhihu.com/?target=https%3A//github.com/yaseen21khan/Classification-of-Heart-S
ound-Signal-Using-Multiple-Features- 表 2 Yaseen[2018]数据集总览
文件夹 心音状态 音频数量
N 正常 200
AS 主动脉瓣狭窄 200
MS 二尖瓣狭窄 200
MR 二尖瓣反流 200
MVP 二尖瓣脱垂 200
其中的心音数据包含正常和异常，进一步被细分为 5 个类别，一类正常（N）和四类异
常。四类异常分别为:主动脉瓣狭窄（AS）、二尖瓣狭窄（MS）、二尖瓣反流（MR）和
二尖瓣脱垂（MVP）。整个数据集中包含了 1000 个音频文件，其中每个类别有 200 个音
频文件，文件格式为.wav。
2011 年 PAsCAL chanllnge 数据集。
下载地址：
https://link.zhihu.com/?target=http%3A//www.peterjbentley.com/heartchallenge/
表 3 2011 年 PAsCAL chanllnge 数据集总览
文件夹 正常音频数 异常音频数 额外音频数 人造音频数
A 31 34 19 40
B 320 95 46 195
这些数据来源于两个不同的途径：（A）通过 iStethoscope Pro iPhone 应用程序从公
众那里收集，（B）来自医院使用数字听诊器 DigiScope 的临床试验。其中（B）中的数
据中部分存在明显的噪声。
本课题是对噪声条件下的心脏音频进行识别，因此需要对初始心音数据添加环境噪
声。考虑到实际中听诊器采集的心脏音频中含有的环境噪声类别并不是没有限制的，需
要对噪声的类别进行筛选，结合实际情况可以考虑衣服摩擦声、呼吸声、咳嗽声、走路
24
声、敲门声、鼠标点击声、键盘打字声等。
环境噪声的选择可以从公开的噪声数据集中自行选取，如 ESC-50 数据集。
下载地址：
GitHub - karolpiczak/ESC-50: ESC-50: Dataset for Environmental Sound Classification
添加了环境噪声的音频示例可以访问以下地址：
GitHub - siqilv/Cloud at master
参考文献：
[1] Early Detection of Heart Symptoms with Convolutional Neural Network and Scattering
Wavelet Transformation ， Klec´, M. 24th International Symposium, ISMIS 2018. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 11177), p 24-31, 2018
[2] Feature extraction of heart sounds using velocity and acceleration of MFCCs based on
support vector machines，Azmy, Mohamed Moustafa, v 2018-January, p 1-4, July 1, 2017, 2017 IEEE Jordan Conference on Applied Electrical Engineering and Computing
Technologies, AEECT 2017
[3] A robust correlation method to detect heterogeneous heart valve symptoms，Suboh, M.Z., Mansor, M.N.; Junoh, A.K.; Daud, W.S.W.; Muhamad, W.Z.A.W.; Idris, AIP Conference
Proceedings, v 1660, p 090053 (8 pp.), 2015
[4] Automated diagnosis of cardiac abnormalities using heart sounds，Perera, Ishanka S.;
Muthalif, Fathima A.; Selvarathnam, Mathuranthagaa; Liyanaarachchi, Madhushanka R.;
Nanayakkara, Nuwan D. PHT 2013, p 252-255, 2013, IEEE EMBS Special Topic
Conference on Point-of-Care
[5] Early detection of heart symptoms with convolutional neural network and scattering
wavelet transformation ， Kle, Mariusz, v 11177 LNAI, p 24-31, 2018, Foundations of
Intelligent Systems - 24th International Symposium, ISMIS 2018, Proceedings
[6] A non-negative matrix factorization approach based on spectro-temporal clustering to
extract heart sounds，Canadas-Quesada, F.J. ; Ruiz-Reyes, N.; Carabias-Orti, J.; Vera-Candeas, P.; Fuertes-Garcia, J. Source: Applied Acoustics, v 125, p 7-19, October 1, 2017
[7] Precision/recall trade-off analysis in abnormal/normal heart sound classification，Bopaiah, Jeevith ；Kavuluru, Ramakanth； v 10721 LNCS, p 179-194, 2017, Big Data Analytics - 5th
25
International Conference, BDA 2017, Proceedings
[8] Heart sound analysis for symptom detection and computer-aided diagnosis，Reed, T.R. ;
Reed, N.E.; Fritzson, P. Simulation Modelling Practice and Theory, v 12, n 2, p 129-46, May
2004
[9] Development of embedded stethoscope for Heart Sound，Tiwari, Hemant Kumar; Harsola, Ashish ； Proceedings of the 2016 IEEE International Conference on Wireless
Communications, Signal Processing and Networking, WiSPNET 2016, p 1547-1551, September 13, 2016, [10] Classification of heart sound based on S-Transform and neural network ， Hadi, H.M. ;Mashor, M.Y.; Suboh, MohdZubir; Mohamed, Mohamed Sapawi ；ISSPA 2010, p
189-192, 2010, [11] SepidehBabaei,AmirGeranmayeh. Heart sound reproduction based on neural network
classification of cardiac valve disorders using wavelet
[12] A. Sharma, E. Vans, D. Shigemizu, K. A. Boroevich, T. Tsunoda. DeepInsight: A
methodology to transform a non-image data to an image for convolution neural network
architecture. Scientific Reports. 2019, 9: 11399-11405
[13] Z. Wang, T. Oates. Encoding Time Series as Images for Visual Inspection and
Classification Using Tiled Convolutional Neural Networks. in: Conference on Artificial
Intelligence. Los Angeles, USA, 2015, AAAI Press, 2015, 1-7
[14] 心音（PCG）分类算法——基于深度学习-树莓派的心脏疾病听诊 - 知乎
(zhihu.com)
[15] E. A. El-Dahshan, M. Nabih-Ali, A. S. Yahia. An efficient computational approach for
phonocardiogram signals analysis and normal/abnormal heart sounds diagnosis. Arab Journal
of Nuclear Sciences and Applications. 2020, 52(3): 162-177
[16] K. K. Tseng，C. Wang，Y. Huang，G. R. Chen, K. L. Yung, W. H. Ip. Cross-domain
transfer learning for PCG diagnosis algorithm. Biosensors. 2021, 11(4): 127-140
[17] 刘美君, 吴全玉, 丁胜, 潘玲佼, 刘晓杰. 自适应噪声完备经验模态分解排列熵结
合支持向量机的心音分类方法研究. 生物医学工程学杂志. 2022, 39(2): 311-319
[18] 樊庆玲, 杨宏波, 郭涛, 张伟, 王威廉. FrFT-Bark 域特征提取与 CNN 残差收缩网络
心音分类算法. 云南大学学报:（自然科学版）. 2023, 45(3): 564-574
[19] D. B. Springer, L. Tarassenko, G. D. Clifford. Logistic regression-HSMM-based heart
26
sound segmentation. IEEE transactions on biomedical engineering. 2015, 63(4): 822-832
[20] M. Zabihi, A. B. Rad, S. Kiranyaz, M. Gabbouj, A. K. Katsaggelos. Heart sound anomaly
and quality detection using ensemble of neural networks without segmentation. in: 2016
Computing in Cardiology Conference. Vancouver, BC, Canada, 2016, IEEE, 2016, 613-616
指导老师:胡静,by6040130@163.com，qq：413953381
5.8 无人机视角下的车辆目标检测
任务描述：在无人机视角下，由于拍摄角度较高、距离目标较远，车辆目标在图像中呈
现出多尺度、多形态的特点。具体而言，任务要求实现对以下两类目标的精准检测与定
位：
（1）中大目标：例如公交车、卡车等目标，在图像中尺寸较大，特征明显，但在
一些场景下也可能因背景复杂而受到干扰。
（2）小目标：例如私家车、摩托车等目标，由于拍摄距离较远或目标本身尺寸较
小，其特征信息往往较为稀疏，容易导致漏检。
因此，本任务旨在设计一种能够兼顾不同尺度的车辆目标检测方法，使得在无人机
采集的复杂场景图像中能够同时实现大目标、小目标车辆的精准检测。系统要求具有高
检测精度和较快的实时性，同时对复杂背景和光照、天气等环境变化具有较好的鲁棒性。
图 1 VisDrone2019 数据集示例
数据描述：
VisDrone2019 数据集：基准数据集共包含 8,629 张图像，由各种无人机摄像头捕获，覆
盖范围广泛，如图 1 所示。官方提供的数据中训练集是 6,471、验证集是 548、测试集
1,610 张，该数据集共提供了以下 10 个类，分别是：‘pedestrian’, ‘people’, ‘bicycle’, ‘car’, ‘van’, ‘truck’, ‘tricycle’, ‘awning-tricycle’, ‘bus’, ‘motor’，数据集中各类目标的分布如图 2
27
所示。表 1 显示了每个类别中小、中和大目标的分布，其中，小目标占 VisDrone2019
数据集中目标总数的 60.49%，中大型目标占目标总数的 39.51%。
图 2 数据集中对象的分布：(a) 类别数量的分布；(b) 目标宽度和高度的分布；分布的集中程度用
从浅白色到深蓝色的颜色渐变表示，表示分布越来越集中。
表 1 VisDrone2019 数据集中目标尺寸分布
Category
Smal Medium Large Number Proportion(%) Number Proportion(%) Number Proportion(%)
Pedestriar 65227 82.22 13804 17.40 306 0.39
People 23497 86.84 3463 12.80 99 0.37
Bicycle 7111 67.85 3244 30.95 125 1.19
Car 69862 48.22 63079 43.54 11926 8.23
Van 10751 43.08 11517 46.15 2688 10.77
Truck 3982 30.93 6725 52.23 2168 16.84
Tricycle 2176 45.22 2425 50.39 211 4.38
Awning Tricycle 1360 41.90 1699 52.34 187 5.76
Bus 1661 28.03 3265 55.10 1000 16.87
Motor 21986 74.16 7395 25.00 266 0.90
Total 207613 60.49 116616 33.98 18976 5.53
参考文献：
[1] LI Yang, WU Lianquan, YANG Haitao, NIU Jinlin, CHU Xianteng, WANG Huapeng, ZOU Qinglong. A Small Target Detection Algorithm from UAV Perspective[J]. Infrared
Technology, 2023, 45(9): 925. [2] Zhou, S., Zhou, H. & Qian, L. A multi-scale small object detection algorithm
SMA-YOLO for UAV remote sensing images. Sci Rep 15, 9255 (2025).
28
[3] 刘宸, 李士心, 孟范润, 陈范凯. 基于深度学习的无人机目标检测研究综述[J]. 计算
机科学与应用, 2023, 13(5): 1092-1099. [4] K. Ou et al., "Drone-TOOD: A Lightweight Task-Aligned Object Detection Algorithm for
Vehicle Detection in UAV Images," in IEEE Access, vol. 12, pp. 41999-42016, 2024. [5] Qiu, Z.; Bai, H.; Chen, T. Special Vehicle Detection from UAV Perspective via
YOLO-GNS Based Deep Learning Network. Drones.2023, 7, 117. [6] Xu, H.; Zheng, W.; Liu, F.; Li, P.; Wang, R. Unmanned Aerial Vehicle Perspective Small
Target Recognition Algorithm Based on Improved YOLOv5. Remote Sens. 2023,15 , 3583. [7] D. Du et al., "VisDrone-DET2019: The Vision Meets Drone Object Detection in Image
Challenge Results," 2019 IEEE/CVF International Conference on Computer Vision Workshop
(ICCVW), Seoul, Korea (South), 2019, pp. 213-226. [8] Liu, Jianzheng, et al. "Design of UAV target detection network based on deep feature
fusion and optimization with small targets in complex contexts." Neurocomputing (2025):
130207. [9] Xue, B., Zhang, B. & Cheng, Q. Experiment study on UAV target detection algorithm
based on YOLOv8n-ACW. Sci Rep 15, 11352 (2025). [10]Sui, Jiacheng, et al. "A new algorithm for small target detection from the perspective of
unmanned aerial vehicles." Ieee Access 12 (2024): 29690-29697. 指导老师:胡静,by6040130@163.com，qq：413953381
5.9 植物根系图像抠图与表型提取
难度系数: 
任务描述:
图像抠图是底层图像处理技术之一，旨在从背景图像中精确提取感兴趣的前景，如
人像的发丝。图像抠图是许多精细图像分析的基础。植物根系的表型提取就是一类需要
图像抠图技术支撑的任务。如下图所示，植物根系结构细碎，完整提取困难，本题基于
弱监督的图像抠图技术提取植物根系结构，并根据提取结果计算若干形态表型参数。
29
植物根系图像实例
题目要求:
1）复现 Alpha-Free Matting(https://arxiv.org/abs/2408.10539)论文中表 2 的结果；
2）收集网络上公开的植物根系数据，划分训练测试集，同时完成必要的数据标注；
3）在根系数据集上评估 Alpha-Free Matting，尝试改进算法，提升抠图性能；
4）根据提取的根系结果，计算相关形态学的根系表型参数，评估 R2。
代码和文献资源:
[1] https://github.com/wchstrife/Awesome-Image-Matting
[2] https://github.com/poppuppy/alpha-free-matting
指导老师: 陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.10 密集人群运动分析
难度系数: 
任务描述：
基于视频监控的人群运动分析是公共安全管控中的一个重要环节，特别对于机场、火车
站、地铁站等人流密集的场所尤为重要。解决该任务的一个核心在于行人运动方向的精
准估计，即通过光流、运动滤波、轨迹预测等方法，对给定视频序列中行人的运动状态
（如速度、方向等）进行估计并聚类。本任务面向开阔场景中的密集人群，首先要求对
给定的 WuhanMetro 数据集进行标注和划分，并尝试用多种不同的方法作为 baseline，
30
在该数据集上进行训练与测试；根据测试结果选取适当的模型，并根据数据集的特点（如
人群密度、分布、遮挡等）对该模型进行适当的改进，对比不同 baseline 和改进算法
在多项评价指标上的表现。
人群运动分析结果（左）与光流场可视化（右）
题目要求：
1）在 WuhanMetro 数据集和公开数据集（如 TUB CrowdFlow[1]）上对比例如光流法（如
RAFT[2]）、多目标跟踪或运动滤波（如 DeepSORT[3]）、人群流计数（如 [4]）、轨迹预
测（如 SocialVAE[5]）等至少（#小组人数）的运动估计方法，且应至少包含三种不同
类型的方法（如光流法、多目标跟踪、轨迹预测三类），并在该数据集上报告评价指标
及对比结果；
2）运动方向和速度的定量分析需以 MAE、MSE 等为指标进行评估，例如预测运动方向与
实际方向误差小于 30°且预测速度与实际速度误差小于 15%即认为正确预测，反之为错
误预测；轨迹定量分析需以 ADE、FDE 等为指标，常用于轨迹分析等任务，详细见参考
文献[6]。
2）基于一个合适的运动估计模型，针对 WuhanMetro 数据集的挑战进行改进，并基于该
数据集分析人群运动，实现人群运动方向的识别效果。要求可视化不同人群运动方向的
光流向量，输出不同人群运动方向的数量并按上图的形式区分显示，并展示改进前后的
评测指标；
31
3）集成所选算法，在 WuhanMetro 数据集选取典型场景完成 demo 的制作，输出光流、
人群运动场、人群分割图等处理结果，且应尽量保证方法的实时性。
数据描述：
TUB CrowdFlow是专门面向密集人群运动分析而渲染的数据集，包含5 个场景10 个序列, 每个场景渲染两次:用静态视角和动态摄像机模拟基于无人机/无人机的监视。场景渲染
使用虚幻引擎在高清分辨率(1280x720)以 25 fps 实现，总帧数为 3200。下载链接：ht
tps://hidrive.ionos.com/lnk/LUiCHfYG
WuhanMetro 是从武汉地铁站采集的真实数据，包含了不同监控视频视角下丰富的人群运
动场景。本数据集具有良好的运动方向区分度，适合使用光流算法进行处理，该数据集
上的定量评测需要实现部分数据标注工作。本题目的最终演示测试将基于该数据，请联
系指导老师获取。
参考文献：
[1] https://github.com/tsenst/CrowdFlow
[2] Teed Z, Deng J. Raft: Recurrent all-pairs field transforms for optical flow, ECCV:
402-419,2020. [3] Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. "Simple online and realtime tracking
with a deep association metric." 2017 IEEE international conference on image processing
(ICIP). IEEE, 2017. [4] Liu, Weizhe, Mathieu Salzmann, and Pascal Fua. "Counting people by estimating people
flows." IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021):
8151-8166. [5] Xu, Pei, Jean-Bernard Hayet, and Ioannis Karamouzas. "Socialvae: Human trajectory
prediction using timewise latents." European Conference on Computer Vision. Cham:
Springer Nature Switzerland, 2022. [6] Meng, Mancheng, et al. "Forecasting human trajectory from scene history." Advances in
Neural Information Processing Systems 35 (2022): 24920-24933. 指导老师: 陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.11 密集视频行人流量计数
32
难度系数: 
任务描述：
本任务针对地铁视频监控场景统计行人流量。元旦、国庆等节假日期间，武汉江汉路、
中南路等关键地铁换乘站常出现严重拥堵情况，造成了重大的公共安全隐患，交通部门
迫切需要一个实时检测人群流量的有效手段。在计算机视觉中，研究人员已提出了大量
基于图像的人群计数与定位算法[1]。然而，流量是一段时间内某一场所中不同行人的
数量，无法仅凭计数和定位算法无法直接得到。为此，基于多目标跟踪任务的研究工作，
DRNet[2]引入了视频行人流量计数的新任务，旨在对视频序列中出现的不同行人进行计
数。现有的工作[2-5]基于行人匹配的不同方法对此进行了研究尝试,并取得了一定的成
果。然而，现有算法在密集场景上的表现不尽人意，如何在大量遮挡和复杂背景等干扰
下对密集人群的流量进行精准高效的估计成为当下亟待解决的一大难题。
视频行人流量计数任务（左）与流量标注方法（右）
题目要求：
针对 WuhanMetro 数据集，解决密集场景下的视频行人流量计数问题，具体而言：
1）选取若干具有代表性的视频片段，完成人群的流量数据标注工作，并分析所标注数
据的人群分布状况（如每帧的人群平均密度等）；
2）在 WuhanMetro 数据集和公开数据集（如 VSCrowd[6]、UAVVIC[7]）上训练和测试至
少（#小组人数）的与流量相关的算法，对比不同算法对流入、流出行人数量预测的准
确性。评价时，需以 MAE、MSE、WRAE 为评价指标，详细公式见[2]；
3）在 WuhanMetro 数据集上，针对人群密集遮挡和复杂背景干扰等问题，完成模型的改
进与微调，通过评价指标和可视化展示改进后模型在密集场景的优越性；
4）完成演示 demo。
数据描述：
33
WuhanMetro 是在武汉地铁采集的真实数据，包含了多种视频监控视角下的密集人群场
景。本数据集能有效展现当前学术上最新的算法迁移到真实场景下会面临的若干问题。
请联系指导老师获取相关数据。
参考文献：
[1] Liu, C., Lu, H., Cao, Z., & Liu, T. (2023). Point-query quadtree for crowd counting, localization, and more. In Proceedings of the IEEE/CVF International Conference on
Computer Vision (pp. 1676-1685). [2] Han, Tao, et al. "Dr. vic: Decomposition and reasoning for video individual
counting." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2022. [3] Liu, Xinyan, et al. "Weakly Supervised Video Individual Counting." Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. [4] Wan, Chang-Lin, Feng-Kai Huang, and Hong-Han Shuai. "Density-Based Flow Mask
Integration via Deformable Convolution for Video People Flux Estimation." Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024. [5] Li, Rui, et al. "Prototype-Guided Dual-Transformer Reasoning for Video Individual
Counting." Proceedings of the 32nd ACM International Conference on Multimedia. 2024. [6] https://github.com/HopLee6/VSCrowd-Dataset
[7] https://github.com/streamer-AP/CGNet
指导老师: 陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.12 全球麦穗分割挑战赛
难度系数: 
任务描述：
参加 Global Wheat Full Semantic Segmentation Competition
（https: //www.global-wheat.com/gwfss.html）
34
题目要求：
1）以 ViT-Adaptor 为分割模型基线；
2）分析分割性能的瓶颈，探索能使分性能有效提升的工程技巧；
3）从技术层面改进模型，提升分割效果；
4）力争获得名次。
指导老师：陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.13 面向视觉 ViT 模型的通用特征上采样
难度系数: 
任务描述：
当前的视觉基础模型架构 DINOv2、DiT 等多以 ViT 架构主。然而，ViT 具有固有的空间
分辨率缺陷，难以满足高分辨率输出的视觉稠密预测任务的需求。近年来，
Model-Agnostic Feature Upsampling 的概念开始兴起，如何在不改变预训练模型参数
的前提下，提升特征空间分辨率以及特征质量成为当前学界关注的重点。本题要求复现
当前若干模型无关特征上采样算子，并评估其在垂直领域图像（如农业）中的效果。
模型无关特征上采样方法的特征可视化
题目要求：
35
1）复现几种代表性的模型无关特征上采样方法，在论文所示的公开数据集中汇报相关
性能，需与公开文献中的数值相近。
2）选取（#小组人数）个下游任务，如农业、遥感、医学图像，评估相关方法在下游任
务的有效性。
参考文献：
[1] Fu, Stephanie, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, and
William T. Freeman. "Featup: A model-agnostic framework for features at any
resolution." arXiv preprint arXiv:2403.10516 (2024). [2] Suri, Saksham, et al. "Lift: A surprisingly simple lightweight feature transform for dense
vit descriptors." European Conference on Computer Vision. Cham: Springer Nature
Switzerland, 2024. [3] Ranzinger, Mike, Greg Heinrich, Pavlo Molchanov, Jan Kautz, Bryan Catanzaro, and
Andrew Tao. "FeatSharp: Your Vision Model Features, Sharper." arXiv preprint
arXiv:2502.16025 (2025). 指导老师：陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.14 品种无关的植物计数
难度系数: 
任务描述：
植物具有生物多样性，在植物分类学中，有界、门、纲、目、科、属、种几大类别
层级，实现面向不同品种的植物计数方法是植物表型的核心。然而，常规计数算法需要
预知计数对象的类别，针对该类别收集数据、标注数据、开发模型、训练模型、评估计
数结果。由于植物品种繁多，且每年还会不断选育新品种，技术上亟需一种品种无关的
植物计数方法。近年来，类别无关计数无需事先知道或识别物体的具体类别，它不依赖
于待计数对象的类别信息，而专注于学习如何计数。如下图所示，本题以植物为研究对
象，给定 RGB 的植物图像和计数对象示例（常以标注框呈现）输入，实现可适用真实开
放环境下跨尺度、跨场景、跨时空的高效精准的品种无关植物计数。
题目要求：
1）数据收集与标注：联系指导老师获取训练数据，从互联网和已有公开数据集中收集
36
并标注新的测试数据，小组每位成员需至少收集 10 个类别，每个类别至少包含 20 张图
像，数据标注方式和工具可联系指导老师。测试集需保证和训练集中的植物品种不重合，
推荐的植物品类和数据说明如下：
植物品种
中文
名
观测尺度
观测对
象
数据说明
Carica papaya
番木
瓜
近地观测
果实、
叶片
果实和叶片为主要表型特征，需高分辨
率近距离观测
Artocarpus
heterophyllus
菠萝
蜜
近 地 观
测、无人
机航拍
果实、
整株
果实需近地观测细节，整株高大适合航
拍获取全貌
Tulipa gesneriana
郁金
香
近地观测
花、叶
片
花和叶片为关键表型，需近地观测以捕
捉颜色和形态细节。
Citrus limon 柠檬
近 地 观
测、无人
机航拍
果实、
整株
果实需近地观测细节，整株可用航拍监
测生长状态
Hordeum vulgare 大麦
无人机航
拍
整株、
穗
重要谷物，整株和穗是大田作物关键特
征，航拍适合大范围群体监测
Avena sativa 燕麦
无人机航
拍
整株、
穗
主要经济作物。整株和穗为主要观测目
标，航拍可捕捉田间生长态势
Brassica oleracea
西兰
花
近地观测
花蕾、
叶片
常见蔬菜。花蕾和叶片为食用和表型重
点，需近地观测以记录细节
Spinacia oleracea 菠菜 近地观测
叶片、
整株
食用作物。叶片为主要食用部分，整株
形态需近地观测以评估生长状况
Lentinula edodes 香菇 近地观测 子实体
重要食用菌。子实体为关键特征，需近
地观测以记录形态和质地
Flammulina
velutipes
金针
菇
近地观测 子实体
常见食用菌。子实体细长，需近地观测
以捕捉细节特征
Ganoderma lucidum 灵芝 近地观测 子实体
药用真菌。子实体为药用部分，需近地
观测以记录形态和颜色变化
37
植物品种
中文
名
观测尺度
观测对
象
数据说明
Panax ginseng 人参 近地观测
根、叶
片
药用植物。根和叶片为药用与表型特
征，需近地观测以评估品质
Astragalus
membranaceus
黄芪 近地观测
根、整
株
药用植物。根为药用核心，整株形态需
近地观测以记录生长特性
近地观测：适用于需要高分辨率细节的特征，如果实（木瓜、柠檬）、花（郁金香）、叶
片（菠菜）、子实体（香菇、灵芝）。这些对象的形态、颜色或质地需近距离捕捉。
无人机航拍：适用于高大植株（如香蕉树、无花果树）或大田作物（如大麦、燕麦、玉
米），可监测整株结构或田间群体生长态势。
2）算法基准对比与改进：对比（#小组成员）个类别无关计数方法，以其中表现最优的
模型为基线，根据植物的特点改进模型，提升模型的计数精度或效率。以 MAE、rMAE、
RMSE、rRMSE、R2、FLOPS 等指标评估计数和模型性能。
3）仿照 SAM 实现一个人工交互界面，集成算法，在验收时进行 demo 演示。
SAM 交互界面：Segment Anything | Meta AI (segment-anything.com)
不同视角下拍摄的不同类别的植物图像与计数示例
参考文献：
[1] Hao Lu, Zhiguo Cao, Yang Xiao, Bohan Zhuang, and Chunhua Shen. "TasselNet: counting
maize tassels in the wild via local counts regression network", Plant Methods 13, no. 1, pp. 1-17, 2017. [2] Wang, Z., Xiao, L., Cao, Z., & Lu, H. (2023). Vision Transformer Off-the-Shelf: A
38
Surprising Baseline for Few-Shot Class-Agnostic Counting. arXiv preprint arXiv:2305.04440. [3] Ranjan, V., Sharma, U., Nguyen, T., & Hoai, M. (2021). Learning to count everything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (pp. 3394-3403). [4] Shi, M., Lu, H., Feng, C., Liu, C., & Cao, Z. (2022). Represent, compare, and learn: A
similarity-aware framework for class-agnostic counting. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (pp. 9529-9538). 指导老师：陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.15 模特虚拟街拍合成
难度系数: 
任务描述：人物背景替换技术在多媒体制作与电子商务领域具有重要的应用价值，其应
用场景涵盖影视特效制作中的绿幕合成、广告创意设计中的场景适配以及电商平台中商
品模特的动态背景生成等。该题目探究给定模特图像和背景图像进行图像合成的方法，
如下图所示。
人物背景替换示例
题目要求：
1）从互联网上收集约 2000 张在不同背景下模特图片，利用 Image-inpainting 与数据增
39
强方法合成数据集。
2）复现（#小组人数）个图像合成算法，在收集数据上进行训练，并计算衡量合成结果
真实性的相关指标。
3）选择合适的模型，实现人机交互界面，进行 demo 演示。
注意事项：本题目对算力有较高要求（需要用到 4 卡 3090），建议熟悉 PyTorch 环境配
置以及租用服务器流程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] Yang B, Gu S, Zhang B, et al. Paint by example: Exemplar-based image editing with
diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. 2023: 18381-18391. [2] Chen X, Huang L, Liu Y, et al. Anydoor: Zero-shot object-level image
customization[C]//Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. 2024: 6593-6602. [3] Song Y, Zhang Z, Lin Z, et al. Objectstitch: Object compositing with diffusion
model[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2023: 18310-18319. [4] Zhang B, Duan Y, Lan J, et al. Controlcom: Controllable image composition using
diffusion model[J]. arXiv preprint arXiv:2308.10040, 2023. 指导老师：陆昊，e-mail: hlu@hust.edu.cn， QQ：370875068
5.16 虚拟试衣
难度系数: 
任务描述:
在电子商务模式下，服装商家需要拍摄大量模特展示图来呈现商品，而消费者也期望获
得更直观的服装交互体验。为降低商家因频繁更换模特拍摄产生的高昂成本，同时提升
消费者的购物体验，虚拟试衣技术逐渐成为研究热点。该技术旨在基于给定的模特图像
和服装平铺图，生成真实自然的穿衣效果图（如下图所示）。与当前主要针对女性夏季
服装的研究不同，本题目重点关注现有虚拟试衣模型在不同季节男装上的应用效果，并
40
深入分析其存在的技术瓶颈与改进方向。
基于掩码与无需掩码的虚拟试衣示例
题目要求：
（1）从电商网站爬取春、夏、秋、冬四个季节各约 1000 对服装-模特数据。
（2）复现基于掩码的 CatVTON 模型，进行训练，并计算指标。
（3）利用要求（2）训练的 CatVTON 生成图片，合成模特图-服装-换衣模特图三元数
据集，并训练无需掩码的 CatVTON 模型。
（4）基于生成结果分析基于掩码和无需掩码的模型的优缺点。
特别注意：本题目可能需要用到 4 卡 3090，建议熟悉 Pytorch 环境配置以及租用服务器
流程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] Chong Z, Dong X, Li H, et al. CatVTON: Concatenation Is All You Need for Virtual
Try-On with Diffusion Models[C]//The Thirteenth International Conference on Learning
Representations. [2] Issenhuth T, Mary J, Calauzenes C. Do not mask what you do not need to mask: a
41
parser-free virtual try-on[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16. Springer International
Publishing, 2020: 619-635. [3] Ge Y, Song Y, Zhang R, et al. Parser-free virtual try-on via distilling appearance
flows[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2021: 8485-8493. [4] Choi S, Park S, Lee M, et al. Viton-hd: High-resolution virtual try-on via
misalignment-aware normalization[C]//Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. 2021: 14131-14140. 指导老师：陆昊，e-mail: hlu@hust.edu.cn， qq：370875068
5.17 三维视线估计
难度：☆☆☆☆
任务描述：三维视线估计的目标是从人二维的眼睛或人脸图片中推导出其三维的视线方
向。请设计算法，遵循指定数据集的基准进行训练和测试，汇报性能指标。
42
视线检测示例
参考代码：https://github.com/hysts/pytorch_mpiigaze
参考代码基于 PyTorch，具有使用基础网络对 MPIIGaze 数据集进行训练、测试以及 demo
演示的功能。
数据描述：MPIIGaze 数据集介绍及下载：
https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaz
e-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/
根据作者所提出的评价基准，汇报“Within-dataset leave-one-person-out evaluation”的性能
指标。
参考文献：
[1] Zhang X, Sugano Y, Fritz M, et al. Appearance-based gaze estimation in the wild. In IEEE
conference on computer vision and pattern recognition (CVPR). 2015: 4511-4520. [2] Zhang X, Park S, Beeler T, et al. Eth-xgaze: A large scale dataset for gaze estimation under
extreme head pose and gaze variation. In European Conference on Computer Vision (ECCV). Springer, Cham, 2020: 365-381. 指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.18 基于 RGB 图像的三维交互手势估计
难度：☆☆☆☆☆
任务描述：三维交互手部姿态估计任务是对交互中的手部关节点进行估计，而基于 RGB
图像则要求输入数据为 RGB 图像。请设计算法，遵循指定数据集的基准进行训练和测试，
43
汇报性能指标。
基于 RGB 图像的三维交互手势估计结果图
参考代码：GitHub - facebookresearch/InterHand2.6M: Official PyTorch implementation of "InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a
Single RGB Image", ECCV 2020
参考代码基于 PyTorch，具有使用基础网络对 InterHand2.6M 数据集进行训练、测试以
及 demo 演示的功能。
数据描述：
InterHand2.6M 数据集介绍及下载：
InterHand2.6M dataset | InterHand2.6M (mks0601.github.io)
注意，该数据集的总量较大，为方便实验进行，请下载以下百度网盘链接中的文件并仔
细阅读 Readme.txt 文件，使用该文件中的小型数据集进行实验。
链接：https://pan.baidu.com/s/1SSf6v_UI5OpaTeNUH_2LiQ
提取码：4z8s
根据作者所提出的评价基准，汇报 MPJPE 的性能指标。
参考文献：
[1]Moon G, Yu S I, Wen H, et al. Interhand2. 6m: A dataset and baseline for 3d interacting
hand pose estimation from a single rgb image[C]//European Conference on Computer Vision. Springer, Cham, 2020: 548-564. [2]Zimmermann C, Brox T. Learning to estimate 3d hand pose from single rgb
images[C]//Proceedings of the IEEE international conference on computer vision. 2017:
4903-4911.
44
指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.19 非受限条件下的人体眨眼检测
难度：☆☆☆☆
任务描述：眨眼为上下眼睑快速的闭合以及远离过程，直观上表现为眼部快速的睁眼- 闭眼-睁眼的状态变化（如下图所示）。本任务旨在在非受限场景下实现眨眼检测。如图
所示，在非受限场景下，人物面部属性、姿态、光照条件等方面均有巨大差异。请设计
算法，遵循指定数据集的基准进行训练和测试，汇报性能指标。
眨眼示例
非受限场景实例
参考代码：
https://github.com/thorhu/Eyeblink-in-the-wild
https://github.com/rezaghoddoosian/Early-Drowsiness-Detection
数据描述：
HUST-LEBW 数据集[1]：https://thorhu.github.io/Eyeblink-in-the-wild/
（1） 对裁剪短视频的眨眼识别：输入短视频段，预测眨眼/非眨眼的二分类任务。请
汇报 Recall, Precision 以及 F1 指标。
（2） 对未裁剪长视频的眨眼检测：输入长视频，定位每次眨眼的开始和结束帧位置。
请汇报 指标，指标计算方法请参考提供的示例代码：
链接：https://pan.baidu.com/s/1ggeZkIPPpAKgjp68SGNv2w 提取码：tkc2
参考文献：
[1] G. Hu, Y. Xiao, Z. Cao, L. Meng, Z. Fang, J. T. Zhou, and J. Yuan. Towards real-time
eyeblink detection in the wild: Dataset, theory and practices. IEEE Transactions on
Information Forensics and Security, 15:2194–2208, 2020. [2] Roberto Daza, Daniel DeAlcala, Aythami Morales, Ruben Tolosana, Ruth Cobos, and
Julian Fierrez. Alebk: Feasibility study of attention level estimation via blink detection
45
applied to e-learning. arXiv preprint arXiv:2112.09165, 2021. [3] Tereza Soukupov´a and Jan Cech. Real-time eye blink detection using facial landmarks. In
Proc. Computer Vision Winter Workshop (CVWW), pages 1–8, 2016. [4] R. Ghoddoosian, M. Galib, and V. Athitsos, “A realistic dataset and baseline temporal
model for early drowsiness detection,” in Proc. IEEE Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW), 2019, pp. 0–0
指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.20 基于视觉和大语言模型的羽毛球比赛战术分析与预测
难度：☆☆☆☆
任务描述：结合目标检测、目标跟踪、三维重建和人体骨架等算法得到的数据和战术序
列文本标签，研究大语言推理模型（例如 DeepSeek-R1）的思维链（CoT）的生成方式，
生成带 CoT 的 Prompt 数据集并微调 DeepSeek-R1 实现对羽毛球比赛中球员所采用战术
的分析与预测。
对羽毛球比赛视频运用目标检测和目标跟踪技术，获得运动员和羽毛球在 2D 图像上的
坐标，再结合三维重建算法，得到运动员和羽毛球在 3D 空间的位置和速度数据，并且
运用人体骨架算法提取运动员的姿态，将这些数据综合成 Prompt 数据集的 Question，
将未来的战术序列文本标签作为数据集的 Response，研究复杂思维链（Complex_CoT）
的生成方式，并利用整合而成的 Prompt 数据集微调 DeepSeek-R1 推理模型，实现战术
分析与预测。
(1) 数据标注（运动员、羽毛球）
下 载 labelimg ， 教 程 链 接 如 下 ：
https://blog.csdn.net/knighthood2001/article/details/125883343
标注球员 标注羽毛球
(2)运动员和羽毛球的检测与跟踪
搭建和训练目标检测与跟踪模型，实现对运动员和羽毛球的检测与跟踪：
46
(3)基于大语言推理模型实现战术分析与预测
结合三维重建算法得到运动员和羽毛球的三维坐标和速度信息，结合人体骨架算法
得到运动员的姿态信息，生成 Prompt 数据集，研究复杂思维链（Complex_CoT）的生成
方式以更好地微调 DeepSeek-R1 进行战术分析与预测。
参考文献：
[1] Gao R, Wang L. MeMOTR: Long-term memory-augmented transformer for multi-object
tracking[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 9901-9910. [2] Tian Y, Ye Q, Doermann D. Yolov12: Attention-centric real-time object detectors[J]. arXiv
preprint arXiv:2502.12524, 2025. 指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.21 基于立体手势信息的人体身份识别*
难度：☆☆☆☆
任务描述：使用深度相机采集的包含人体手势的图片序列，对人体身份进行识别。
不同人的手势拥有特定的属性，类似于人脸识别、步态识别，手势也可以作为识别人体
身份的一个重要标识。任务要求采集不同人的手势动作序列构建数据集，并设计算法识
别出手势对应的人体身份。并在嵌入式硬件系统华为 Atlas 200 上尝试运行所提出的算
法。
可以考虑使用原始深度图以及深度图的 3D 关节点坐标信息（下图）作为算法的原始输
47
入。
数据描述：
本任务的数据集需要自己构建（至少采集 30 人的样本库），建议使用 Intel R300 相机进
行深度图数据获取，组织多个同学协助数据采集。最终在构建的数据集上划分训练测试
集，并进行算法的准确率评测。
数据集的采集工作可以与张明阳（微信：wxid_yc7ayl93dkrs22）联系。
深度图手势关节点示意
参考文献：
[1] Afifi, Mahmoud. “11K Hands: Gender recognition and biometric identification using a
large dataset of hand images.” Multimedia Tools and Applications (2017): 1-20.(基于手势身
份 识 别 ， 提 供 了 RGB 的 手 势 身 份 数 据 库 ， 下 载 地 址 为 ：
https://sites.google.com/view/11khands，可以参照这个数据集来构建自己的数据集)
[2] Xiong, Fu, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong Yu, Joey Tianyi Zhou and
Junsong Yuan. “A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation
from a Single Depth Image.” ArXiv abs/1908.09999 (2019) (深度图关节点定位)
[3] 华为云，https://www.huaweicloud.com/、https://bbs.huaweicloud.com/
指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
48
5.22 基于文本的手-物体交互动作序列生成
难度：☆☆☆☆
任务描述：本任务聚焦于具身智能领域，面向机器人灵巧手自主操作任务，旨在设计人
工智能模型理解文本语义意图，并根据交互物体运动轨迹生成相应的手部操作动作序列。
如图 1 所示，本任务以自然语言任务描述和物体运动轨迹作为模型输入，生成相应的符
合语义以及物理规律的手部动作序列。任务要求设计多模态融合算法，将文本语义与物
体轨迹联合建模，生成精确抓取、操作等手部姿态参数序列，同时保证手-物体交互的
物理合理性，如提高物理稳定性、防止手-物体穿模。本任务可以采用回归、Diffusion、
VAE 以及自回归等多种类型方法，在 OAKINK2 数据集上进行实验，需在接触率(contact
ratio)、固体交叉体积(solid intersection volume)、FID、PSKL-J，MPJPE 等指标上比较不同
类型算法性能，性能指标评估参考 https://github.com/oakink/OakInk2-TaMF。
基于文本的手-物体交互动作序列生成结果
数据描述：
OAKINK2 数据集格式参考链接：https://github.com/oakink/OakInk2#dataset-format. 本任务不使用图像数据，模型输入包括文本描述、交互物体点云、交互物体 6d pose 序
列，模型输出交互手的 MANO 参数序列。
考虑到计算资源受限，本任务采用部分数据集进行实验，数据集百度网盘链接：
https://pan.baidu.com/s/11r6e0ejbcD7KperxwUeWHw?pwd=wmae，提取码: wmae
49
参考文献：
[1] Zhan, Xinyu, et al. "Oakink2: A dataset of bimanual hands-object manipulation in
complex task completion." Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 2024. (oakink/OakInk2-TaMF: [CVPR 2024] OakInk2 baseline
model: Task-aware Motion Fulfillment (TaMF) via Diffusion) (Diffusion 方法)
[2] Cha, Junuk, et al. "Text2hoi: Text-guided 3d motion generation for hand-object
interaction." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2024. (JunukCha/Text2HOI: Text2HOI: Text-guided 3D Motion Generation for
Hand-Object Interaction) (Diffusion 方法)
[3] Zhang, Jianrong, et al. "Generating human motion from textual descriptions with discrete
representations." Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2023. (Mael-zys/T2M-GPT: (CVPR 2023) Pytorch implementation of “T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete
Representations” ) (人体动作序列生成领域自回归方法)
指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.23 中文唇语识别
难度：☆☆☆☆
50
任务描述：根据提供的数据集以及参考代码，完成指定数据集上面的唇语识别任务，有
可演示的 Demo，如真实场景下识别自己说话。
参考代码：https://github.com/Fengdalu/Lipreading-DenseNet3D
参考代码给出了一个基于上述参考文献的 PyTorch 实现，阅读参考代码并进行训练，可
基于自己的硬件资源和识别效果自行决定样本大小，终得到模型以及预测结果，演示真
实场景下的唇语识别结果可容忍一定程度的错误（在该数据集下 CNN 的表现约为 37%）。
数据描述：LRW-1000 语料库是由 Yang 等人于 2019 年发表于 IEEE 自动面部识别和手
势识别国际会议。视频分辨率为高清（1920×1080）与标清（1024×576）两类。从 718 018
个样本中收集了 1 000 个不同的分类，每个分类包含一个或者多个汉字。该语料库为单
纯唇语语料库而非视听交叉语料库，其对中文唇语发音具有重要意义。提供的数据集
（LRW1000）可以从以下链接获取：
链接：https://pan.baidu.com/s/12BQ89Sj9XE276NyFrVrmCg 提取码：nfrj
文件解压后会产生下面的文件目录：
注意事项：请阅读 Data_Annotation.txt 对于各个文件的详细描述。
参考文献：S. Yang et al. LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip
Reading in the Wild. InIEEE International Conference on AutomaticFace & Gesture
Recognition，2019:1-8. 指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.24 非受限条件下的跌倒检测
难度：☆☆☆☆
任务描述：基于 RGB 图片序列，对人体跌倒行为进行检测。现有的公开跌倒行为数据
集（如 SDU Fall Dataset[1]）往往是在实验室条件下的模拟，与现实世界的跌倒行为模
式存在较大差异。任务要求搜集网络上真实的跌倒案例自建数据集，并设计算法识别出
人体跌倒行为。
数据描述：
51
本任务的数据集需要自己构建，参考数据如图所示，组织多个同学协助数据收集。最终
在构建的数据集上划分训练测试集，并进行算法的准确率评测。
人体跌倒行为 RGB 图像序列。
参考文献：
[1] Ma, X., Wang, H., Xue, B., Zhou, M., Ji, B., & Li, Y. (2014). Depth-based human fall
detection via shape features and improved extreme learning machine. IEEE journal of
biomedical and health informatics, 18(6), 1915-1922. (现有数据集参考)
[2] Wang, X., Ellul, J., & Azzopardi, G. (2020). Elderly fall detection systems: A literature
survey. Frontiers in Robotics and AI, 7, 71. (跌倒检测系统综述，了解目前主流方法和问题)
[3] Nogas, J., Khan, S. S., & Mihailidis, A. (2020). Deepfall: Non-invasive fall detection with
deep spatio-temporal convolutional autoencoders. Journal of Healthcare Informatics Research, 4(1), 50-70. (基于深度图异常检测的方法，代码地址：https://github.com/JJN123/Fall-Detection)
指导老师:肖阳,Yang_Xiao@hust.edu.cn，wx：hustcowboy
5.25 无人机关键部位识别（Altlas200 模块部署*）
任务描述：低空无人机关键部位识别。通过对训练样本图像中的无人机目标及其他背景
图像进行特征提取与分类学习，构建无人机目标分类识别模型，然后对测试图像数据中
的目标，识别选取特征点，统计识别准确率等指标。算法适应天空或建筑物等复杂背景，
完成模型在华为 Altlas200 模块上的部署与评估。
52
数据集：ImageNet，PASCALVOC，MSCOCO 等公开数据集，或自拍数据
检测预期效果如下：
无人机关键部件识别示意图
参考文献：
[1] Diogo C. Luvizon, David Picard, HediTabia, 2D/3D Pose Estimation and Action
Recognition using Multitask Deep Learning.CVPR2018
[2] MarkEveringham·LucVanGool,AndrewZisserman.ThePASCALVisualObjectClasse
s(VOC)Challenge.IJCV2009. [3] Liu,Wei,etal."Ssd:Singleshotmultiboxdetector."Europeanconferenceoncomputervisi
on.Springer,Cham,2016. [4] RenS,HeK,GirshickR,etal.Fasterr-cnn:Towardsreal-time object detection with region
proposal networks[C].NIPS,2015. [5] MarkEveringham,S.M.AliEslami,LucVanGool.ThePascalVisualObjectClassesChall
enge–aRetrospective. IJCV2015,111:98–136
指导老师：杨卫东,yangwd@hust.edu.cn，qq14223185
53
5.26 雷达图像强点检测（Altlas200 模块部署*）
任务描述：SAR 雷达成像具有全天时、全天候特点，在遥感、测绘、环境监测等领域具
有独特的优势，金属类结构在 SAR 图像中体现为强散射点，复杂场景下对其的检测难
度大。目标检测 Faster R-CNN、RetinaNet、YOLO 等在公开的可见光影像数据集上取得
不错的效果，设计能适应雷达图像的强散射点目标检测算法是本任务的重点，并完成算
法在 Altlas200 模块上的部署。
数据：标注 SAR 数据集
雷达图像小目标检测
参考文献：
[1] J. -S. Lim, M. Astrid, H. -J. Yoon and S. -I. Lee, "Small Object Detection using Context
and Attention," 2021 International Conference on Artificial Intelligence in Information
and Communication (ICAIIC), 2021, pp. 181-186, doi:
10.1109/ICAIIC51459.2021.9415217. [2] M. Özbay and M. C. Şahingil, "A fast and robust automatic object detection algorithm to
detect small objects in infrared images," 2017 25th Signal Processing and
Communications Applications Conference (SIU), 2017, pp. 1-4, doi:
10.1109/SIU.2017.7960456.
54
[3] Quantization Mimic: Towards Very Tiny CNN for Object Detection,ECCV2018
[4] DeTone, D., Malisiewicz, T., & Rabinovich, A. (2018). Superpoint: Self-supervised
interest point detection and description. In Proceedings of the IEEE conference on
computer vision and pattern recognition workshops, pp. 224–236. 指导老师：杨卫东，yangwd@hust.edu.cn，qq14223185. 5.27 复杂场景红外弱小目标检测
任务描述：红外弱小目标检测技术是反无人机探测系统的关键技术之一，对低空飞行无
人机处于云层、甚至地面复杂背景，远距离成像目标像素少，成像信杂比低、对比度低
等问题，导致目标准确检测难，虚警率高。设计能适应复杂环境下弱小红外目标检测算
法是本任务的重点。
数据集：WideIRSTD、SIRST、MWIRSTD
红外小目标检测结果
参考文献：
[1] SAEED Z, YOUSAF M H, AHMED R, et al. On-board small-scale object detection for
unmanned aerial vehicles (UAVs)[J]. Drones, 2023, 7(5): 310. [2] BISIO I, HALEEM H, GARIBOTTO C, et al. Performance evaluation and analysis of
55
drone- based vehicle detection techniques from deep learning perspective[J]. IEEE
Internet of Things Journal, 2021, 9(13): 10920-10935. [3] M. Qi et al., “FTC-Net: Fusion of transformer and CNN features for infrared small target
detection,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15, pp. 8613–8623, 2022. [4] LIN, Liangkui, WANG, et al. Using deep learning to detect small targets in infrared
oversampling images[J]. Journal of Systems Engineering & Electronics, 2018. S. Meng, C. Zhang, Q. Shi, Z. Chen, W. Hu, and F. Lu, “A robust infrared small target detection
method jointing multiple information and noise prediction: Algorithm and benchmark,”
IEEE Trans. Geosci. Remote Sens., vol. 61, 2023. [1] LIU T, DING X Y, ZHANG B B, et al. Improved YOLOv5 for remote sensing image
detection[J]. Computer Engineer-ing and Applications, 2023, 59(10): 253-261
指导老师：杨卫东,yangwd@hust.edu.cn，qq14223185
5.28 无人机航拍图像拼接
任务描述：无人机遥感技术在人们生活的许多方面发挥了重要作用，无人机可以从常人
难以企及的角度进行拍摄，但是由于获得的影像数据量大、重叠度高、覆盖范围小，需
要使用图像拼接技术才能及时获取宽视角、大氛围、高分辨率、过渡自然的无缝图片。
目前的图像拼接技术大多基于传统点特征提取算法完成，存在实时性不高，鲁棒性差的
缺点，因此，需要针对此任务，设计实现基于深度学习的无人机图像拼接算法，输入多
张无人机拍摄图片，输出一个完整、过渡自然的大幅图像。
数据：https://github.com/OpenDroneMap/ODMdata
56
输入图片及拼接后大图
参考文献：
[1] Adel, Ebtsam, Mohammed Elmogy, and Hazem Elbakry. "Real time image mosaicing
system based on feature extraction techniques." 2014 9th International Conference on
Computer Engineering & Systems (ICCES). IEEE, 2014. [2] Wang, Zhaobin, and Zekun Yang. "Review on image-stitching techniques." Multimedia
Systems 26.4 (2020): 413-430. [3] Yan, Ni, et al. "Deep learning on image stitching with multi-viewpoint images: A
survey." Neural Processing Letters 55.4 (2023): 3863-3898. [4] Chen, Jun, et al. "Drone image stitching based on compactly supported radial basis
function." IEEE Journal of Selected Topics in Applied Earth Observations and Remote
Sensing 12.11 (2019): 4634-4643. [5] Sarlin, Paul-Edouard, et al. "Superglue: Learning feature matching with graph neural
networks." Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2020. 指导老师：杨卫东，yangwd@hust.edu.cn，qq14223185. 5.29 红外图像样本生成
任务描述：红外成像具有全天时工作特点，在导航、监视等领域应用广泛，算法设计与
验证需要生成大量红外图像数据。近年来生成对抗网络、扩散模型等得到了越来越多研
57
究者的关注，在图像生成领域展现出了强大的能力。本任务主要研究如何利用可见光等
多源图像生成红外图像数据，分析算法适应性。
可见光场景 真实红外 GAN 生成结果
图像样本生成
数据集：FREE FLIR Thermal Dataset for Algorithm Training
参考文献：
[1] Mao, Fangyuan et al., Pid: Physics-Informed Diffusion Model for Infrared Image
Generation. CVPR2024, https://doi.org/10.48550/arXiv.2407.09299
[2] Lingyan Ran et al., DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language
Understanding, https://arxiv.org/abs/2503.19012
[3] Brock A, Donahue J and Simonyan K, Large Scale GAN Training for High Fidelity
Natural Image Synthesis[C]. In: International Conference on Learning Representations. 2018. [4] Huang S W, Lin C T, Chen S P, Wu Y Y, Hsu P H and Lai S H, AugGAN: Cross Domain
58
Adaptation with GAN-based Data Augmentation[C]. In: Proceedings of the European
Conference on Computer Vision, 2018: 718-731. [6] Wang X, Yu K, Wu S, Gu J, Liu Y, Dong C, Qiao Y and Change Loy C, ESRGAN:
Enhanced Super-resolution Generative Adversarial Networks[C]. In: Proceedings of the
European Conference on Computer Vision . 2018. 指导老师：杨卫东,yangwd@hust.edu.cn，qq14223185
5.30 基于深度学习的遥感图像变化检测
任务描述：变化检测作为摄影测量与遥感领域的研究热点之一,也是人工智能体系中极具
研究价值的技术分支。二者的快速发展与深度融合,已使海量、复杂和多样的遥感数据快
速智能化处理成为可能,广泛应用于资源监测、城市规划、灾害评估等诸多领域。本任务
需要根据多时相遥感图像，采用深度学习的方案自动检出遥感图像中发生变化的区域。
数据：LEVIR-CD、CDD、DSIFN-CD、google earth pro
遥感图像变化检测
参考文献：
[1] Shi, Wenzhong, et al. "Change detection based on artificial intelligence: State-of-the-art
and challenges." Remote Sensing 12.10 (2020): 1688. [2] Bai, Ting, et al. "Deep learning for change detection in remote sensing: a
review." Geo-spatial Information Science 26.3 (2023): 262-288. [3] Wu, Chen, Bo Du, and Liangpei Zhang. "Fully convolutional change detection
framework with generative adversarial network for unsupervised, weakly supervised and
regional supervised change detection." IEEE Transactions on Pattern Analysis and
Machine Intelligence (2023).
59
[4] Lei, Tao, et al. "Ultralightweight spatial–spectral feature cooperation network for change
detection in remote sensing images." IEEE Transactions on Geoscience and Remote
Sensing 61 (2023): 1-14. 指导老师：杨卫东，yangwd@hust.edu.cn，qq14223185
5.31 目标识别对抗样本生成
任务描述：目标识别一直是计算机视觉领域的研究热点，其任务是从给定图像中提取感
兴趣区域并标记类别和位置。面向图像分类识别的神经网络会遭受数据对抗攻击的影响，
攻击者向目标图像分类器输入带有精心构造的带对抗噪声的图像，即对抗样本，会使分
类器预测错误。对抗样本的研究能促进识别网络模型的改进，提高其鲁棒性。结合典型
的目标识别任务，设计对抗样本生成算法，完成识别算法性能测试与评估。
数据：典型目标识别数据集
对抗样本检测效果示意图
参考文献：
[1] ZHENG L Y, TANG M, CHEN Y Y, et al. Improving multiple object tracking with single
object tracking[C]// CVPR 2021: 2453-2462. [2] SHEKAR A K, GOU L, REN L, et al. Label-free robustness estimation of object
detection CNNs for autonomous driving applications[J]. International Journal of
Computer Vision, 2021, 129(4): 1185-1201. [3] ZENG Y L, ZHANG L H, ZHAO J H, et al. JRL-YOLO: a novel jump-join repetitious
60
learning structure for real-time dangerous object detection[J]. Computational Intelligence
and Neuroscience, 2021, 2021: 1-16. [4] XIE C H, WANG J Y, ZHANG Z S, et al. Adversarial examples for semantic
segmentation and object detection[C]// ICCV 2017: 1378-1387. [5] ZHANG C N, BENZ P, KARJAUV A, et al. Investigating top-k white-box and
transferable black-box attack[C]// CVPR 2022: 15064-15073. 指导老师：杨卫东，yangwd@hust.edu.cn，qq14223185. 5.32 图像线特征提取与匹配（Arm/Atlas200 模型移植*）
任务描述：图像匹配在图像检索、无人飞行器导航及遥感测绘领域应用广泛。道路网是
遥感图像中常见的元素之一，可用于飞行器的目标识别、跟踪和定位校准等应用中。与
图像相比，矢量图存储更高效，与道路网表征契合，适合大范围内的飞行器导航应用。
利用样本数据，构建路网等线特征提取网络，设计相似性度量计算准则，提高特征匹配
准确率，分析特征的有效性和场景适应性。完成模型 Altlas200I 模块上的部署与评估。
数据：红外、可见光图对数据集（自建）。
线特征提取与匹配结果
参考文献：
[1] L. Fitriani, S. Dianti, D. Kurniadi, A. Mulyani, R. Setiawan. Mapping-Based Using
Geographic Information Systems for Smart Transportation. In: 2021 International
Conference on ICT for Smart Society. Bandung City, Indonesia, 2021, IEEE, 2021: 1-5. [2] N. Islam, K. Haseeb, A. Almogren, I. U. Din, M. Guizani, A. Altameem. A framework for
61
topological based map building: A solution to autonomous robot navigation in smart
cities. Future Generation Computer Systems. 2020, 111: 644-653. [3] W. Xu, C. Zhang, Q. Wang, P. Dai. FEA-Swin: Foreground Enhancement Attention Swin
Transformer Network for Accurate UAV-Based Dense Object Detection. Sensors. 2022, 22(18):6993. [4] Z. Xu, Y. Liu, L. Gan, Y. Sun, X. Wu, M. Liu, L. Wang. RNGDet: Road Network Graph
Detection by Transformer in Aerial Images. IEEE Transactions on Geoscience and
Remote Sensing. 2022, 60:1-12. [5] R. C. Poonia, M. K. Gupta, I. Abunadi, A. A. Albraikan, F. N. AlWesabi, M. A. Hamza. Intelligent Diagnostic Prediction and Classification Models for Detection of Kidney
Disease. Healthcare. 2022, 10(2):371. [6] K. P. Wijewardena, S. A. Grosz, K. Cao. A. K. Jain. Fingerprint Template Invertibility:
Minutiae vs. Deep Templates. IEEE Transactions on Information Forensics and Security. 2022, 18:744-757. [7] 华为云，https://www.huaweicloud.com/
指导老师：杨卫东，yangwd@hust.edu.cn，qq14223185. Atlas200模块使用管理老师：郑老师，13986105363. 5.33 海底目标检测与识别
任务描述：在真实海底图片数据中检测出不同目标（海参、海胆、扇贝、海星）的位置。
预期效果如下图所示：
62
海底检测图
海底目标检测图
数据说明：数据集由 5000 张训练图像以及 2600 张测试图像组成；这些图片之间不存在
帧间连续性。图片路径示例如下：train/image/ c000001.jpg，其对应的目标检测标注真值
位于路径 train/box/ c000001.xml 文件中，该文件包含了对应图像中所有物体的类别以
及目标框参数（位置和尺寸）。本届比赛需检测的目标类别包括海参“holothurian”，海胆
“echinus”，扇贝“scallop”和海星“starfish”四类。训练数据真值中可能存在水草“waterweeds” 这一类别，请忽略这一类。
数据集下载地址： https://pan.baidu.com/s/1gQyf2F0FhtjstI0gg44n-Q?pwd=1dkg
提取码：1dkg
注意：若电脑性能有限，可以从训练集和测试集中抽取部分数据（例如 2000 张训练集，
63
1500 张测试集）进行实验。
关于标注：
1、训练图片中存在一定的标注噪声，如有需要选手可自行清洗数据。其中，前缀 c 为
基本干净标注数据，前缀 u 为含噪声标注数据。
2、关于扇贝的标注歧义性较大，测试评估时基本以“前缀 c 中的扇贝标注标准”为准。
（必要时可忽略前缀为 u 的标注数据中的“扇贝”标注结果）
参考资料：
水下目标检测介绍：https://blog.csdn.net/qq_41088475/article/details/106764923
YOLOv1 原理：https://zhuanlan.zhihu.com/p/70387154
YOLOv1-YOLOv4 发展：https://zhuanlan.zhihu.com/p/139128562
YOLOv5 原理：https://blog.csdn.net/WZZ18191171661/article/details/113789486
YOLOv5 训练自己的数据集：https://blog.csdn.net/oJiWuXuan/article/details/107558286
特别注意：本课设很可能用到 NVIDIA GPU（显存不小于 6G）。请检查电脑或者服务器
是否配备独立显卡以及显卡显存大小，建议熟悉 Anaconda 安装 Pytorch 和其它环境的流
程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] YOLOv5 code : https://github.com/ultralytics/yolov5
[2] Review On Deep Learning Technique For Underwater Object Detection ，
（https://arxiv.org/abs/2209.10151）
[3] SWIPENET: Object detection in noisy underwater images ，
（https://arxiv.org/abs/2010.10006）
[4] RoIMix: Proposal-Fusion among Multiple Images for Underwater Object Detection ，
（https://arxiv.org/abs/1911.03029）
[5] A high-precision underwater object detection based on joint self-supervised deblurring and
improved spatial transformer network，（https://arxiv.org/abs/2203.04822）
[6] Achieving Domain Generalization in Underwater Object Detection by Domain Mixup and
Contrastive Learning，（https://arxiv.org/abs/2104.02230）
指导老师: 邹腊梅, zlmhust@163.com，qq：156685941
5.34 遥感图像中的目标检测
64
任务描述：实现对于遥感图像中的目标进行自动检测，并标记出其具体区域。
数据说明：LEVIR 数据集由 800 × 600 像素和 0.2m-1.0m /像素的高分辨率 Google Earth
图像图像组成，所有图像总共标记了 11k 个独立边界框，包括 4724 架飞机，3025 艘船
和 3279 个油罐。【若训练时间过久，可以选择只检测某一类目标】
参考资料：
数据下载地址：http://pan.baidu.com/s/1geTwAVD
YOLOv1 原理：https://zhuanlan.zhihu.com/p/70387154
YOLOv1-YOLOv4 发展：https://zhuanlan.zhihu.com/p/139128562
YOLOv5 原理：https://blog.csdn.net/WZZ18191171661/article/details/113789486
YOLOv5 训练自己的数据集：https://blog.csdn.net/oJiWuXuan/article/details/107558286
预期效果如下图所示：
检测结果示意图
特别注意：本课设需用到 NVIDIA GPU（显存不小于 3G）。建议笔记本或台式机的 GPU
65
性能够用，建议熟悉 Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同学
请谨慎选择。
参考文献：
[1] YOLOv5 code : https://github.com/ultralytics/yolov5
[2] Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint
arXiv:1804.02767, 2018. [3] Van Etten A. You only look twice: Rapid multi-scale object detection in satellite
imagery[J]. arXiv preprint arXiv:1805.09512, 2018. [4] Zou Z, Shi Z. Random access memories: A new paradigm for target detection in high
resolution aerial remote sensing images[J]. IEEE Transactions on Image Processing, 2017, 27(3): 1100-1111. 指导老师: 邹腊梅, zlmhust@163.com，qq：156685941
5.35 密集场景下的行人跟踪
任务描述：利用深度学习算法实现复杂环境下拥挤的任务目标定位，并且能够准确跟踪
人物对象。
数据说明：MOT15 数据集是多目标跟踪的基准，旨在评估算法在复杂环境中同时追踪多
个目标（如行人）的能力，包含 jpeg 格式的图像和 CSV 格式的目标信息与轨迹信息标注
文件。每个目标由 10 个数值描述，包括帧数、轨迹 ID、bounding box 坐标、置信度分
数和 3D 位置。ground truth 文件使用置信度分数（第 7 个值）来决定是否考虑某个目标，
值为 0 表示忽略，值为 1 表示活动目标。
数据集下载：
MOT Challenge - Data （https://motchallenge.net/data/MOT15/）
预期效果如下图所示：
66
特别注意：本课设会用到 NVIDIA GPU。建议笔记本或台式机的 GPU 性能够用，建议熟
悉 Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu.Fairmot: On
the fairness of detection and re-identification in multiple object tracking. International
Journal of Computer Vision, 129:3069–3087, 2021. [2] Chao Liang, Zhipeng Zhang, Xue Zhou, Bing Li, Shuyuan Zhu, and Weiming Hu. Rethinking the competition between detection and reid in multiobject tracking. IEEE
Transactions on Image Processing, 31:3182–3196, 2022
[3] Xiao D ,Wang Z ,Shen Z , et al.A FairMOT approach based on video recognition for
real-time automatic incident detection on expressways[J].Signal, Image and Video
67
Processing, 2024, 18(10): 7333-7348. [4] Dong Q ,Zhangang W .Improved FairMOT multi-target tracking method for complex
scenes[J].Journal of Physics: Conference Series,2022,2303(1):
其它相关参考资料：
[1] https://github.com/ifzhang/FairMOT
[2] https://arxiv.org/abs/2004.01888
[3] https://arxiv.org/abs/2010.12138
[4] https://zhuanlan.zhihu.com/p/345818662
[5] https://blog.csdn.net/zhangzhao147/article/details/137052708?ops_request_misc=&re
quest_id=&biz_id=102&utm_term=fairmot&utm_medium=distribute.pc_search_result.n
one-task-blog-2~all~sobaiduweb~default-3-137052708.142^v102^pc_search_result_bas
e2&spm=1018.2226.3001.4187
[6] https://blog.csdn.net/maohule/article/details/133710512?ops_request_misc=&request _id=&biz_id=102&utm_term=fairmot&utm_medium=distribute.pc_search_result.none-t
ask-blog-2~all~sobaiduweb~default-4-133710512.142^v102^pc_search_result_base2&s
pm=1018.2226.3001.4187
[7] https://blog.csdn.net/mzpmzk/article/details/126186658?ops_request_misc=%257B%2
522request%255Fid%2522%253A%2522be60306d473b8252c24fb060f1994f59%2522%2
52C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=be
60306d473b8252c24fb060f1994f59&biz_id=0&utm_medium=distribute.pc_search_resu
lt.none-task-blog-2~all~baidu_landing_v2~default-5-126186658-null-null.142^v102^pc_
search_result_base2&utm_term=fairmot&spm=1018.2226.3001.4187
指导老师: 邹腊梅, zlmhust@163.com，qq：156685941
5.36 基于轻量化网络的麦穗目标检测
任务描述：实现对于小麦图像中的麦穗目标进行自动检测，并标记出其具体区域。
数据说明：数据集由 3432 张图像组成，只包含“麦穗”一个类别，图片大小为 1024×1024。
数据下载地址：https://www.kaggle.com/c/global-wheat-detection/data
MobilenetV1-V3：https://zhuanlan.zhihu.com/p/70703846
YOLOv1 原理：https://zhuanlan.zhihu.com/p/70387154
68
YOLOv1-YOLOv4 发展：https://zhuanlan.zhihu.com/p/139128562
YOLOv5 原理：https://blog.csdn.net/WZZ18191171661/article/details/113789486
YOLOv5 训练自己的数据集：https://blog.csdn.net/oJiWuXuan/article/details/107558286
YOLOv5 修改网络结构：https://blog.csdn.net/wa1tzy/article/details/114492726
注意事项：下载的文件中包含 train.zip(训练图像)和 test.zip (测试图像)，由于 test.zip 没
有标签，所以本次课设只使用 train.zip 中的数据。
首先对 train 文件夹下面图片根据名称进行排序(第一张图片为：00b5c6764.jpg)，然
后从第一张图片开始选取 700 张图片（第 700 张图片为：7fcfe0ae1.jpg）作为测试集（即
这 700 张图片不参与训练，只作为训练完成后测试模型准确度的数据）；剩余图片作为
训练集。
评判标准：使用 mAP（mean Average Precision）作为评判标准。
模型参考：可以考虑使用 Mobilenet-V3 作为特征提取的 Backbone，使用 YOLOv5 的检
测部分输出具体定位框。
预期效果如下图所示：
检测结
果示意图
特别注意：本课设很可能用到 NVIDIA GPU（显存不小于 4G）。建议笔记本或台式机的 GPU
性能够用，建议熟悉 Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同
学请谨慎选择。
参考文献：
[1] YOLOv5 code : https://github.com/ultralytics/yolov5
69
[2] David E, Madec S, Sadeghi-Tehran P, et al. Global Wheat Head Detection (GWHD)
dataset: a large and diverse dataset of high-resolution RGB-labelled images to develop and
benchmark wheat head detection methods[J]. Plant Phenomics, 2020. [3] Howard A, Sandler M, Chu G, et al. Searching for mobilenetv3[C]. Proceedings of the
IEEE/CVF International Conference on Computer Vision. 2019: 1314-1324. 指导老师: 邹腊梅, zlmhust@163.com，qq：156685941
5.37 物体 6D 姿态估计
任务描述：利用深度学习算法实现对图像中目标物体的 6D 姿态估计，即估计物体相对于相机的姿
态变换矩阵，从而获取目标物体的位置及姿态信息。 数据说明：数据集 LM-O 为 BOP 的七个常用数据集之一。LM-O 中包含 8 个常见工业
物体（如胶水罐、电源适配器等）。LM-O 专注于解决物体在严重遮挡场景下的姿态估
计问题，它是对原始 LineMOD 数据集的扩展，增加了复杂的遮挡条件，更贴近真实世
界的应用场景。
LM-O 数据集中每个类别都有约 1300 个训练数据（无遮挡场景，用于模型预训练），
其中训练数据包括以下几项：RGB 图像：分辨率为 640x480；深度图：与 RGB 对齐
的深度信息（可选）；姿态标注：每个物体的 6D 姿态（3D 平移 + 3D 旋转矩阵）；遮
挡标注：目标物体被遮挡的像素级掩码。
而测试集约有 200 张图像（含复杂遮挡，用于评估），其中每张图像中的目标物体
都有独立的真值标注。
官方数据集地址：BOP: Benchmark for 6D Object Pose Estimation
（https://bop.felk.cvut.cz/datasets/）
数据集官方说明：项目文件预览 - bop_toolkit:A Python toolkit of the BOP benchmark for
6D object pose estimation. - GitCode
LM-O 数据集下载地址：bop-benchmark/lmo at main
预期效果如下图：
70
特别注意：本课设会用到 NVIDIA GPU。建议笔记本或台式机的 GPU 性能够用，建议熟悉
Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] Wen B, Yang W, Kautz J, et al. Foundationpose: Unified 6d pose estimation and tracking
of novel objects[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 2024: 17868-17879. [2] Lin J, Liu L, Lu D, et al. Sam-6d: Segment anything model meets zero-shot 6d object
pose estimation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 2024: 27906-27916. [3] Li Y, Mao Y, Bala R, et al. Mrc-net: 6-dof pose estimation with multiscale residual
correlation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 2024: 10476-10486. [4] Nguyen V N, Groueix T, Salzmann M, et al. Gigapose: Fast and robust novel object pose
estimation via one correspondence[C]//Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2024: 9903-9913. 指导老师: 邹腊梅, zlmhust@163.com，qq：156685941
71
5.38 人脸关键点检测
任务描述：利用深度学习算法实现图像中的人脸关键点检测，需要检测出图像中的人脸
关键点，并给出检测结果。
数据说明：人脸关键点检测数据集 WFLW 一共标注了 98 个人脸关键点, WFLW 数据集
具有更多在不寻常条件下拍摄的图像，比如化妆（206 张图像）、模糊（773 张图像）、
姿势（326 张图像）、遮挡（736 张图像）、各种光照条件（698 张图像）等，可以全面评
估算法在不同条件下的鲁棒性。该数据集一共包含 10000 张人脸图像，其中 7500 张用
于训练，剩下的 2500 张用于测试。
数据集下载地址：https://wywu.github.io/projects/LAB/WFLW.html
预期效果如下图：
人脸关键点检测效果图
特别注意：本课设会用到 NVIDIA GPU。建议笔记本或台式机的 GPU 性能够用，建议
熟悉 Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] Jin H, Liao S, Shao L. Pixel-in-pixel net: Towards efficient facial landmark detection in
the wild[J]. International Journal of Computer Vision, 2021, 129(12): 3174-3194. [2] Guo X, Li S, Yu J, et al. PFLD: A practical facial landmark detector[J]. arXiv preprint
arXiv:1902.10859, 2019. 数据集联系科技楼十楼谢佳，联系方式：QQ：392316514。
指导老师：邹腊梅，zlmhust@163.com qq：156685941
72
5.39 工业品表面缺陷检测
任务描述：利用深度学习算法实现工业品表面的缺陷检测，并且定位出缺陷的位置。
数据说明：MVTec AD 为无监督缺陷检测数据集，共包含 3466 张无标注图，1888 张有标
注图（像素级分割标注），大小均为 700×700 或 1024×1024；training dataset 有 3629
张，全部为无缺陷图；test dataset 有 1725 张，为有缺陷图片+无缺陷图；图片可划分
为 5 种纹理和 10 种物体，包含 73 种缺陷（划痕，凹陷，脏污，形变，缺料等，均为人
工制作），总计标注 1900 个 region。 MVTec A 数据集示例图片:
特别注意：本课设会用到 NVIDIA GPU。建议笔记本或台式机的 GPU 性能够用，建议
熟悉 Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同学请谨慎选择。
参考文献：
[1] Bergmann P, Fauser M, Sattlegger D, et al. Uninformed students: Student-teacher
anomaly detection with discriminative latent embeddings[C]//Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 2020: 4183-4192. [2] Wang G, Han S, Ding E, et al. Student-teacher feature pyramid matching for anomaly
detection[J]. arXiv preprint arXiv:2103.04257, 2021. [3] Zavrtanik V, Kristan M, Skočaj D. Draem-a discriminatively trained reconstruction
embedding for surface anomaly detection[C]//Proceedings of the IEEE/CVF International
Conference on Computer Vision. 2021: 8330-8339. [4] Yu J, Zheng Y, Wang X, et al. Fastflow: Unsupervised anomaly detection and localization
73
via 2d normalizing flows[J]. arXiv preprint arXiv:2111.07677, 2021. 数据集下载：
MVtec AD:MVTec Anomaly Detection Dataset: MVTec Software
指导老师：邹腊梅，zlmhust@163.com qq：156685941
5.40 无人机视角下的目标检测与识别
任务描述：任务描述：无人机已广泛应用于各种应用，例如监视、自主检测、车队导航
和农业。尽管自然图像如 COCO 数据集上的目标检测取得了显着进展，但检测无人机视
角图像中的目标仍然具有挑战性，这主要源于小尺度和极端尺度变化。本课题基于深度
学习方法，实现对于无人机航拍视角下的图片进行目标检测，能够区分不同类别目标并
给出目标的准确位置。
数据说明：数据集为 VisDrone2019 数据集，其中图像分辨率为 2000*1500，包含 6471
张训练图像，548 张验证图像和 1610 张测试图像，且都具有标签文件。数据集包含十类
不同类型目标如车辆，行人等等。
VisDrone 数 据 集 ： GitHub - VisDrone/VisDrone-Dataset: The dataset for drone based
detection and tracking is released, including both image/video, and annotations. VisDrone 数 据 集 （ COCO 格 式 ）： GitHub - PuAnysh/UFPMP-Det: The official
implementation of UFPMP-Det
YOLOv8 原理：https://zhuanlan.zhihu.com/p/668516241
YOLOv8 训练 VisDrone 数据集：https://dhexx.cn/news/show5056287.html?action=onClick
YOLOv8 修改网络结构： https://blog.csdn.net/wahaha987656789/article/details/134141589
注意事项：下载的文件中包含四个压缩文件，除了 VisDrone2019-DET-test-challenge.zip
外都具有标签。 其他标签格式数据集也提供了下载方式。
注意，虽然测试集也有标签文件，但是禁止将其加入训练或验证中，否则最后测试较为
麻烦。
评判标准：使用 mAP（mean Average Precision）作为评判标准。
预期检测效果如下图所示：
74
检测结果示意图
特别注意：本课设需用到 NVIDIA GPU（显存不小于 4G）。建议笔记本或台式机的 GPU
性能够用，建议熟悉 Anaconda 安装 Pytorch 和其它环境的流程，不满足实验条件的同学
请谨慎选择。
参考文献：
[1] YOLOv8 code : https://github.com/ultralytics/ultralytics
[2] Zhao L L, Zhu M L. MS-YOLOv7: YOLOv7 Based on Multi-Scale for Object Detection
on UAV Aerial Photography[J]. Drones, 2023, 7(3): 188. [3] Du D, Zhu P, Wen L, et al. VisDrone-DET2019: The vision meets drone object detection
in image challenge results[C]//Proceedings of the IEEE/CVF international conference on
computer vision workshops. 2019: 0-0. [4] Huang Y, Chen J, Huang D. UFPMP-Det:Toward Accurate and Efficient Object Detection
on Drone Imagery [J]. Proceedings of the AAAI Conference on Artificial Intelligence, 2022, 36(1): 1026–1033. [5] Wang G, Chen Y, An P, et al. UAV-YOLOv8: a small-object-detection model based on
improved YOLOv8 for UAV aerial photography scenarios[J]. Sensors, 2023, 23(16): 7190. 指导老师: 邹腊梅, zlmhust@163.com，qq：156685941
75
5.41 基于深度学习的鸟声识别
任务描述：设计合适的深度学习网络，实现从音频录音中辨识鸟类个体及其物种，利用
降噪、数据增强等方法提高识别性能，并与其他文献方法的性能进行比较分析。
数据说明：BirdCLEF 2023 公开数据集，训练数据由 xenocanto.org 网站的用户慷慨上
传的单个鸟类鸣叫的短音频组成。这些文件已被下采样至 32 千赫兹，并转换为 ogg 格
式。train_metadata.csv 文件中为训练数据提供了广泛的元数据，最直接相关的字段包
括：1）primary_label：鸟类物种的代码。可以通过将代码添加到
https://ebird.org/species/ 来查看有关鸟类代码的详细信息，例如对于美洲乌鸦
（American Crow），链接为 https://ebird.org/species/amecro。2）latitude &
longitude：录制音频的经纬度坐标。有些鸟类物种可能有当地的鸣叫 “方言”，所以
您可能希望在训练数据中寻找地理上的多样性。
3）author：提供音频记录的用户。4）filename：相关音频文件的名称。
下载链接：BirdCLEF 2023 | Kaggle 或 BirdCLEF 2023_数据集-飞桨AI Studio星河社区
参考文献：
[1] Daidai Liu,Hanguang Xiao,Kai Chen. Research progress in bird sounds recognition based
on acoustic monitoring technology: A systematic review, Applied Acoustics, 2025(228):
110285. [2] René Heinrich，Lukas Rauch，Bernhard Sick, et al. AudioProtoPNet: An interpretable deep
learning model for bird sound classification. Ecological Informatics,2025(87),10381. [3] Xie JJ, Zhong YJ, Zhang JG, et al. A review of automatic recognition technology for bird
vocalizations in the deep learning era. Ecological Informatics, 2023(73), 101927. [4] Carvalho S, Gomes EF. Automatic classification of bird sounds: Using MFCC and Mel
spectrogram features with deep learning. Vietnam Journal of Computer Science, 2023(10), 39–54.
76
[5] Tang Q, Xu LM, Zheng BC, et al. Transound: Hyper-head attention transformer for birds
sound recognition. Ecological Informatics, 2023(75), 102001. [6] Zabidi MM, Wong KL, Sheikh UU, et al. Bird sound detection with binarized neural
networks. Journal of Electrical Technology, 2022(21), 48–53. [7] Zhang CY, Chen YH, Hao ZZ, et al. An efficient time-domain end-to-end single-channel
bird sound separation network. Animals, 2022(12), 3117. [8] Zhang FY, Zhang LY, Chen HX, et al. Bird species identification using spectrogram based
on multi-channel fusion of DCNNs. Entropy, 2021(23), 1507–1518. 指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.42 视频序列中人脸微表情识别
任务描述：选择一种方法实现对视频序列中的人脸微表情进行识别，并给出识别性能指
标，与其他文献方法的性能进行比较。
数据说明：MMEW 数据集，micro-and-macro expression warehouse2021 年发布，在实验
室环境下，通过观看情感视频诱惑微表情，同时试图保持中性表情，该数据集中包含微
表情视频样本 300，宏表情视频样本 900,受试者 36，平均年龄 22.35，亚洲人，样本标
签包含 7 种情感类型, Happiness (36), Anger (8), Surprise (89), Disgust (72), Fear (16), Sadness (13) and Others (66)，视频分辨率为 1920*1080，每秒帧数（fps) 200，视频中人
脸尺寸 400*400。下载链接 GitHub - benxianyeteam/MMEW-Dataset: Micro-and-Macro
Expression Warehouse (MMEW) Dataset
Happiness Surprise Disgust
Fear Sadness Others
MMEW 数据集示例
参考文献：
77
[1] M. Verma, S. K. Vipparthi and G. Singh, "Deep Insights of Learning-Based Micro
Expression Recognition: A Perspective on Promises, Challenges, and Research Needs,"
in IEEE Transactions on Cognitive and Developmental Systems, vol. 15, no. 3, pp. 1051-1069, Sept. 2023, doi: 10.1109/TCDS.2022.3226348. [2] X. Ben et al. Video-Based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9), 2022: 5826-5846, doi: 10.1109/TPAMI.2021.3067464
[3] Z. Shang, J. Liu and X. Li. Micro-Expression Recognition Based on Spatio–Temporal
Capsule Network. IEEE Access, 11, 2023: 13704-13713, doi:
10.1109/ACCESS.2023.3242871. [4] X. Cai, H. Tang and L. Chai. Micro Expression Recognition based on Graph
Convolutional Networks with LSTM[C]. 2023 35th Chinese Control and Decision
Conference (CCDC), Yichang, China, 2023: 5449-5453, doi:
10.1109/CCDC58219.2023.10327083. [5] N. Kim, S. Cho, C. H. Ahn, et al. Facial Micro-Expression Recognition in Video using
Squeezed Landmark Feature Maps[C]. 2021 International Conference on Information and
Communication Technology Convergence (ICTC), Jeju Island, Korea, Republic of, 2021:
1107-1110, doi: 10.1109/ICTC52510.2021.9620973. [6] S. Indolia, S. Nigam, R. Singh, et al. Micro Expression Recognition Using Convolution
Patch in Vision Transformer[J]. IEEE Access, 11, 2023: 100495-100507. [7] P. Gupta. MERASTC: Micro-Expression Recognition Using Effective Feature Encodings
and 2D Convolutional Neural Network[J]. IEEE Transactions on Af ective Computing, 14(2)
2023: 1431-1441, doi: 10.1109/TAFFC.2021.3061967. [8] Y. Li, X. Huang and G. Zhao. Joint Local and Global Information Learning With Single
Apex Frame Detection for Micro-Expression Recognition[J]. IEEE Transactions on Image
Processing, 30, 2021: 249-263, doi: 10.1109/TIP.2020.3035042. 指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.43 基于视觉的垃圾检测分类
任务描述：选择一种方法实现图像中垃圾的检测和分类，以便回收利用其中的可回收垃
78
圾，与其他文献方法的性能进行比较分析。
数据说明：TrashNet 数据集中包含垃圾图片 2507 张，标记为 6 类，包括可回收垃圾类
别纸、金属、塑料、玻璃等。 GitHub - garythung/trashnet: Dataset of images of trash;
Torch-based CNN for garbage image classification
Carboard Glass Metal
Paper Plastic Trash
TrashNet 数据集示例
参考文献：
[1] H. Abdu, M. H. Mohd Noor. A Survey on Waste Detection and Classification Using Deep
Learning[J]. IEEE Access, 10,2022: 128151-128165, doi: 10.1109/ACCESS.2022.3226682. [2] J. Li et al. Automatic Detection and Classification System of Domestic Waste via
Multimodel Cascaded Convolutional Neural Network[J]. IEEE Transactions on Industrial
Informatics, 18(1), 2022: 163-173, doi: 10.1109/TII.2021.3085669. [3] S. I. Jalal, H. A. Ahmed, M. H. Ahmed. Design a Robust Real-Time Trash Detection
System Using YOLOv5 Variants[C]. 2023 IEEE IAS Global Conference on Emerging
Technologies (GlobConET), London, United Kingdom, 2023: 1-6, doi:
10.1109/GlobConET56651.2023.10149899. [4] S. Krishna Varshan, M. Ashish, E. Binu, et al. Trash Image Classification Using
Autoencoder[C]. 2023 4th International Conference on Electronics and Sustainable
Communication Systems (ICESC), Coimbatore, India, 2023: 1278-1284, doi:
10.1109/ICESC57686.2023.10193687. [5] A. Kurz, E. Adams, A. C. Depoian, et al. WMC-ViT: Waste Multi-class Classification
Using a Modified Vision Transformer[J]. 2022 IEEE MetroCon, Hurst, TX, USA, 2022: 1-3, doi: 10.1109/MetroCon56047.2022.9971136. [6] A. Masand, S. Chauhan, M. Jangid, et al. ScrapNet: An Efficient Approach to Trash
79
Classification[J]. IEEE Access, 9, 2021: 130947-130958, doi:
10.1109/ACCESS.2021.3111230. [7] Z. Yang, D. Li. WasNet: A Neural Network-Based Garbage Collection Management
System[J]. IEEE Access, 8, 2020: 103984-103993, doi: 10.1109/ACCESS.2020.2999678. [8] W. Ma, X. Wang, J. Yu. A Lightweight Feature Fusion Single Shot Multibox Detector for
Garbage Detection. IEEE Access, 8, 2020: 88577-188586, doi:
10.1109/ACCESS.2020.3031990
指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.44 基于图像的饮食营养评估
任务描述： 选择一种方法实现图像中的食物类别识别，同时估计食物的体积，给出图
像中食品的营养评估。
数据说明： Food2K 是一个大型食品识别数据集，包含 2000 个类别和超过 100 万张图
像。数据集下载链接 http://123.57.42.89/FoodProject.html
FOOD2K 数据集示例
参考文献：
[1] F. S. Konstantakopoulos, E. I. Georga, D. I. Fotiadis. A Review of Image-Based Food
Recognition and Volume Estimation Artificial Intelligence Systems. IEEE Reviews in
Biomedical Engineering, 17, 2024: 136-152, doi: 10.1109/RBME.2023.3283149.
80
[2] Weiqing Min, Zhiling Wang, Yuxin Liu, et al. Large Scale Visual Food Recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 45(8), 2023: 9932-9949, DOI:10.1109/TPAMI.2023.3237871. [3] Sergio Romero-Tapiador, Ruben Tolosana, Aythami Morales, et al. Leveraging Automatic
Personalised Nutrition: Food Image Recognition Benchmark and Dataset based on Nutrition
Taxonomy. 14, 2022. [4] Chi-Sheng Chen, Guan-Ying Chen, Dong Zhou, et al. Res-VMamba: Fine-Grained Food
Category Visual Classification Using Selective State Space Models with Deep Residual
Learning. 24, 2024·
[5] H. He, F. Kong, J. Tan. DietCam: Multiview Food Recognition Using a Multikernel
SVM[J]. IEEE Journal of Biomedical and Health Informatics, 20(3), 2016: 848-855, doi:
10.1109/JBHI.2015.2419251. [6] B. Magid et al. CalorieMe: An Image-based Calorie Estimator System[C]. 2023 Eleventh
International Conference on Intelligent Computing and Information Systems (ICICIS), Cairo, Egypt, 2023: 555-560, doi: 10.1109/ICICIS58388.2023.10391113. [7] E. D. Cherpanath, P. R. Fathima Nasreen, K. Pradeep, et al. Food Image Recognition and
Calorie Prediction Using Faster R-CNN and Mask R-CNN[C]. 2023 9th International
Conference on Smart Computing and Communications (ICSCC), Kochi, Kerala, India, 2023:
83-89, doi: 10.1109/ICSCC59169.2023.10335053. [8] K. Moumane, I. El Asri, T. Cheniguer, et al. Food Recognition and Nutrition Estimation
using MobileNetV2 CNN architecture and Transfer Learning[C]. 2023 14th International
Conference on Intelligent Systems: Theories and Applications (SITA), Casablanca, Morocco, 2023: 1-7, doi: 10.1109/SITA60746.2023.10373725. 指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.45 基于图像的植物叶片病害识别轻量化方法
任务描述：实现两种轻量化的叶片病害识别方法，对比所实现的两种算法的性能。
数据说明：PlantVillage 数据集包括 54303 张健康和病害图片，分为 38 个类别。
下载链接 Data for: Identification of Plant Leaf Diseases Using a 9-layer Deep Convolutional
Neural Network - Mendeley Data
81
PlantVillage 数据集示例
（ (a) Apple with black rot, (b) grape with leaf blight, (c) tomato with leaf mold, (d) cherry
with powdery mildew, (e) potato with early blight, (f) healthy soybean, (g) peach with
bacterial spots and (h) healthy blueberry.）
参考文献：
[1] L. Li, S. Zhang, B. Wang. Plant Disease Detection and Classification by Deep
Learning—A Review[J]. IEEE Access, 9, 2021: 56683-56698, doi:
10.1109/ACCESS.2021.3069646. [2] Z. Xiao, Y. Shi, G. Zhu, et al. Leaf Disease Detection Based on Lightweight Deep
Residual Network and Attention Mechanism[J]. IEEE Access, 11, 2023: 48248-48258, doi:
10.1109/ACCESS.2023.3272985. [3] Joao Paulo Schwarz Schuler, Santiago Romani, Mohamed Abdel-Nasser, et al. Color-aware two-branch DCNN for efficient plant disease classification[J]. Mendel 28(1), 2022:55-62. [4] X. Zhu et al. LAD-Net: A Novel Light Weight Model for Early Apple Leaf Pests and
Diseases Classification[J]. IEEE/ACM Transactions on Computational Biology and
Bioinformatics, 20(2), 2023: 1156-1169, doi: 10.1109/TCBB.2022.3191854. [5] Y. Zhao et al. Plant Disease Detection Using Generated Leaves Based on DoubleGAN[J]. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(3), 2022:
1817-1826, doi: 10.1109/TCBB.2021.3056683. [6] C. Zhou, S. Zhou, J. Xing, et al. Tomato Leaf Disease Identification by Restructured Deep
82
Residual Dense Network[J]. IEEE Access, 9, 2021: 28822-28831, doi:
10.1109/ACCESS.2021.3058947. [7] J, ARUN PANDIAN, GOPAL, GEETHARAMANI. Data for: Identification of Plant Leaf
Diseases Using a 9-layer Deep Convolutional Neural Network. Mendeley Data, 2019, V1, doi:
10.17632/tywbtsjrjv.1
指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.46 基于结构信息挖掘的鸟类识别
任务描述：设计一种方法对图像中鸟的种类进行识别，能够挖掘图像中用于细粒度视觉
分类的结构信息，提高鸟类识别性能，并与其他文献方法的性能进行对比。
数据说明：CUB-200-2011 数据集，加州理工大学与 2010 年提出的公开鸟类数据集，也
是目前细粒度图像识别领域研究的基准数据集, 该数据集包括了北美 200 种鸟类，11788
张鸟类图像，训练图像 5994 张，测试图像 5794 张，每一种鸟类 30 张左右训练数据以
及测试数据，提供了图像级别的分类标签，鸟类边界框信息，鸟的关键点区域以及鸟类
的属性信息。下载链接 CUB-200-2011 (caltech.edu)
83
CUB-200-2011 数据集示例
参考文献：
[1] Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha,et al. Fine-Grained Image Analysis with
Deep Learning: A Survey. Computer Vision and Pattern Recognition, arXiv:
2111.06119 [cs.CV] . https://doi.org/10.48550/arXiv.2111.06119
[2] Wah, C., Branson, S., Welinder, P., Perona, P., & Belongie, S. (2022). CUB-200-2011 (1.0)
[Data set]. CaltechDATA. https://doi.org/10.22002/D1.20098
[3] H. Sun, X. He, J. Xu and Y. Peng. SIM-OFE: Structure Information Mining and
Object-Aware Feature Enhancement for Fine-Grained Visual Categorization. IEEE
Transactions on Image Processing, 2024(33): 5312-5326.
84
[4] Asish Bera, Zachary Wharton, Yonghuai Liu, et al. SR-GNN: Spatial Relation-aware
Graph Neural Network for Fine-Grained Image Categorization. CVPR, 2022. [5] U. E. Akpudo, X. Yu, J. Zhou，Y. Gao. What EXACTLY are We Looking at?: Investigating
for Discriminance in Ultra-Fine-Grained Visual Categorization Tasks[C]. 2023 International
Conference on Digital Image Computing: Techniques and Applications (DICTA), Port
Macquarie, Australia, 2023: 129-136, doi: 10.1109/DICTA60407.2023.00026. [6] B. -S. Wang, J. -W. Hsieh, Y. -K. Hsieh, et al. COFENet: Co-Feature Neural Network
Model for Fine-Grained Image Classification[C]. 2022 IEEE International Conference on
Image Processing (ICIP), Bordeaux, France, 2022: 3928-3932, doi:
10.1109/ICIP46576.2022.9897463. [7] S. Li, S. Wang, Z. Dong, et al. PSBCNN: Fine-grained image classification based on
pyramid convolution networks and SimAM[C]. 2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl
Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology
Congress (DASC/PiCom/CBDCom/CyberSciTech), Falerna, Italy, 2022: 1-4, doi:
10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927801. [8] M. Ju, H. Ryu, S. Moon, et al. GAPNet: Generic-Attribute-Pose Network For
Fine-Grained Visual Categorization Using Multi-Attribute Attention Module[C]. 2020 IEEE
International Conference on Image Processing (ICIP), Abu Dhabi, United Arab Emirates, 2020: 703-707, doi: 10.1109/ICIP40778.2020.9190875. 指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.47 基于迁移学习的花卉识别
任务描述：识别图像中的花卉类别，针对花卉图像样本少的问题进行样本增强，采用迁
移学习方法提高模型性能，给出所实现方法的性能分析。
数据说明：中国植物主题数据库的五类花卉数据集，下载链接
http://download.tensorflow.org/example_images/flower_photos.tgz
85
daisy dandelion rose sunflowers tulips
中国植物主题数据库的五类花卉数据集
牛津大学视觉几何组提出的 Oxford-17 flowers 数据集和 Oxford-102 flowers 数据集，下
载链接 Visual Geometry Group - University of Oxford
参考文献：
[1] R. Xiao and R. Wang. Transfer Learning-Based Flower Image Classification: Leveraging
The Pre-Trained Alexnet Model[C]. 2023 4th International Symposium on Computer
Engineering and Intelligent Communications (ISCEIC), Nanjing, China, 2023 : 519-522, doi:
10.1109/ISCEIC59030.2023.10271209. [2] P. Shourie, V. Anand and S. Gupta. Flower Classification using a Transfer-based Model[C]. 2023 2nd International Conference on Applied Artificial Intelligence and Computing
(ICAAIC), Salem, India, 2023: 1-6, doi: 10.1109/ICAAIC56838.2023.10140969. [3] G. Yifei, Q. Chuxian, X. Jiexiang, et al. Flower image classification based on improved
convolutional neural network[C]. 2022 12th International Conference on Information
Technology in Medicine and Education (ITME), Xiamen, China, 2022: 81-87, doi:
10.1109/ITME56794.2022.00028. [4] M. He, H. Zhu, Y. Li, et al. Flower Image Classification Based on Multi-scale Dense
Residual Network[C]. 2021 6th International Conference on Image, Vision and Computing
(ICIVC), Qingdao, China, 2021: 144-148, doi: 10.1109/ICIVC52351.2021.9526970. [5] F. Hu, F. Yao and C. Pu. Learning Salient Features for Flower Classification Using
Convolutional Neural Network[C]. 2020 IEEE International Conference on Artificial
Intelligence and Information Systems (ICAIIS), Dalian, China, 2020: 476-479, doi:
10.1109/ICAIIS49377.2020.9194931. [6] F. R. Siregar and W. F. Al Maki. Hybrid Method for Flower Classification in High
Intra-class Variation[C]. 2020 3rd International Seminar on Research of Information
Technology and Intelligent Systems (ISRITI), Yogyakarta, Indonesia, 2020: 73-78, doi:
86
10.1109/ISRITI51436.2020.9315379. 指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.48 花粉识别方法对比分析
任务描述：选择两种方法实现图像中花粉类别的识别，比较分析两种方法的性能。
数据说明：
POLLEN73S 花粉图像数据集由 Astolfi 于 2020 年提出，包含了 73 种花粉类别共 2523
张图像。该数据集采集巴西南马托格罗索州首府坎波格兰德市城区 20°23'16.8“S
54°36'36.3”W 坐标 1.5 公里半径范围内的开花物种样本。下载链接：POLLEN73S
(figshare.com)
POLLEN73S 数据集示例
参考文献：
[1] GONÇALVES A B, SOUZA J S, SILVA G G, et al. Feature extraction and machine
learning for the classification of Brazilian Savannah pollen grains[J]. PloS One, 2016, 11(6):
1-20. [2] ASTOLFI G, GONCALVES A B, MENEZES G V, et al. POLLEN73S: An image dataset
87
for pollen grains classification[J]. Ecological Informatics, 2020, 60: 101165-101173. [3] N. Khanzhina, M. Kashirin and A. Filchenkov. New Bayesian Focal Loss Targeting
Aleatoric Uncertainty Estimate: Pollen Image Recognition[C]. 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023：4253-4262, doi: 10.1109/CVPRW59228.2023.00448. [4] K. Duan, S. Bao, H. Zhou, et al. Adaptive Global Token Pooling Transformer for
Microscope Pollen Image Recognition[C]. 2023 8th International Conference on
Computational Intelligence and Applications (ICCIA), Haikou, China, 2023: 198-204, doi:
10.1109/ICCIA59741.2023.00043. [5] W. Cheng et al. Attention to Contour: A Contour-Guided Deep Network for Pollen
Classification[C]. 2022 IEEE International Conference on Systems, Man, and Cybernetics
(SMC), Prague, Czech Republic, 2022: 809-814, doi: 10.1109/SMC53654.2022.9945495. [6] S. Battiato, A. Ortis, F. Trenta, et al. Detection and Classification of Pollen Grain
Microscope Images[C]. 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), Seattle, WA, USA, 2020: 4220-4227, doi:
10.1109/CVPRW50498.2020.00498. 指导老师：胡若澜 huruolan@hust.edu.cn qq:20536555
5.49 有限样本条件三维场景重建
任务描述：三维场景重建是计算机视觉和人工智能领域近年来研究的热点主题之一，能
应用于建立数字资产、样本生成等多种。然而在实际应用场景中可能获取的样本数量有
限，此外不同样本可能采集于不同时间、受光照、分辨率、部分遮挡、气象等因素影响。
本课题的研究目的在于探索不同的三维场景重建方法，并提升其在有限样本条件下的效
果。本题目要求制作的样本数量不少于 10 个，评估标准包括 PSNR 和 LPIPS，重建的方
法包括：SparseNeRF、SparseGS、LRM、NoPoSplat、TRELLIS 等。
88
数据描述：
MVImgNet 数据集可从 https://gaplab.cuhk.edu.cn/projects/MVImgNet/获取。在制作
新样本时可以拍摄大量照片，然后进行 colmap 稀疏重建，在测试时选取少量的图片进
行评估。
参考文献：
[1] Yu, Alex, et al. "pixelnerf: Neural radiance fields from one or few images." Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. [2] ain, Ajay, Matthew Tancik, and Pieter Abbeel. "Putting nerf on a diet: Semantically
89
consistent few-shot view synthesis." Proceedings of the IEEE/CVF International
Conference on Computer Vision. 2021. [3] Wang, Guangcong, et al. "Sparsenerf: Distilling depth ranking for few-shot novel view
synthesis." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. [4] Yu, Xianggang, et al. "Mvimgnet: A large-scale dataset of multi-view images." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023. [5] Schonberger, Johannes L., and Jan-Michael Frahm. "Structure-from-motion revisited." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [6] H. Xiong, S. Muttukuru, H. Xiao, R. Upadhyay, P. Chari, Y. Zhao, et al. SparseGS:
Sparse View Synthesis using 3D Gaussian Splatting. International Conference on 3D
Vision, 2025, Singapore, IEEE, 2025: 1-10
[7] Y. Hong, K. Zhang, J. Gu, S. Bi, Y. Zhou, D. Liu, et al. LRM: Large Reconstruction
Model for Single Image to 3D, in: International Conference on Learning Representations, Vienna, Austria, 2024, International Conference on Learning Representations, 2024: 1-25
[8] B. Ye, S. Liu, H. Xu, X. Li, M. Pollefeys, M. Yang, et al. No Pose, No Problem:
Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images, in: International
Conference on Learning Representations, Singapore Expo, Singapore, 2025, International
Conference on Learning Representations, 2025: 1-25
[9] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, et al. Structured 3D Latents for
Scalable and Versatile 3D Generation. ArXiv, 2024, 2412.01506
[10]
指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.50 动态场景重建-分割-理解一体化方法
任务描述：新视角生成和场景理解是近年来的热点研究主题之一，是 3D 计算机视觉领
域的关键任务，可应用于自动驾驶、AR/VR 中。然而现有三维场景场景重建的方法一般
聚焦于静态场景，实际应用中采集到的场景序列数据不可避免地包含动态运动目标，直
接将现有场景重建方法应用于动态场景往往难以取得满意的结果。本课题的研究目的在
90
于探索动态场景重建-分割-理解一体化方法。
数据描述：
采用 D-Nerf（https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0）、HyperNeR
F（https://github.com/google/hypernerf/releases/tag/v0.1）等数据集。
参考文献：
[1] Labe, I.; Issachar, N.; Lang, I.; and Benaim, S. 2024. DGD: Dynamic 3D Gaussians
Distillation. In ECCV. [2] Barron, J. T.; Mildenhall, B.; Verbin, D.; Srinivasan, P. P.; and Hedman, P. 2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In CVPR. [3] Cao, A.; and Johnson, J. 2023. Hexplane: A Fast Representation for Dynamic Scenes. In
CVPR. [4] Chen, G.; and Wang, W. 2024. A Survey on 3D Gaussian Splatting. arXiv:2401.03890.
91
[5] Chen, Y.; Chen, Z.; Zhang, C.; Wang, F.; Yang, X.; Wang, Y.; Cai, Z.; Yang, L.; Liu, H.;
and Lin, G. 2024. GaussianEditor: Swift and Controllable 3D Editing with Gaussian
Splatting. In CVPR. [6] Duan, Y.; Wei, F.; Dai, Q.; He, Y.; Chen, W.; and Chen, B. 2024. 4D-Rotor Gaussian
Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes. arXiv:2402.03307. [7] Fridovich-Keil, S.; Yu, A.; Tancik, M.; Chen, Q.; Recht, B.; and Kanazawa, A. 2022. Plenoxels: Radiance Fields without Neural Networks. In CVPR. [8] Gao, K.; Gao, Y.; He, H.; Lu, D.; Xu, L.; and Li, J. 2023. NeRF: Neural Radiance Field
in 3D Vision, A Comprehensive Review. arXiv:2210.00379. 指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.51 伪装目标检测
任务描述：自然界中许多生物都具有“伪装”的本领，将自身融入周围环境避免被识别和
捕食。与通用目标检测以及显著性目标检测不一样，伪装目标检测关注将伪装目标和背
景进行二分类分割。具体而言，本题目需要熟悉 2～3 个当前流行的伪装检测算法模型
（如 SINet、ANet、PopNet 等），并将其应用到本任务中，对检测性能指标（如 S-measure、
E-Measure、weighted F-measure、MAE）进行汇报并分析提升检测性能和造成检测失败
的原因。
数据描述：可以采用 COD10K、CAMO 等数据集进行公平比较和分析。COD10K(可参
考：https://github.com/DengPingFan/SINet?tab=readme-ov-file#4-proposed-cod10k-datasets)
数据集包含 6000 个训练样本和 4000 个测试样本。
92
CAMO 数据集（https://github.com/ltnghia/CAMO）包含 2000 个训练样本和 500 个
测试样本。
参考文献：
[1] D.-P. Fan, G.-P. Ji, G. Sun, M.-M. Cheng, J. Shen, and L. Shao,“Camouflaged object
detection,” CVPR, 2020. [2] Y. Lyu, J. Zhang, Y. Dai, A. Li, B. Liu, N. Barnes, and D.-P. Fan,“Simultaneously localize, segment and rank the camouflaged objects,” CVPR, 2021. [3] M. Zhang, S. Xu, Y. Piao, D. Shi, S. Lin, and H. Lu, “Preynet: Preying on camouflaged
objects,” ACMMM, 2022. [4] Y. Sun, S. Wang, C. Chen, and T.-Z. Xiang, “Boundary-guided camouflaged object
93
detection,” in IJCAI, 2022. [5] Y. Pang, X. Zhao, T.-Z. Xiang, Z. Lihe, and H. Lu, “Zoom in and out: A mixed-scale
triplet network for camouflaged object detection,” CVPR, 2022. [6] Z. Wu, D. P. Paudel, D.-P. Fan, J. Wang, S. Wang, C. Demonceaux, R. Timofte, and L. V. Gool, “Source-free depth for object pop-out,” ICCV, 2023. 指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.52 增量学习目标检测
任务描述：目标检测是计算机视觉和人工智能领域最热点的研究主题之一。然而现有方
法大多依赖于有限类别的海量高质量标注数据，而实际开放应用场景中会不断的采集到
新类别的新样本数据。本课题的研究目的在于探索不同的增量目标检测方法，并提升其
在增量学习过程中兼顾新增类别可塑性的同时对旧数据保持抗遗忘的能力。。
数据描述：采用 Pascal VOC 2007 和 MS-COCO 数据集。PASCAL VOC 2007 数据集
包含 20 个类别的 9,963 张图像，其中 50%作为训练集和验证集，剩余半数留作测试集；
MS-COCO 是一个更具挑战性的数据集，涵盖 80 个物体类别。在类增量目标检测中，
我们将数据集划分为不同的增量任务序列。PASCAL VOC 2007 被划分为两阶段与多阶
段增量任务设定。
 两阶段增量设定中，任务被划分为 19-1 类、15-5 类、10-10 类和 5-15 类四种
模式，分别对应每次新增 1、5、10 及 15 个类别的学习过程。
 多阶段增量设定则进一步细分为 10-5 类、5-5 类、10-2 类、15-1 类和 10-1 类
五种模式，分别包含 3、4、6、6 和 11 个任务。
94
参考文献：
[1] Nader Asadi, MohammadReza Davari, Sudhir Mudur, Rahaf Aljundi, and Eugene
Belilovsky. Prototype-sample relation distillation: towards replay-free continual learning. In International Conference on Machine Learning (ICML), 2023
[2] rancisco M Castro, Manuel J Mar´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and
Karteek Alahari. End-to-end incremental learning. In European Conference on Computer
Vision (ECCV), 2018
[3] Fabio Cermelli, Antonino Geraci, Dario Fontanel, and Barbara Caputo. Modeling missing
annotations for incremental learning in object detection. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2022. [4] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning
efficient object detection models with knowledge distillation. Advances in neural
information processing systems (NeurIPS), 2017. [5] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting
in classification tasks. Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 44 (7):3366–3385, 2021. [6] Dipam Goswami, Yuyang Liu, Bartłomiej Twardowski, and Joost van de Weijer. Fecam:
Exploiting the heterogeneity of class distributions in exemplar-free continual learning. Advances in Neural Information Processing Systems (NeurIPS), 2024
[7] Akshita Gupta, Sanath Narayan, KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and
95
Mubarak Shah. Ow-detr: Open-world detection transformer. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2022. [8] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open world object detection. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2021. 指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.53 红外弱小目标检测
任务描述：
随着数字图像处理和计算机视觉技术的飞速发展,红外目标检测技术开始广泛应用
于天文观测、遥感、跟踪等诸多邻域。但由于红外弱小目标具有距离远、成像面积小、
形状特征微弱、细节特征缺失等特点,红外弱小目标智能检测仍存在诸多挑战。本课题需
要熟悉 2～3 个当前流行的基于深度学习的红外弱小目标检测算法模型，并将其应用到
本任务中，与传统红外弱小目标检测算法进行对比，对检测性能指标进行汇报并分析。
数据描述：
采 用 IRDST 、 NUST-SRST 、 NUAA-SIRST 、 NUDT-SIRST 、 IRSTD-1K 、
NUDT-SIRST-Sea 等数据集（参考 https://github.com/XinyiYing/BasicIRSTD/tree/main）进
行测试和分析。其中 IRDST 包含 142727 帧真实或仿真的图像，每帧图像包括由精到粗
的像素级、边界框级和中心像素位置的标签。
96
参考文献：
[1] Y. Qin, L. Bruzzone, C. Gao, and B. Li, “Infrared small target detection based on facet
kernel and random Walker,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 9, pp. 7104–7118, Sep. 2019. [2] C. Q. Gao, D. Meng, Y. Yang, Y. Wang, X. Zhou, and A. G. Hauptmann, “Infrared
patch-image model for small target detection in a single image,” IEEE Trans. Image
Process., vol. 22, no. 12, pp. 4996–5009, Dec. 2013. [3] L. Zhang and Z. Peng, “Infrared small target detection based on partial sum of the tensor
nuclear norm,” Remote Sens., vol. 11, no. 4, p. 382, Feb. 2019. [4] S. Yao, Y. Chang, and X. Qin, “A coarse-to-fine method for infrared small target
detection,” IEEE Geosci. Remote Sens. Lett., vol. 16, no. 2, pp. 256–260, Feb. 2019. [5] A. Farhadi and J. Redmon, “YOLOv3: An incremental improvement,” in Proc. Comput. Vis. Pattern Recognit., 2018, pp. 1804–2767. [6] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “YOLOX: Exceeding Yolo series in 2021,” 2021, arXiv:2107.08430. [7] H. Wang, L. Zhou, and L. Wang, “Miss detection vs. false alarm: Adversarial learning for
small object segmentation in infrared images,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 8509–8518. [8] Y. Dai, Y. Wu, F. Zhou, and K. Barnard, “Attentional local contrast networks for infrared
small target detection,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 11, pp.
97
9813–9824, Nov. 2021. [9] B. Li et al., “Dense nested attention network for infrared small target detection,” 2021, arXiv:2106.00487. 指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.54 小样本目标检测
任务描述：针对目标检测任务，现有深度学习方法多数仍以数据驱动，即需要成千上万
的类别实例训练，使得模型能够“认识”类别的新实例。小样本目标检测通过对大量公开
可用数据(基类)进行预训练检测模型后，旨在仅对新类目标做少量实例标注后就能快速
实现目标检测，具有广阔应用前景。小样本目标检是一个相对新兴的研究领域，大多数
方法仅在过去两年内发表。本课题需要梳理小样本目标检测的发展脉络，熟悉 2～3 个
当前流行的小样本目标检测算法模型，并将其应用到本任务中，对检测性能指标进行汇
报并分析。
数据描述：采用 PASCAL VOC 数据集和 MS-COCO 数据集进行分析测试（参考
https://github.com/ucbdrive/few-shot-object-detection）。PASCAL VOC 数据集包括三个不
同的数据集划分，每个划分随机地将 20 类目标分为 15 个基类目标和 5 个新类目标。对
于每个新类，1、2、3、5、10 个样本用于微调的设置被采用，采用 AP(IoU=0.5)的 nAP50
作为新类目标的检测性能评价指标。MS-COCO 数据集包括 80 个类别，其中 20 个为新
类目标，60 个为基类目标，对于每个新类，1、2、3、5、10、30 个样本用于微调的设
置被采用，采用 COCO 风格的 AP 作为目标检测性能评价指标。
98
参考文献：
[1] Xin Wang, Thomas E Huang, Trevor Darrell, Joseph E Gonzalez, and Fisher Yu. Frustratingly simple few-shot object detection. In ICML, 2020. [2] Limeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, JiananWu, and Chi Zhang. Defrcn:
Decoupled faster r-cnn for few-shot object detection. In ICCV, 2021. [3] Shuang Wu, Wenjie Pei, Dianwen Mei, Fanglin Chen, Jiandong Tian, and Guangming Lu. Multi-faceted distillation of base-novel commonality for few-shot object detection. In
ECCV, 2022. [4] Jingyi Xu, Hieu Le, and Dimitris Samaras. Generating features with increased
crop-related diversity for few-shot object detection. In CVPR, 2023. [5] Jiaming Han, Yuqiang Ren, Jian Ding, Ke Yan, and Gui- Song Xia. Few-shot object
detection via variational feature aggregation. AAAI, 2023. [6] Adrian Bulat, Ricardo Guerrero, Brais Martinez, and Georgios Tzimiropoulos. Fs-detr:
Few-shot detection transformer with prompting and without re-training. In ICCV, 2023. [7] Xiaonan Lu, Wenhui Diao, Yongqiang Mao, Junxi Li, Peijin Wang, Xian Sun, and Kun
Fu. Breaking immutable: information-coupled prototype elaboration for few-shot object
detection. In AAAI, 2023. [8] Qi Fan, Chi-Keung Tang, and Yu-Wing Tai. Few-shot object detection with model
calibration. In ECCV, 2022. 指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.55 旋转目标检测
任务描述：相比通用目标检测（水平框检测），旋转目标检测在通用目标检测的基础上
同时输出目标的朝向角，能实现对目标更准确地定位。本课题需要对目标旋转角度的表
达方法进行调研、梳理和分析，熟悉 4～6 个当前流行的旋转目标检测算法模型（如 RoI
Trans、Gliding Vertex、CSL、ReDet、GWD、KLD 等），并将其应用到本任务中，对检
测性能指标进行汇报并分析。
数 据 描 述 ： 采 用 DOTA 数 据 集 进 行 评 测 。 DOTA 数 据 集 （ 参 考
https://captain-whu.github.io/DOTA/dataset.html）包括 2806 个航拍图像（图像尺寸从
99
800×800 到 4000×4000），包括 188282 个目标实例和 15 个目标类别。HRSC2016 数据集
（参考）包括 436 张训练图像、181 张验证图像和 444 张测试图像。
参考文献：
[1] J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu, “Learning RoI transformer for oriented
object detection in aerial images,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. (CVPR), Jun. 2019, pp. 2849–2858. [2] X. J. Pan et al., “Dynamic refinement network for oriented and densely packed object
detection,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2020, pp. 11207–11216. [3] Y. Xu et al., “Gliding vertex on the horizontal bounding box for multioriented object
detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 4, pp. 1452–1459, Apr.
100
2021. [4] X. Yang, J. Yan, Q. Ming, W. Wang, X. Zhang, and Q. Tian, “Rethinking rotated object
detection with Gaussian Wasserstein distance loss,” in Proc. 38th Int. Conf. Mach. Learn., 2021, pp. 11830–11841. [5] X. Yang et al., “Learning high-precision bounding box for rotated object detection via
Kullback–Leibler divergence,” in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021, pp. 18381–18394. [6] X. Yang et al., “Detecting rotated objects as Gaussian distributions and its 3-D
generalization,” IEEE Trans. Pattern Anal. Mach. Intell., early access, Aug. 8, 2022, doi:
10.1109/TPAMI.2022.3197152. [7] X. Yang and J. C. Yan, “Arbitrary-oriented object detection with circular smooth label,”
in Proc. Eur. Conf. Comput. Vis., 2020, pp. 677–694. [8] X. Yang, L. Hou, Y. Zhou, W. Wang, and J. Yan, “Dense label encoding for boundary
discontinuity free rotation detection,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. (CVPR), Jun. 2021, pp. 15819–15829. [9] X. Xie, G. Cheng, J. Wang, X. Yao, and J. Han, “Oriented R-CNN for object detection,”
in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 3520–3529. [10] Y. Wang, Z. Zhang, W. Xu, L. Chen, G. Wang, L. Yan, S. Zhong, X. Zou, “Learning
Oriented Object Detection via Naïve Geometric Computing”， IEEE Transactions on
Neural Networks and Learning Systems, 2023. 指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.56 人-物-交互检测
任务描述：人-物交互行为（Human-Object Interaction，HOI）检测通过挖掘场景中人与
人或人与物体之间的交互关系，可以提供更加丰富、更加细粒度的场景理解信息。人- 物交互行为检测任务，不仅需要精准地定位出图像中存在交互作用的人和物体的位置，
还要求正确地识别它们之间的交互动作类别。具体而言，本题目需要熟悉 2～3 个当前
流行的 HOI 检测算法模型（如 HO-RCNN、PPDM、QPIC 等），并将其应用到本任务中，
对检测性能指标（如 mAP）进行汇报并分析提升检测性能和造成检测失败的原因。
101
数据描述：
可 以 采 用 V-COCO 、 HICO-DET 、 HOI-A 等 数 据 集 进 行 公 平 比 较 和 分 析 。
V-COCO(Verbs in COCO，可参考：https://github.com/s-gupta/v-coco)数据集由斯坦福大学
的计算机视觉实验室在 COCO 数据集的基础上构建的，在领域发展的初期极大地推动了
人-物交互行为检测算法的进步。作为 COCO 数据集的子集，该数据集包含的 10369 张
图像全部从 COCO 数据集中抽取出来的，并针对人-物交互行为检测任务做了专门的标
注和整理；整个数据集按近似 1:1:2 的比例划分为训练集、验证集、测试集，其中，训
练集包含 2533 张图像、验证集包含 2867 张图像、测试集包含 4969 张图像。V-COCO
数据集中共有 16199 个标注框，涵盖了 29 个交互动作类别，80 个物体类别。
HICO-DET 数据集是由密西根大学牵头收集创建的，于 2018 年公开发布。
HICO-DET（可参考：https://public.websites.umich.edu/~ywchao/hico/）是人-物交互行为
检测领域迄今为止最大的数据集，一经发布便得到了相关研究人员的青睐和广泛引用，
成为了评估算法综合性能的重要参考依据。该数据集共有 47776 张图像，15 万个 HOI
实例标注边框；按照 4:1 的比例将数据集划分为训练集和测试集；此数据集涵盖了生活
中常见的 80 个类别的物体、117 个交互动作，排除一些如“人-吃-自行车”、“人-骑-蛋糕” 等不合理的类别后，共保留 600 个 HOI 类别。
HOI-A （ Human-Object Interaction for Application ， 可 参 考 ：
https://github.com/YueLiao/PPDM?tab=readme-ov-file）是一个以实际应用为导向的数据集，
与上述 HICO-DET、V-COCO 数据集相比，HOI-A 在 HOI 类别的选择上更注重实际应
用需求，挑选了实际应用场景中出现频率较高的 HOI 类别，并对其进行了人工标注。同
时，为了增加数据的多样性，HOI-A 数据集中每个交互动作都包含了三种场景包括室内、
102
室外和车内，三种光照条件包括暗光、自然光和强光，以及各种人体姿势和不同的拍摄
角度。HOI-A 数据集共包含 38,668 张标注图像，11 种物体类别和 10 种交互动作类别。
具体来说，该数据集包含了 43820 个人体实例，60,438 个物体实例和 96,160 个交互实例。
参考文献：
[1] L. Dong, Z. Li, K. Xu, Z. Zhang, L. Yan, S. Zhong, et al. Category-Aware Transformer
Network for Better Human-Object Interaction Detection. In: 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 19-24
June 2022, Proceedings of IEEE, 2022: 19516-19525
[2] Z. Hou, X. Peng, Y. Qiao, D. Tao. Visual Compositional Learning for Human-Object
Interaction Detection. In: 2020 European Conference on Computer Vision (ECCV), Glasgow, UK, 23-28 August 2020, Springer International Publishing, 2020: 584-600
[3] C. Gao, J. Xu, Y. Zou, J. B. Huang. DRG: Dual Relation Graph for Human-Object
Interaction Detection. In: 2020 European Conference on Computer Vision (ECCV), Glasgow, UK, 23-28 August 2020, Springer International Publishing, 2020: 696-712
[4] C. Zou, B. Wang, Y. Hu, J. Liu, Q. Wu, Y. Zhao, et al. End-to-End Human Object
Interaction Detection with HOI Transformer. In: 2021 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20-25 June
2021, Proceedings of IEEE, 2021: 11820-11829
[5] T. Wang, T. Yang, M. Danelljan, F. S. Khan, X. Zhang, J. Sun. Learning Human-Object
Interaction Detection Using Interaction Points. In: 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), Virtual Event, 19-25 June 2020, Proceedings of IEEE, 2020: 4115-4124
[6] M. Chen, Y. Liao, S. Liu, Z. Chen, F. Wang, C. Qian. Reformulating HOI Detection as
Adaptive Set Prediction. In: 2021 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), Nashville, TN, USA, 20-25 June 2021, Proceedings of
103
IEEE, 2021: 9000-9009
[7] B. Kim, J. Lee, J. Kang, E. S. Kim, H. J. Kim. HOTR: End-to-End Human-Object
Interaction Detection with Transformers. In: 2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20-25 June 2021, Proceedings of IEEE, 2021: 74-83
[8] M. Tamura, H. Ohashi, T. Yoshinaga. QPIC: Query-Based Pairwise Human-Object
Interaction Detection with Image-Wide Contextual Information. In: 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20-25 June 2021, Proceedings of IEEE, 2021: 10405-10414
指导老师：邹旭，zoux@hust.edu.cn，qq：275666087
5.57 高分辨率时-空-谱融合计算成像
问题描述：高分辨率时空谱成像在生物医学、航天遥感领域发挥着关键作用。传统‘所
见即所得’图像获取方式受限于传感器性能和传输带宽，难以同时获得高时间、空间、
光谱分辨率的图像。计算成像技术应运而生，通过前端光学调制、后端计算解码实现高
分辨率成像。本题要求完成多光谱-高光谱双模融合成像相关文献梳理形成综述文档，
重点研究多光谱 + 高光谱数据的双模融合计算成像方法，并对 MHF-Net [5]算法重新训
练并测试，从性能、速度、泛化性等维度评价。
数据描述：CAVE 数据集 https://www.cs.columbia.edu/CAVE/databases/multispectral/
104
The database consists of 32 scenes, divided into 5 sections. Each scene has an associated
zip file. These zip files include full spectral resolution reflectance data from 400nm to 700nm
at 10nm steps (31 bands total). Each band is stored as a 16-bit grayscale PNG image. Image
filenames are of the format 'object_ms_01.png', where the '01' at the end signifies that this is
the first image (captured at 400nm). Thus, '02' corresponds to 410nm, and so on, until '31' for
700nm. Each scene also contains a single representative color image, displayed using sRGB
values rendered under a neutral daylight illuminant (D65). Harford 数据集 https://vision.seas.harvard.edu/hyperspec/d2x5g3/
50 Indoor & outdoor images under daylight: CZ_hsdb.tgz (5.3GB)
27 Indoor images under artificial & mixed illumination: CZ_hsdbi.tgz (2.2GB)
参考文献：
[1] Lizhi Wang, Zhiwei Xiong, et al. High-Speed Hyperspectral Video Acquisition By
Combining Nyquist and Compressive Sampling [J]. IEEE transactions on pattern analysis and
machine intelligence, 2019. [2] Yang Liu, Xin Yuan, et al. Rank Minimization for Snapshot Compressive Imaging [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [3] Ming Cheng, Zhan Ma, et al. A Dual Camera System for High Spatiotemporal Resolution
Video Acquisition [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [4] Yibo Xu, Liyang Lu, et al. A Compressive Hyperspectral Video Imaging System using a
Single-pixel Detector [J]. Nature Communications, 2024. [5]Qi Xie, Minghao Zhou, et al. MHF-Net: An interpretable deep network for multispectral
and hyperspectral image fusion [J]. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2022. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
105
5.58 低光图像饱和去模糊
问题描述：结构噪声（线状、斑状）与模糊广泛存在与各类成像系统中，成像质量
严重下降，探测目标遭到破坏，制约图像后续使用。解决结构噪声干扰下图像去模糊问
题，可提升图像质量，保障图像高层任务性能。本题要求完成低光饱和图像去模糊相关
文献梳理形成综述文档，重点研究饱和情况下图像去模糊复原方法，掌握 OID[3]实现细
节，从性能、速度、泛化性等维度评价。
数据描述：RealBlur Dataset： http://cg.postech.ac.kr/research/realblur/
This work present a large-scale dataset of real-world blurred images and ground truth
sharp images for learning and benchmarking single image deblurring methods. To collect the
dataset, the authors build an image acquisition system to simultaneously capture
geometrically aligned pairs of blurred and sharp images, and develop a post-processing
method to produce high-quality ground truth images.
RealBlur：1）training set : 3,758 image pairs of 182 different scenes. 2）test set : 980 image
pairs of 50 different scenes. 参考文献：
[1] Qi Shan, Jia Jiaya, Aseem Agarwala. High-quality Motion Deblurring From A Single
Image. ACM Transactions on Graphics, 2008, 27(3): 557-566. [2] Cao Shuning, Fang Houzhang, Chen Linqun, et al. Robust blind deblurring under stripe
noise for remote sensing images. IEEE Trans. on Geoscience and Remote Sensing, 2022, 60: 1-17.
106
[3] Liang Chen , Faming Fang, et al. Outlier Identifying and Discarding in Blind Image
Deblurring. European Conference on Computer Vision, 2020, 2478-2486. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
5.59 基于语言-视觉模型的图像复原方法
问题描述：实际场景中，可见光图像易受到雨天、雾天、雪天等恶劣天气退化效应
干扰，现有方法多集中于利用图像的单一模态信息进行复原，忽视了其他模态的潜在作
用。本课题旨在探索语言文本这一额外模态在恶劣天候图像复原中的作用，借助语言- 视觉模型，利用语言描述提供的语义线索与图像内容的协同，提升恶劣天气图像视觉质
量，进而提升计算机视觉系统对恶劣天气退化的鲁棒性。
数据描述：https://github.com/Algolzw/daclip-uir
107
DA-CLIP 所采用的混合退化图像数据集是基于多个已有公开数据集收集整理而成，
旨在提升模型在多种图像退化场景下的图像复原能力。该数据集主要来源于广泛应用的
视觉图像数据集，包括 BSD、GoPro、LOL、RESIDE、Synthetic Rain 数据集及 AWMM-100k
等。这些数据集提供了大量成对的高质量图像与其对应的退化图像，涵盖了包括雨、雪、
雾霾等天气相关退化，以及运动模糊、JPEG 压缩、噪声干扰、阴影遮挡与图像缺失等
多种退化类型，具备高度的多样性与代表性。数据集中所涉及的图像覆盖了丰富的场景
内容，包含室内与室外环境、不同的光照条件以及动态与静态目标等。如雾霾退化图像
主要选自 I-Haze 和 O-Haze 数据集，分别针对室内与室外场景，每个子集均包含约 45
对图像，图像分辨率为 4560×3040 像素；雨天退化图像主要来源于 RainCityscapes
数据集，包含 9437 张带有真实雨天效果的图像，分辨率为 2048×1024 像素；而降雪
类图像则主要使用 Snow100K 数据集，涵盖约 100,000 张含雪景图像，分辨率为 1280
×720 像素。为实现文本引导的图像修复，每对高质量与低质量图像均配备由 BLIP 框
架自动生成的图像描述文本，进而构建图像-文本-退化类型的三元组训练样本。这种跨
模态配对方式不仅增强了模型对退化类型的识别能力，也为引导式图像复原任务提供了
有效的数据支持。
参考文献：
[1] Z. Luo, F. Gustafsson, Z. Zhao, et al. Controlling Vision-Language Models for
Multi-Task Image Restoration. International Conference on Learning Representation. 2024
[2] H. Yang, L. Pan, Y. Yang, et al. Language-driven All-in-one Adverse Weather
Removal. IEEE Conference on Computer Vision and Pattern Recognition. 2024:
24902-24912. [3] Y. Wei, Y. Zhang, K. Li, et al. Leveraging vision-language prompts for real-world
image restoration and enhancement. Computer Vision and Image Understanding, 2024:
104222. [4] X. Xu, S. Kong, T. Hu, et al. Boosting Image Restoration via Priors from Pre-trained
Models. IEEE Conference on Computer Vision and Pattern Recognition. 2024: 2900-2909. [5] B. Gella, H. Zhang, R. Upadhyay, et al. WeatherProof: Leveraging Language Guidance
for Semantic Segmentation in Adverse Weather. arXiv preprint:2403.14874, 2024. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
108
5.60 事件相机高速高动态成像
问题描述：高速高动态范围成像可以为视觉感知任务提供更丰富的时空信息。传统
帧相机采用全局同步曝光的方式积分成像，难以同时适应场景中局部过亮和过暗区域对
于曝光的要求，导致其动态范围较低；且积分成像需要一定时间累积能量，难以在无补
光的自然场景高速成像。而事件相机各像素异步差分触发，时间分辨率达到 1 微秒，动
态范围达到 120dB，具有同时进行高速高动态成像的能力。本课题要求完成基于事件相
机的高速高动态成像相关文献梳理形成综述文档，重点研究事件 + 图像的双模融合计
算成像方法，对算法[5]重新训练并测试，并以合理的方式评价融合成像的质量。
数据描述：BS-ERGB 数据集 https://uzh-rpg.github.io/timelens-pp/
The authors built an experimental setup with a global shutter RGB Flir 4096x2196
camera and a Prophesee Gen4M 1280x720 event camera arranged with the beam splitter. The
dataset consists of 123 diverse and challenging scenes with varying depth. And the dataset is
divided into 78 training scenes, 19 validation scenes, and 26 test scenes. ERF-X170FPS 数据集 https://github.com/intelpro/CBMNet
109
The authors build the large-scale dataset named ERF-X170FPS (High Resolution Events
and RGB Frames with eXtreme Motions at 170FPS) with the beam-splitter-based camera rigs
comprising of the Prophesee Gen4 event cameras (1280×720) and Blackfly-S global shutter
cameras (1440×1080, maximum 226fps). These two cameras are hardware-level synchronized
using a micro-controller (the external trigger). This dataset contains a total of 36 sequences
containing 990 frames per sequence. 参考文献：
[1] Tulyakov S, Gehrig D, Georgoulis S, et al. Time lens: Event-based video frame
interpolation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2021: 16155-16164. [2] Tulyakov S, Bochicchio A, Gehrig D, et al. Time lens++: Event-based frame
interpolation with parametric non-linear flow and multi-scale fusion[C]//Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 17755-17764. [3] Kim, Taewoo, et al. Event-based video frame interpolation with cross-modal
asymmetric bidirectional motion fields. Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2023. [4] Han J, Zhou C, Duan P, et al. Neuromorphic camera guided high dynamic range
imaging[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2020: 1730-1739. [5] Wenming Weng, Yueyi Zhang, Zhiwei Xiong. Event-based Blurry Frame
Interpolation under Blind Exposure. CVPR, 2023. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
110
5.61 远距高清晰成像
问题描述：远距成像是大气科学、天文观测、军事国防等领域不可或缺的关键技术
手段。然而，长距离观测不可避免会受到大气湍流的干扰。具有不同尺度涡流的湍流介
质在光波的传播过程中会引起折射率的波动，从而对原有的空间关系、相位关系和光程
产生干扰，带来成像模糊、畸变扭曲的退化。本题要求完成大气湍流退化图像复原方法
相关文献梳理形成综述文档，重点研究可见光数据的大气湍流校正方法，并对 DATUM
[3]算法重新训练并测试，从算法原理实测性能、推理速度、泛化性等维度评价算法。
数据描述：ATD 数据集 https://pan.baidu.com/s/1vErExH0U5oRDU3-n04S5mg?pwd=
vakg#list/path=%2F
To fully demonstrate the performances of the different models in neutralizing the impact
of turbulence, we constructed a dataset composed of 5,201 real-world video sequences with
78,015 frames containing thermally driven turbulence. The real-world data were composed of
two parts. The first contained 4,303 video sequences shot in Australia and 898 clips
containing turbulence sampled from a series of documentaries. In Brisbane, Australia, we
used a Nikon D750 camera to capture images in street view when the local temperature was ~93 °F to 99 °F. Among the collected data, 4,303 video sequences involving typical scenes
(including buildings, vehicles and pedestrians) were sampled and constitute the second part of
the dataset. These were mainly recorded in Queen Street (153◦ 2′E,−27◦ 28′N), Victoria
Bridge and Pinelands Road (152◦ 45′E,−27◦ 35′N). The related camera parameter settings
were from auto mode, with shutter speed of 1/250–1/200, aperture size of 1/11–1/9 and ISO
of 100–300. The resolution of the original video was 1,920 × 1,080, and the sequences used in
the dataset were cropped to an appropriate size. Real-world turbulence, which is influenced by
various complicated factors as well as moving objects at different distances, could further
validate the efficacy of the proposed and comparison models.
111
TMT 数据集 https://github.com/xg416/TMT?tab=readme-ov-file
ATSyn 数据集 https://xg416.github.io/DATUM/
参考文献：
[1] Jin D, Chen Y, Lu Y, et al. Neutralizing the impact of atmospheric turbulence on
complex scene imaging via deep learning[J]. Nature Machine Intelligence, 2021, 3(10):
876-884. [2] Zhang X, Mao Z, Chimitt N, et al. Imaging through the atmosphere using turbulence
mitigation transformer[J]. IEEE Transactions on Computational Imaging, 2024. [3] Zhang X, Chimitt N, Chi Y, et al. Spatio-Temporal Turbulence Mitigation: A
Translational Perspective. Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2024. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
5.62 远距透雾成像
问题描述：远距成像过程中不可避免会受到严重大气散射效应影响，导致远距场景
对比度以及纹理细节极度劣化。现有单帧图像去雾方法主要依赖手工先验或者数据驱动
方式，从退化图像内寻找线索从而恢复出无雾的清晰图像。但是此类方法无法处理远距
成像散射效应导致的严重场景信息丢失。为了找回丢失的信息，受益于近红外波段天然
的透雾能力，部分方法提出将雾天图像增强问题转化为可见光+近红外图像融合问题，
从额外的近红外波段找回可见光中丢失的场景内容以及纹理细节信息。本题要求完成雾
天图像复原/增强以及双模图像融合相关文献梳理形成综述文档，重点研究可见光+近红
外双模融合远距雾天图像增强方法，对 SwinFusion [3]和 CDDFuse[4] 算法重新训练并
测试，并从性能、速度、泛化性等维度评价算法。
数据描述：RGB-NIRScene 数据集
https://www.epfl.ch/labs/ivrl/research/downloads/rgb-nir-scene-dataset/
112
This dataset consists of 477 images in 9 categories captured in RGB and Near-infrared (NIR). The images were captured using separate exposures from modified SLR cameras, using
visible and NIR filters. For more info on NIR photography, see the references below. The
scene categories are: country, field, forest, indoor, mountain, oldbuilding, street, urban, water. RANUS 数据集
https://drive.google.com/file/d/1WJ7rcCeMBPy9Qb2c_pwI0rUIlFqotkzT/view
Database contains 40k spatially-aligned RGB-NIR scenes captured from a vehicle driven
across various urban, rural and campus roads consisting of buildings and natural landscapes. 参考文献：
[1] Schaul L, Fredembach C, Süsstrunk S. Color image dehazing using the near-infrared. 2009
International Conference on Image Processing. 2009: 1629-1632. [2] Liu L, Wang F, Jung C. LRINet: Long-range imaging using multispectral fusion of RGB
and NIR images. Information Fusion, 2023, 92: 177-189. [3] Ma J, Tang L, Fan F, et al. SwinFusion: Cross-domain long-range learning for general
image fusion via swin transformer. IEEE/CAA Journal of Automatica Sinica, 2022, 9(7):
1200-1217. [4] Zhao Z, Bai H, Zhang J, et al. Cddfuse: Correlation-driven dual-branch feature
decomposition for multi-modality image fusion. Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2023: 5906-5916.
113
指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
5.63 雨雪雾霾恶劣天气图像统一增强方法
问题描述：恶劣天气图像增强对提升计算机视觉系统在不同天气环境下的适应性具
有重要作用，在自动驾驶、侦察监视领域具有极大应用前景。现有图像增强方法往往专
注于增强某一类特定恶劣天气退化（如雨、雾、雪）。然而，实际场景中不同种类退化
可能不定时出现。因此，能够同时处理多种退化的恶劣天气图像统一增强模型具有重大
研究意义。本题要求完成恶劣天气图像增强相关文献梳理形成综述文档，重点研究恶劣
天气图像统一增强模型与方法，对 WeatherDiff[5]算法重新训练并测试，从性能、速度、
泛化性等维度评价算法。
数据描述：RESIDE 数据集
https://sites.google.com/view/reside-dehaze-datasets/reside-standard
LHPRain 数据集
https://yunguo224.github.io/LHP-Rain.github.io/
114
Snow100K 数据集
https://sites.google.com/view/yunfuliu/desnownet
This dataset consists of 1) 100k synthesized snowy images, 2) corresponding snow-free
ground truth images and 3) snow masks, and 4) 1,329 realistic snowy images. The snow-free
and snowy realistic images 2) and 4) are downloaded from Flickr, and we manually validate
each sample to see whether it is snow-free or not. The largest size of the image boundary is
640 pixels. This dataset consists of three subsets as per the variations inside single image. 1)
Snow100K-S: Samples in this subset are only synthesized with small snow particles. 2)
Snow100K-M: It contains the samples that are of both snow particles in small and medium
115
sizes. 3) Snow100K-L: Snow particles of sizes small, medium, and large are all used for
synthesizing samples. Each subset contains around 33k images. 参考文献：
[1] Zhu Y, Wang T, Fu X, et al. Learning weather-general and weather-specific features for
image restoration under multiple adverse weather conditions. Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 2023: 21747-21758. [2] Valanarasu J, Yasarla R, Patel M. Transweather: Transformer-based restoration of images
degraded by adverse weather conditions. Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2022: 2353-2363. [3] Patil W, Gupta S, Rana S, et al. Multi-weather Image Restoration via Domain Translation. Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023:
21696-21705. [4] Kulkarni A, Phutke S, Murala S. Unified transformer network for multi-weather image
restoration. European Conference on Computer Vision. 2022: 344-360. [5] Ozan Özdenizci and Robert Legenstein. Restoring Vision in Adverse Weather Conditions
with Patch-based Denoising Diffusion Models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2023. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
5.64 零样本单目度量深度估计
问题描述：单目深度估计是计算机视觉领域中的一个经典任务，可以分为度量（绝对）
深度估计和相对深度估计两个派系。度量深度估计能够估计物体的绝对物理单位的深度，
其优点是在如建图、导航、物体识别和三维重建等下游应用中具有实用价值，但度量深
度估计模型泛化能力较差，通常在特定的数据集上过拟合，而不能很好地推广到其他数
据集。而相对深度估计能够估计每个像素与其它像素的相对深度差异，可以在各种类型
环境中估计深度，但其深度无尺度信息，应用场景有限。本题要求完成单目度量深度估
计相关文献梳理形成综述文档，重点研究度量深度和相对深度结合的零样本单目度量深
度估计方法，掌握 ZoeDepth[3]实现细节，重新训练并在未参与训练的数据集上进行测
试，从误差、泛化性等维度评价算法。
116
数据描述：KITTI 数据集 https://www.cvlibs.net/datasets/kitti/
We take advantage of our autonomous driving platform Annieway to develop novel challenging
real-world computer vision benchmarks. Our tasks of interest are: stereo, optical flow, visual odometry, 3D object detection and 3D tracking. For this purpose, we equipped a standard station wagon with two
high-resolution color and grayscale video cameras. Accurate ground truth is provided by a Velodyne
laser scanner and a GPS localization system. Our datsets are captured by driving around the mid-size
city of Karlsruhe, in rural areas and on highways. Up to 15 cars and 30 pedestrians are visible per
image. Besides providing all data in raw format, we extract benchmarks for each task. For each of our
benchmarks, we also provide an evaluation metric and this evaluation website. Preliminary
experiments show that methods ranking high on established benchmarks such as Middlebury perform
below average when being moved outside the laboratory to the real world. Our goal is to reduce this
bias and complement existing benchmarks by providing real-world benchmarks with novel difficulties
to the community. NYU Depth v2 数据集 https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html
We present an approach to interpret the major surfaces, objects, and support relations of an indoor
scene from an RGBD image. Most existing work ignores physical interactions or is applied only to
tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests
is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute
a novel integer programming formulation to infer physical support relations. We offer a new dataset of
1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments
demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues
117
and inferred support lead to better object segmentation. 参考文献：
[1] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Localbins: Improving depth
estimation by learning local distributions. In European Conference on Computer Vision, pages
480–496. Springer, 2022. [2] Jun, Jinyoung, et al. "Depth map decomposition for monocular depth
estimation." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. [3] Shariq Farooq Bhat, et al. "Zoedepth: Zero-shot transfer by combining relative and metric
depth." arXiv preprint arXiv:2302.12288 (2023). [4] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. [5] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation
and support inference from rgbd images. In Computer Vision – ECCV 2012, pages 746760, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. 指导老师：昌毅，yichang@hust.edu.cn，qq 458449613
5.65 基于单细胞转录组数据的细胞状态转换分析
任务描述：细胞状态转换是生物发育、免疫响应及疾病进展等过程中的核心生物学现象。
通过单细胞转录组数据解析细胞状态动态变化（如分化、激活、功能重塑等），能够揭
示基因调控网络的时序性变化及驱动状态转换的关键分子特征。本课题拟利用单细胞转
录组测序数据，结合拟时序分析、聚类、降维等方法，区分稳定细胞和过渡细胞，构建
细胞状态转换轨迹，并量化细胞状态之间所有可能转变轨迹的可能性。
数据说明：使用单细胞测序数据，每个数据集的基本形式为 样本数*基因数 的矩阵，
可以使用[1][2]的方法模拟生成数据，同时，GEO 数据库
（https://www.ncbi.nlm.nih.gov/geo/）中有丰富的真实单细胞测序数据，具体的真
实数据可以参考[2-4]使用的数据。
118
参考文献：
[1] Cai M, Vesely A, Chen X, Li L, Goeman J J. NetTDP: permutation-based true
discovery proportions for differential co-expression network analysis[J]. Briefings in
Bioinformatics, 2022, 23(6): bbac417. [2] Zhou P, Wang S, Li T, Nie Q. Dissecting transition cells from single-cell transcriptome
data through multiscale stochastic dynamics[J]. Nature Communications, 2021, 12(1):
5609. [3] Sha Y, Wang S, Zhou P, Nie Q. Inference and multiscale model of
epithelial-to-mesenchymal transition via single-cell transcriptomic data[J]. Nucleic
Acids Research, 2020, 48(17): 9505–9520. [4] Yang X H, Goldstein A, Sun Y, Wang Z, Wei M, Moskowitz I P, Cunningham J M. Detecting critical transition signals from single-cell transcriptomes to infer
lineage-determining transcription factors[J]. Nucleic Acids Research, 2022, 50(16): e91 –e91. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.66 面向系统状态时序数据的状态转变临界预警信号识别
任务描述：临界预警是指在系统发生显著状态转变（例如故障、性能下降、模式切换等）
119
之前，提前识别出预示转变即将发生的关键信号。利用机器学习方法，处理系统状态的
时序数据，识别出系统发生状态转变前的临界预警信号，预测系统的临界点，并尝试区
分状态转变的类型，基于可解释机器学习方法进行解释分析。
数据说明：可以通过论文[1][2]提供的模拟数据和生成数据方法，生成训练集，数据为
表示系统状态的一维时序数据，包括不同的状态转变和不发生状态转变的数据，论文
[1][3]提供了实际系统的数据，可以用于验证有效性。
结果示意图如下：
120
参考文献：
[1] Bury T M, Sujith R I, Pavithran I, et al. Deep learning for early warning s
ignals of tipping points[J]. Proceedings of the National Academy of Sciences, 2021, 118(39): e2106140118. [2] Huang Y, Bathiany S, Ashwin P, et al. Deep learning for predicting rate-ind
uced tipping[J]. Nature Machine Intelligence, 2024, 6: 1556–1565. [3] Grziwotz F, Chang C W, Dakos V, et al. Anticipating the occurrence and ty
pe of critical transitions[J]. Science Advances, 2023, 9(1): eabq4558. [4] Liu Z, Zhang X, Ru X, et al. Early predictor for the onset of critical transi
tions in networked dynamical systems[J]. Physical Review X, 2024, 14(3): 0
31009. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.67 面向网络瓦解的节点重要性评估
任务描述：网络瓦解是指由于节点或边的失效（如攻击、故障或自然事件）导致网络功
能严重退化甚至崩溃的过程。节点重要性评估旨在识别对网络瓦解最关键的节点，量化
121
其失效对网络全局或局部性能的影响。利用图机器学习方法学习节点的重要性，量化不
同节点对网络瓦解的潜在影响。模拟节点失效过程，按学习到的重要性排序逐步移除节
点，观察网络功能（如最大连通分量占比、平均路径长度）的衰减趋势。
数据说明：可以按照论文[1]中的方法生成模拟的小网络进行训练，并使用[1-3]中涉及到
的真实网络进行网络瓦解，同时其他网站也有丰富的网络数据
(https://snap.stanford.edu/data/index.html, https://manliodedomenico.com/data.php)。
参考文献：
[1] Fan C, Zeng L, Sun Y, et al. Finding key players in complex networks through
122
deep reinforcement learning[J]. Nature machine intelligence, 2020,2(6): 317-324. [2] Grassia M, De Domenico M, Mangioni G. Machine learning dismantling and ear
ly-warning signals of disintegration in complex systems[J]. Nature communication
s, 2021, 12(1): 5190. [3] Morone F, Makse H A. Influence maximization in complex networks through op
timal percolation[J]. Nature, 2015, 524(7563): 65–68. [4] Zhang W, Jiang Z, Yao Q. DND: Deep learning-based directed network disintegr
ator[J]. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2023, 13(3): 841-850. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.68 网络韧性推断
任务描述：韧性是系统抵抗干扰或受到干扰后恢复的能力，是系统的重要属性，网络是
对复杂系统进行建模和分析的通用工具。网络韧性推断旨在基于网络的当前状态（如拓
扑结构、节点/边属性），通过理论分析（图论、统计物理）或图机器学习（GNN、表示
学习）等方法，量化网络的韧性，并预测其在潜在干扰下的表现。
数据描述：可以使用论文[1]中的数据，包括利用模型生成的模拟数据和实验收集的真实
数据。
123
参考文献：
[1] Liu C, Xu F, Gao C, Wang Z, Li Y, Gao J. Deep learning resilience inference
for complex networked systems[J]. Nature Communications, 2024, 15(1): 9203. [2] Liu C, Ding J, Song Y, et al. TDNetGen: Empowering complex network resilien
ce prediction with generative augmentation of topology and dynamics[C]//Proceed
ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 2024: 1875-1886. [3] Gao J, Barzel B, Barabási A-L. Universal resilience patterns in complex network
s[J]. Nature, 2016, 530(7590): 307–312. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.69 药物-靶标相互作用预测
任务描述：利用图机器学习方法在生物医学网络中提取药物和靶标蛋白质的特征，还可
以进一步补充药物和靶标蛋白质的自身属性作为特征（如药物的 SMILES 和靶标的蛋白
质序列），得到特征后进行链接预测，预测药物-靶标相互作用。
数据描述：可使用论文[1]中的生物医学网络数据，药物的 SMILES 和靶标的蛋白质序列
可以从[2][3]找到。
124
参考文献：
[1] Luo Y, Zhao X, Zhou J, et al. A network integration approach for drug-target in
teraction prediction and computational drug repositioning from heterogeneous info
rmation. Nat Commun 2017;8(1):1–13. [2] Wishart DS, Feunang YD, Guo AC, et al. DrugBank 5.0: a major update to the
DrugBank database for 2018. Nucleic Acids Res 2018;46(D1):D1074–82. [3] Consortium U. UniProt: a worldwide hub of protein knowledge. Nucleic Acids
Res 2019;47(D1):D506–15. [4] Wang H, Huang F, Xiong Z, et al. A heterogeneous network-based method with
attentive meta-path extraction for predicting drug–target interactions[J]. Briefings
in Bioinformatics, 2022, 23(4): bbac184. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.70 药物组合相互作用预测
任务描述：利用图机器学习方法在生物医学网络中提取药物的特征，还可以进一步补充
药物的自身属性作为特征（如药物的 SMILES），得到特征后进行链接预测，预测药物
相互作用类型。
数据描述：可以使用论文[1]中的数据。
结果示意图如下：
125
参考文献：
[1] Zitnik M, Agrawal M, Leskovec J. Modeling polypharmacy side effects with gra
ph convolutional networks[J]. Bioinformatics, 2018, 34(13): i457–i466. [2] Cheng F, Kovács I A, Barabási A-L. Network-based prediction of drug combinat
ions[J]. Nature Communications, 2019, 10(1): 1197. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.71 药物重定位
任务描述：利用图机器学习方法在生物医学网络中提取药物和疾病的特征，还可以进一
步补充药物和疾病的自身属性作为特征（如药物的 SMILES），得到特征后进行链接预
测，预测药物治疗疾病的关系，实现药物重定位，并尝试探索药物治疗疾病的机制。
数据描述：可以使用论文[1]中的数据。
126
参考文献：
[1] Ruiz C, Zitnik M, Leskovec J. Identification of disease treatment mechanisms th
rough the multiscale interactome[J]. Nature Communications, 2021, 12(1): 1796. [2] Yang J, Li Z, Wu W K K, Yu S, Xu Z, Chu Q, Zhang Q. Deep learning ident
ifies explainable reasoning paths of mechanism of action for drug repurposing fr
om multilayer biological network[J]. Briefings in Bioinformatics, 2022, 23(6): bb
ac469. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.72 疾病-microRNA 关联预测
任务描述：利用图机器学习方法在生物医学网络中提取 microRNA 和疾病的特征，还可
以进一步补充药物和疾病的自身属性作为特征，得到特征后进行链接预测，预测疾病
-microRNA 关联。
数据描述：可以使用论文[1]中的数据。
参考文献：
127
[1] Huang Z, Shi J, Gao Y, et al. HMDD v3. 0: a database for experimentally supp
orted human microRNA–disease associations[J]. Nucleic acids research, 2019, 47
(D1): D1013-D1017. [2] Liu Y, Zeng X, He Z, Zou Q. Inferring MicroRNA-Disease Associations by Ran
dom Walk on a Heterogeneous Network with Multiple Data Sources[J]. IEEE/AC
M Transactions on Computational Biology and Bioinformatics, 2017, 14(4): 905–
915. 指导老师：刘雪明，xm_liu@hust.edu.cn，qq:290115594@qq.com
5.73 真实环境海面目标检测
任务描述：在近年实海域航行中，出现了诸如：海面反光导致目标检测虚警、波浪抨击
导致跟踪目标突然丢失、喷溅液滴悬挂导致光学识别失效等特殊场景的人工智能算法应
用问题。海洋智能装备与系统最典型的技术特征就是具有智能演进能力。有鉴于此基于
实海域试验场，积累大量测试数据，持续更新迭代算法参数与架构，实现海洋装备智能
演进，是智能船艇发展的关键路径。通过真实环境下的水面实景视频（训练集为海面船
艇海试过程中录制的影像视频），检测出不同海上目标（如船只、海岸等）在画面中的
位置。
数据描述：需检测的目标类别包括障碍物“barrier”，海岸“coast” 两类。训练数据真值
中可能存在 this“船体” 这一类别，这一类不需要检测。需自行处理数据缺失、异常和少
量的质量问题。
数据集请于南一楼程骏处拷贝。联系方式: 微信 15673628667
数据标注样例：
128
参考文献：
[1] Y. Huang, H. Wang, J. Ma, J. Lou and H. Yi, "Research and Practical Exploration of
Test and Validation Technologies Applied on Unmanned Surface Vehicle Optical
Recognition," 2021 IEEE
[2] Long, J., Shelhamer, E., & Darrell, T. (2015). Fully Convolutional Networks for
Semantic Segmentation. [3] Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2018). DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous
Convolution, and Fully Connected CRFs. [4] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for
Biomedical Image Segmentation. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
129
5.74 智能水下感知
任务描述：声呐是一种声学探测设备，利用声波在水中的传播和反射特性，通过电声转
换和信息处理进行探测、测距和导航，是水声学中应用最广泛、最重要的一种装置，广
泛应用于搜救打捞、海洋工程、码头检查、环境监测、现场清理和水下调查等领域。将
待测目标悬挂在实验转台下方，目标旋转 360°，获取目标全方位的声图像；将图像声纳
吊放在目标同一深度，在目标旋转过程中，记录目标全方位的声图像。通过提供的真实
环境下的全方位水下声呐图像，对不同种类水下目标进行分类标注。可以自行补充数据
集以提高识别的泛化能力。
数据描述：需识别的目标类别包括单柱目标“single”，双柱目标“double”，三柱目标“triple”，
四柱目标“quadruple”，T 型目标“T-shape”五类。部分图像中有鱼游过，须自行处理。
训练集：240 张用于训练的图像，包含了五个类别目标的全方位探测数据；有
1000 张用于测试的图像；
130
备注：如遇数据缺失、异常和少量的质量问题，需自行处理。
数据集请于南一楼程骏处拷贝。联系方式: 微信 15673628667
参考文献：
[1] Y. Huang, H. Wang, J. Ma, J. Lou and H. Yi, "Research and Practical Exploration of
Test and Validation Technologies Applied on Unmanned Surface Vehicle Optical
Recognition," 2021 IEEE
[2] Long, J., Shelhamer, E., & Darrell, T. (2015). Fully Convolutional Networks for
Semantic Segmentation. [3] Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2018). DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous
Convolution, and Fully Connected CRFs. [4] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for
Biomedical Image Segmentation. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
5.75 心电可解释性分析
任务描述：心电图是心脏产生的电活动。先后有序的电激动的传播会引起一系列电位改
变，形成心电图上的相应波段。主要有五个重要的波段：P 波、Q 波、R 波、S 波、T
波，加上心房去极化导致的心房收缩前的一个小转折点。心电图当中比较重要的特征包
括有：P 波、PR 间隔、QRS 波群、ST 段、T 波和 QT 周期。标准的心电图称为 12 导联
131
心电图，它是由 12 根导线、10 个电极获得。通过分析心电图波形和间隔是否正常可以
进行心电图解释。
数据描述：提供左心室肥大和正常两类 12 导联心电信号的 excel 数据。自行对信号提取
特征然后基于决策树等模型进行分类和分析。
数据集在南一楼中 315 江一诺处拷贝。联系方式: QQ 25643385
数据可视化样例（单导联下重要性分析）：
参考文献：
[1] Friesen G M, Jannett T C, Jadallah M A, et al. A comparison of the noise sensitivity of
nine QRS detection algorithms[J]. IEEE Transactions on biomedical engineering, 1990, 37(1): 85-98. [2] Pan J, Tompkins W J. A real-time QRS detection algorithm[J]. IEEE transactions on
biomedical engineering, 1985 (3): 230-236. [3] Li Q, Rajagopalan C, Clifford G D. A machine learning approach to multi-level ECG
signal quality classification[J]. Computer methods and programs in biomedicine, 2014, 117(3): 435-447. [4] Song Y Y, Ying L U. Decision tree methods: applications for classification and
prediction[J]. Shanghai archives of psychiatry, 2015, 27(2): 130. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
5.76 左心室肥大二分类检测
任务描述：左心室肥大在心内科临床诊断治疗中非常常见，可能导致高血压等疾病。因
此，对左心室肥大进行及时有效的诊断，让患者早发现早治疗，具有重要的临床意义。
临床上心电图和心脏彩超是心室肥大诊断的常用手段，但心脏彩超检查价格昂贵并容易
受到设备和检查时间的限制，而心电图诊断作为一种经济实用、简单有效的方法，在心
132
脏类疾病诊断中应用广泛。利用深度学习进行左心室肥大心电图诊断，可以有效提取心
电图的复杂特征，摆脱传统人工诊断的局限性。
数据描述：提供左心室肥大和正常两类心电信号的 excel 数据，数据维度为（5000，12）。
所提供的数据类别即为 B 超诊断的标签，利用心电信号进行二分类诊断。
数据集在南一楼中 315 江一诺处拷贝。联系方式: QQ 25643385
数据可视化样例（12 导联心电信号）：
参考文献：
[1] Zhu H, Cheng C, Yin H, et al. Automatic multilabel electrocardiogram diagnosis of
heart rhythm or conduction abnormalities with deep learning: a cohort study[J]. The
Lancet Digital Health, 2020, 2(7): e348-e357. [2] Al Rahhal M M, Bazi Y, Al Zuair M, et al. Convolutional neural networks for
electrocardiogram classification[J]. Journal of Medical and Biological Engineering, 2018, 38(6): 1014-1025. [3] Zubair M, Kim J, Yoon C. An automated ECG beat classification system using
convolutional neural networks[C]. in: 2016 6th international conference on IT
convergence and security (ICITCS). IEEE, 2016: 1-5. [4] Hannun A Y, Rajpurkar P, Haghpanahi M, et al. Cardiologist-level arrhythmia
detection and classification in ambulatory electrocardiograms using a deep neural
network[J]. Nature medicine, 2019, 25(1): 65-69. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
5.77 3D Match 数据集点云配准
任 务 描 述 ： 对 3D Match 数 据 集 中 的 点 云 选 择 2 到 3 个 传 统 点 云 配 准 算 法
（ICP,NDT,RANSAC 等）以及深度学习的方法（PointNetLK，PRNet，IDAM，RPM-Net
等）进行配准。本题目需要基于该数据集评估至少三种点云配准方法在该数据集上的性
能表现并思考原因。
133
3D Match 几何配准基准
数据说明：3DMatch 数据集收集了来自于 62 个场景的数据，该数据集常用于 3D 点云的
关 键 点 ， 特 征 描 述 子 ， 点 云 配 准 等 任 务 。 数 据 集 可 通 过 如 下 网 址 获 取 ：
https://3dmatch.cs.princeton.edu/
参考文献：
[1] Zeng A, Song S, Nießner M, et al. 3dmatch: Learning local geometric descriptors
from rgb-d reconstructions[C]//Proceedings of the IEEE conference on computer vision
and pattern recognition. 2017: 1802-1811. [2] Aoki Y, Goforth H, Srivatsan R A, et al. Pointnetlk: Robust & efficient point cloud
registration using pointnet[C]//Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. 2019: 7163-7172. [3] Yang H, Shi J, Carlone L. Teaser: Fast and certifiable point cloud registration[J]. IEEE
Transactions on Robotics, 2020, 37(2): 314-333. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
134
5.78 Modelnet40 数据集点云分类
任务描述：点云分类是计算机视觉领域的一个重要任务，具有广泛的应用，例如三维物
体识别、场景理解和自动驾驶等。实现一个点云分类模型，通过对点云数据进行分类，
将其归类到各个事先定义好的类别中。使用 ModelNet40 数据集作为基准数据集评估模
型的性能。
Modelnet40 中椅子类别
数据说明：ModelNet40 数据集是一个常用的三维模型分类数据集，包含来自 40 个不同
类别的大约 12,000 个点云模型。每个点云模型由一系列三维点坐标表示，这些点坐标构
成了模型的形状信息。所有点云模型都经过了预处理和标准化，以确保每个点云模型具
有相同的维度和尺寸。数据集可通过如下网址获取：https://modelnet.cs.princeton.edu/
参考文献：
[1] Wu Z, Song S, Khosla A, et al. 3d shapenets: A deep representation for volumetric
shapes[C]//Proceedings of the IEEE conference on computer vision and pattern
recognition. 2015: 1912-1920. [2] Hackel T, Savinov N, Ladicky L, et al. Semantic3d. net: A new large-scale point cloud
classification benchmark[J]. arXiv preprint arXiv:1704.03847, 2017. [3] Ben-Shabat Y, Lindenbaum M, Fischer A. 3dmfv: Three-dimensional point cloud
classification in real-time using convolutional neural networks[J]. IEEE Robotics and
Automation Letters, 2018, 3(4): 3145-3152.
135
指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
5.79 铣削加工刀具状态监测
任务描述：铣削加工过程中刀具状态监测是一项重要任务，刀具更换不及时会导致损坏
昂贵工件，过早地更换刀具则会导致生产效率的下降。因此，刀具状态监测成为智能制
造中重要的一项任务。目前刀具状态主流方法为数据驱动方法，即根据目标刀具的历史
加工信号与当前加工信号预测当前一次加工完成后的刀具后刀面磨损宽度。其中主要任
务为时间序列表征以及刀具状态识别，要求熟悉时间序列表征方法以及非自回归任务常
用框架。
数据说明：PHM2010 数据集：包括 3 把铣刀全寿命周期铣削信号（切削力，振动，声
发射）与后刀面磨损宽度，每把铣刀使用 300 次。数据集联系助教拷贝，QQ：935359421。
HMoTP 数据集：包括 3 把铣刀全寿命周期铣削信号（切削力，振动，力矩）与后刀面
磨损宽度，每把铣刀使用 100 次。下载链接：
https://drive.google.com/drive/folders/1eBhTSXMd8jVITfQoJifC50fE_NIFqb89?usp=sharin
g。
三轴加工中心与数据采集系统
参考文献：
[1] Pimenov D Y, Bustillo A, Wojciechowski S, et al. Artificial intelligence systems for
136
tool condition monitoring in machining: Analysis and critical review[J]. Journal of
Intelligent Manufacturing, 2023, 34(5): 2079-2121. [2] Wang R, Song Q, Peng Y, et al. Toward digital twins for high-performance
manufacturing: Tool wear monitoring in high-speed milling of thin-walled parts using
domain knowledge[J]. Robotics and Computer-Integrated Manufacturing, 2024, 88:
102723. [3] Hao C, Mao X, Ma T, et al. A novel deep learning method with partly explainable:
Intelligent milling tool wear prediction model based on transformer informed physics[J]. Advanced Engineering Informatics, 2023, 57: 102106. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
5.80 复杂曲面加工质量预测
任务描述：复杂曲面加工过程中存在小样本、工序耦合复杂、误差溯源困难的问题。基
于生产需求，加工复杂曲面产线为柔性产线，即在同一条产线上加工多种类型的复杂曲
面。为了充分利用样本，要求建立的加工质量预测模型能够在不同域（对应不同尺寸外
形的叶片）之间进行域适应学习。主要任务为根据曲面中间工序状态预测最终加工质量，
运用数据补全方法，并要求使用域适应学习方法，如 MMD，CORAL 等。
137
数据说明：包括四种复杂曲面加工的两种中间工序（铣叶根、综合铣）目标点位坐标，
以及最终加工质量数据。数据集联系助教拷贝，QQ：935359421。
参考文献：
[1] Li X, Zhang Z, Gao L, et al. A new semi-supervised fault diagnosis method via deep
CORAL and transfer component analysis[J]. IEEE Transactions on emerging topics in
computational intelligence, 2021, 6(3): 690-699. [2] Li X, Zhang Z, Gao L, et al. A new semi-supervised fault diagnosis method via deep
CORAL and transfer component analysis[J]. IEEE Transactions on emerging topics in
computational intelligence, 2021, 6(3): 690-699. [3] Peng X, Bai Q, Xia X, et al. Moment matching for multi-source domain
adaptation[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 1406-1415. 指导老师：程骋, c_cheng@hust.edu.cn, qq:286652621
5.81 自主竞赛选题
138
任务描述：从以下网址中自主选择公开竞赛项目作为本课程设计题目，以竞赛排名作为
验收的测试依据。（本题选题小组数量不限）
https://www.kaggle.com/competitionshttps://aistudio.baidu.com/aistudio/competition
指导老师：可选择胡静、杨卫东、肖阳、邹腊梅、陆昊、胡若澜，邹旭、昌毅，刘雪明，
程骋老师中的任意一位作为指导老师。（指导老师指导小组数量设置上限，总指导小组
不超过 9 组）
六、资源与设备使用
6.1 Atlas200 硬件
Atlas200 模块的使用通过联系郑定富老师（13986105363）进行学习与实验使用。
使用方法是在科技楼 12 楼设备使用记录表中登记，只能在科技楼 12 楼实验室中使用，
不能带走。离开实验室时，把设备收拾好归还原位置。Atlas200 模块的学习将由郑老师
专门组织学习群，联系华为的技术支持老师约定时间集中授课学习使用方法。
6.2 算力服务器
（1）百度公有云算力资源：可提供 130 多位学生的公有云算力资源，每学期每学
生 100 小时，算力水平为 Nvidia V100/16G。基于公有云算力，可进行 AI 教学和实验实
训。
a）高峰时可能需要排队。
b) 公有云平台算力仅支持 PaddlePaddle，百度公司可以提供培训和技术支持。培训
和学习由郑老师联系百度的技术支持老师约定时间集中学习。
c) 如果 100 多小时不够，用完可以再申请。
（2）学院 AI 平台：第一年测试试用，需要师生共同探索。
建议今年的课程设计以百度算力为主完成，学院 AI 平台为辅。相关事宜与郑定富
老师（13986105363）联系。
139
七、课程设计的考查
7.1 考核方式
由平时检查和结题答辩时的提问抽查、现场演示、课题难易程度和工作量饱满程度，
以及设计报告撰写情况等几方面综合起来考虑。具体包括学习与设计态度的认真性，课
堂知识理解掌握的深入程度，常用工具软件应用的熟练程度，设计方案的正确性或合理
性，图文的质量效果，是否独立完成，是否具有独立分析解决问题的能力和创新精神等。
7.2 成绩评定
课程设计成绩的评价依据 = 工作量（15%）+个人工作表现（10%）+创新性（20%）
+实验与分析（35%）+报告写作（10%）+报告答辩及成果展示（10%）。
评分内容 综合表现 设计报告 答辩
工作量 个人工作表现 创新性 实验与分析 报告写作
报告答辩及
成果展示
评价比重 0.15 0.10 0.20 0.35 0.10 0.10
*切记要独立完成，不得抄袭他人成果。一旦发现抄袭者，课程成绩一律按不及格
处理。
课程设计成绩：按答辩时的评审老师评分平均值计算，采用百分制评定。95-100 为
优秀，90-94 为良好，70-89 为中等，60-69 为及格，低于 60 分的为不及格。
优秀：能独立完成设计要求所规定的全部内容，设计方案正确、基本概念清楚，有
独到的见解或创造性，与当前主流研究成果相比较效果更优异。每个组员的工作量饱满，
贡献突出。
良好：能较好完成设计要求所规定的全部内容，设计方案正确，分析问题正确、基
本概念清楚，运用多种当前主流技术和算法进行了任务实验、性能对比和深入分析。
中等：能完成设计要求规定的全部内容，设计方案基本正确，基本概念清楚。
及格：基本完成设计要求规定的内容，设计方案基本合理，基本概念较清楚。
不及格：未完成设计要求规定的内容，设计方案不合理，或有较严重缺陷，基本概
念不清楚。
140
八、课程设计报告撰写要求
课程设计说明书是课程设计工作的总结，它应该反映出学生在课程设计过程中所做
的主要工作和取得的主要成果。学生必须以积极认真、严谨求实的态度完成课程设计说
明书的撰写。
课程设计说明书写作的具体内容包括：
1、封面
包括题目，指导老师，组员信息（姓名，学号，班级，每个人的工作占比）
2、摘要: 要求写出同一内容的中文和英文的摘要。摘要应说明本设计的中心思想和
主要内容，突出设计中的新见解新方法，说明该设计方案的理论根据及现实意
义。摘要力求简明扼要，字数为 300 字左右。
3、目录: 目录是整个设计的提纲，也是设计的重要组成部分，它方便评阅教师了解
设计的整体结构。目录以章、节两级目录为宜。
4、正文: 课程设计说明书的正文一般可按章、节的格式来书写，正文常常包括如下
几个部分：
1）第一章 课题概述
简要介绍所选课题现行研究现状、存在的主要问题，说明选题的意义及必要性。
2）第二章 算法分析
2.1 需求分析
2.2 研究方案设计
2.3 试验算法选择与分析（或算法设计）
2.4 特征提取算法研究（可选）
2.5 特征分析算法研究（可选）
2.6 模式分类/识别算法研究（可选）
3）第三章 试验系统设计(包含程序流程图、程序功能介绍)
3.1 系统总体结构设计
3.2 代码设计
3.3 输入/输出设计
3.4 模块功能与处理过程设计
4）第四章 软件实施与实验运行
4.1 软件系统实施（编程、调试、运行）
4.2 数据库测试
141
4.3 试验结果与分析
5）第五章 结束语
研究结论,通过课程设计对模式识别学科的认识与体会,每人的工作划分。
6）附录 主要模块代码（注释代码比例不得低于 1：1）
7）参考文献（不少于 6 篇）
以上内容供学生编写设计说明书时参考，学生可根据实际系统开发情况及指导
教师的具体要求进行内容的增删或章节的调整。